\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global/global}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Concetti Preliminari}{1}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Prerequisiti Matematici Essenziali [WIP]}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:prerequisiti}{{1}{3}{Prerequisiti Matematici Essenziali [WIP]}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Grafi}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definizione formale}{5}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Grafo non orientato con quattro vertici \(V=\{A,B,C,D\}\) e archi \(E=\{(A,B),(A,C),(A,D),(B,D),(C,D)\}\). In figura: A è collegato a B (in alto), a C (sinistra) e a D (diagonale); B è collegato a D (destra); C è collegato a D (in basso).\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:grafo_esempio}{{2.1}{5}{Grafo non orientato con quattro vertici \(V=\{A,B,C,D\}\) e archi \(E=\{(A,B),(A,C),(A,D),(B,D),(C,D)\}\). In figura: A è collegato a B (in alto), a C (sinistra) e a D (diagonale); B è collegato a D (destra); C è collegato a D (in basso).\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Network science}{6}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Grafi diretti e indiretti}{6}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Confronto tra grafo non orientato (a sinistra) e grafo orientato (a destra). Nel grafo orientato, gli archi hanno una direzione indicata da frecce, mentre nel grafo non orientato rappresentano relazioni bidirezionali.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:grafo_diretto_indiretto}{{2.2}{6}{Confronto tra grafo non orientato (a sinistra) e grafo orientato (a destra). Nel grafo orientato, gli archi hanno una direzione indicata da frecce, mentre nel grafo non orientato rappresentano relazioni bidirezionali.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Come capire che tipologia usare.}{6}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Grafi pesati e grafi etichettati}{7}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Esempio di grafo pesato (a sinistra) ed etichettato (a destra). Nel grafo pesato, i numeri sugli archi rappresentano i pesi associati a ciascun arco. Nel grafo etichettato, i vertici sono etichettati con nomi specifici.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:grafo_pesato_etichettato}{{2.3}{7}{Esempio di grafo pesato (a sinistra) ed etichettato (a destra). Nel grafo pesato, i numeri sugli archi rappresentano i pesi associati a ciascun arco. Nel grafo etichettato, i vertici sono etichettati con nomi specifici.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Gradi dei vertici}{7}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Distribuzione dei gradi}{8}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Grafi bipartiti}{8}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Esempio di grafo bipartito con partizioni \(U\) (nodi verdi) e \(V\) (nodi viola): gli archi collegano solo vertici appartenenti a insiemi diversi. In basso sono mostrati i grafi proiettati: \emph  {projection \(U\)} (a sinistra), dove due vertici di \(U\) sono adiacenti se condividono almeno un vicino in \(V\), e \emph  {projection \(V\)} (a destra), definita simmetricamente.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:grafo_bipartito}{{2.4}{8}{Esempio di grafo bipartito con partizioni \(U\) (nodi verdi) e \(V\) (nodi viola): gli archi collegano solo vertici appartenenti a insiemi diversi. In basso sono mostrati i grafi proiettati: \emph {projection \(U\)} (a sinistra), dove due vertici di \(U\) sono adiacenti se condividono almeno un vicino in \(V\), e \emph {projection \(V\)} (a destra), definita simmetricamente.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Generalizzazione.}{9}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Grafo completo vs Grafo regolare}{9}{section.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A sinistra: esempi di grafi completi \(K_n\) per \(n=2,\dots  ,7\), in cui ogni coppia di vertici è adiacente. A destra: esempi di grafi \(k\)-regolari (da \(k=1\) a \(k=4\)), in cui ogni vertice ha lo stesso grado \(k\).\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:grafo_regolare}{{2.5}{9}{A sinistra: esempi di grafi completi \(K_n\) per \(n=2,\dots ,7\), in cui ogni coppia di vertici è adiacente. A destra: esempi di grafi \(k\)-regolari (da \(k=1\) a \(k=4\)), in cui ogni vertice ha lo stesso grado \(k\).\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Cammini tra due nodi}{9}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Cammino minimo}{9}{subsection.2.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Esempio di cammino in un grafo: gli archi verdi definiscono la struttura, mentre in arancione è evidenziato un \emph  {cammino} che attraversa i vertici \(1 \to 2 \to 5 \to 7\).\relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:cammino_grafo}{{2.6}{10}{Esempio di cammino in un grafo: gli archi verdi definiscono la struttura, mentre in arancione è evidenziato un \emph {cammino} che attraversa i vertici \(1 \to 2 \to 5 \to 7\).\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Diametro}{10}{subsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Ciclo}{10}{subsection.2.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cappio.}{10}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Connettività}{10}{section.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Connettività forte e debole}{11}{subsection.2.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Coefficiente di Clustering}{11}{section.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Esempio del coefficiente di clustering locale per un nodo centrale (viola) con quattro vicini (arancioni). A sinistra il vicinato è completamente connesso, quindi \(C_i=1\); al centro solo metà delle possibili connessioni tra i vicini è presente (\(C_i=\tfrac  {1}{2}\)); a destra non ci sono archi tra i vicini e il coefficiente è nullo (\(C_i=0\)).\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:clustering-coeff}{{2.7}{11}{Esempio del coefficiente di clustering locale per un nodo centrale (viola) con quattro vicini (arancioni). A sinistra il vicinato è completamente connesso, quindi \(C_i=1\); al centro solo metà delle possibili connessioni tra i vicini è presente (\(C_i=\tfrac {1}{2}\)); a destra non ci sono archi tra i vicini e il coefficiente è nullo (\(C_i=0\)).\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Clustering medio.}{12}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coefficiente di clustering globale.}{12}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Misure di centralità}{12}{section.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.1}Centralità di grado}{12}{subsection.2.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.2}Centralità di vicinanza}{12}{subsection.2.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Esempio di calcolo della centralità di vicinanza per i nodi in un grafo. I numeri accanto ai nodi indicano le distanze minime da ciascun nodo agli altri nodi nel grafo.\relax }}{13}{figure.caption.14}\protected@file@percent }
\newlabel{fig:centralita_vicinanza}{{2.8}{13}{Esempio di calcolo della centralità di vicinanza per i nodi in un grafo. I numeri accanto ai nodi indicano le distanze minime da ciascun nodo agli altri nodi nel grafo.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Esempio (nodo \(3\)).}{13}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.3}Centralità di prossimità}{14}{subsection.2.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Esempio di calcolo della centralità di prossimità per i nodi in un grafo. I numeri accanto ai nodi indicano i valori di centralità di prossimità calcolati in base alle distanze minime da ciascun nodo agli altri nodi nel grafo.\relax }}{14}{figure.caption.16}\protected@file@percent }
\newlabel{fig:centralita_prossimita}{{2.9}{14}{Esempio di calcolo della centralità di prossimità per i nodi in un grafo. I numeri accanto ai nodi indicano i valori di centralità di prossimità calcolati in base alle distanze minime da ciascun nodo agli altri nodi nel grafo.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.4}Centralità di PageRank}{14}{subsection.2.10.4}\protected@file@percent }
\abx@aux@cite{0}{Barabasi2016NetworkScience}
\abx@aux@segm{0}{0}{Barabasi2016NetworkScience}
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{15}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Configurazione iniziale della rete diretta con quattro nodi \(A, B, C, D\), ciascuno con uno score iniziale di \(0.25\).\relax }}{15}{figure.caption.18}\protected@file@percent }
\newlabel{fig:pagerank_initial}{{2.10}{15}{Configurazione iniziale della rete diretta con quattro nodi \(A, B, C, D\), ciascuno con uno score iniziale di \(0.25\).\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Evoluzione dei valori di PageRank per i nodi \(A, B, C, D\) nei round successivi.\relax }}{15}{figure.caption.19}\protected@file@percent }
\newlabel{fig:pagerank_rounds}{{2.11}{15}{Evoluzione dei valori di PageRank per i nodi \(A, B, C, D\) nei round successivi.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Tecniche di Data Mining}{17}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Introduzione al Data Mining}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduzione}{{3}{19}{Introduzione al Data Mining}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Definizione e finalità}{19}{section.3.1}\protected@file@percent }
\newlabel{sec:definizione}{{3.1}{19}{Definizione e finalità}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Caratteristiche dei pattern}{19}{section.3.2}\protected@file@percent }
\newlabel{sec:caratteristiche-pattern}{{3.2}{19}{Caratteristiche dei pattern}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Metodi di data mining}{19}{section.3.3}\protected@file@percent }
\newlabel{sec:metodi}{{3.3}{19}{Metodi di data mining}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Perché fare data mining}{20}{section.3.4}\protected@file@percent }
\newlabel{sec:motivazioni}{{3.4}{20}{Perché fare data mining}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Big Data}{20}{subsection.3.4.1}\protected@file@percent }
\newlabel{subsec:bigdata}{{3.4.1}{20}{Big Data}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Dai dati alla conoscenza e alle comunità coinvolte}{20}{subsection.3.4.2}\protected@file@percent }
\newlabel{subsec:comunita}{{3.4.2}{20}{Dai dati alla conoscenza e alle comunità coinvolte}{subsection.3.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Limiti e insidie del data mining}{20}{section.3.5}\protected@file@percent }
\newlabel{sec:limiti}{{3.5}{20}{Limiti e insidie del data mining}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Caso di studio: Total Information Awareness (TIA)}{20}{subsection.3.5.1}\protected@file@percent }
\newlabel{subsec:tia}{{3.5.1}{20}{Caso di studio: Total Information Awareness (TIA)}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Esempio: co-presenza in hotel come criterio di sospetto}{21}{subsection.3.5.2}\protected@file@percent }
\newlabel{subsec:esempio-hotel}{{3.5.2}{21}{Esempio: co-presenza in hotel come criterio di sospetto}{subsection.3.5.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Dati di partenza.}{21}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ipotesi nulla (random).}{21}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Calcoli numerici.}{21}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Considerazioni.}{21}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Principio di Bonferroni e test multipli}{22}{section.3.6}\protected@file@percent }
\newlabel{sec:bonferroni}{{3.6}{22}{Principio di Bonferroni e test multipli}{section.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretazione operativa.}{22}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quando applicarlo.}{22}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Preprocessing}{23}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Estrazione di feature}{23}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Tecniche di estrazione di feature}{24}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Portabilità dei dati}{24}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Da dati numerici a categorici}{24}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Equi-width ranges.}{24}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Equi-log ranges.}{24}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Equi-depth ranges.}{24}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Da dati categorici a numerici}{25}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One-hot encoding.}{25}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Da testo a dati numerici}{25}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Cleaning dei dati}{25}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Gestione dei valori mancanti}{25}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Gestione dei valori errati}{26}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantili.}{26}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Scala dei dati}{26}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standardizzazione.}{27}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Min-Max scaling.}{27}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Riduzione dei dati}{27}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Sampling dei dati}{27}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Selezione di feature}{28}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Riduzione della dimensionalità}{28}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}PCA: Principal Component Analysis}{28}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}SVD: Singular Value Decomposition}{30}{subsection.4.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretazione geometrica.}{30}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Varianti ridotte della SVD.}{30}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Confronto tra le principali varianti della Decomposizione ai Valori Singolari (SVD): Full SVD, Compact SVD, Thin SVD e Truncated SVD, con rappresentazione delle dimensioni delle matrici coinvolte.\relax }}{31}{figure.caption.36}\protected@file@percent }
\newlabel{fig:SVD}{{4.1}{31}{Confronto tra le principali varianti della Decomposizione ai Valori Singolari (SVD): Full SVD, Compact SVD, Thin SVD e Truncated SVD, con rappresentazione delle dimensioni delle matrici coinvolte.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {paragraph}{SVD vs PCA.}{31}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.6}LSA: Latent Semantic Analysis}{32}{subsection.4.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.7}Riduzione di dimensionalità con trasformazione dei dati}{32}{subsection.4.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio: serie temporali.}{32}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Insiemi Frequenti e Regole d'Associazione}{33}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:frequent-itemsets}{{5}{33}{Insiemi Frequenti e Regole d'Associazione}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Market-basket model e definizioni}{33}{section.5.1}\protected@file@percent }
\newlabel{sec:mbm}{{5.1}{33}{Market-basket model e definizioni}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Supporto}{33}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Soglia di supporto: trade-off.}{33}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Regole d'associazione}{33}{section.5.2}\protected@file@percent }
\newlabel{sec:assoc}{{5.2}{33}{Regole d'associazione}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Qualità di una regola}{34}{subsection.5.2.1}\protected@file@percent }
\newlabel{subsec:qualita-regole}{{5.2.1}{34}{Qualità di una regola}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Confidenza.}{34}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coverage.}{34}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interesse.}{34}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lift.}{34}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nota.}{34}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mini-esempio (toy dataset).}{34}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Insiemi frequenti chiusi e massimali}{34}{section.5.3}\protected@file@percent }
\newlabel{sec:closed-maximal}{{5.3}{34}{Insiemi frequenti chiusi e massimali}{section.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Itemset lattice (minsup 2)}}{35}{figure.caption.46}\protected@file@percent }
\newlabel{fig:ifm}{{5.1}{35}{Itemset lattice (minsup 2)}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Anti-monotonia e Principio di Apriori}{35}{section.5.4}\protected@file@percent }
\newlabel{sec:apriori-principle}{{5.4}{35}{Anti-monotonia e Principio di Apriori}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Principio di Apriori}{35}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Algoritmo Apriori}{35}{section.5.5}\protected@file@percent }
\newlabel{sec:apriori}{{5.5}{35}{Algoritmo Apriori}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Esempio Apriori (minsup = 2)}{36}{subsection.5.5.1}\protected@file@percent }
\newlabel{subsec:apriori-esempio}{{5.5.1}{36}{Esempio Apriori (minsup = 2)}{subsection.5.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{$k=1\!\to \!2$: generazione $C_2$ e conteggi.}{36}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$k=2\!\to \!3$: self-join e prune.}{36}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Generazione dei candidati}{36}{subsection.5.5.2}\protected@file@percent }
\newlabel{subsec:candidate-gen}{{5.5.2}{36}{Generazione dei candidati}{subsection.5.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Ottimizzazioni di Apriori}{36}{section.5.6}\protected@file@percent }
\newlabel{sec:apriori-opt}{{5.6}{36}{Ottimizzazioni di Apriori}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Hashing in bucket: PCY}{36}{subsection.5.6.1}\protected@file@percent }
\newlabel{subsec:pcy}{{5.6.1}{36}{Hashing in bucket: PCY}{subsection.5.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Varianti multistadio e multihash.}{37}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Partizionamento del DB: SON}{37}{subsection.5.6.2}\protected@file@percent }
\newlabel{subsec:son}{{5.6.2}{37}{Partizionamento del DB: SON}{subsection.5.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Campionamento e frontiera negativa: Toivonen}{37}{subsection.5.6.3}\protected@file@percent }
\newlabel{subsec:toivonen}{{5.6.3}{37}{Campionamento e frontiera negativa: Toivonen}{subsection.5.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Perch\'e andare oltre Apriori}{37}{section.5.7}\protected@file@percent }
\newlabel{sec:oltre-apriori}{{5.7}{37}{Perch\'e andare oltre Apriori}{section.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}FP-Growth: idea di base}{37}{section.5.8}\protected@file@percent }
\newlabel{sec:fpgrowth}{{5.8}{37}{FP-Growth: idea di base}{section.5.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}Costruzione dell'FP-tree}{37}{subsection.5.8.1}\protected@file@percent }
\newlabel{subsec:costruzione-fptree}{{5.8.1}{37}{Costruzione dell'FP-tree}{subsection.5.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Esempio di FP-Growth}{38}{subsection.5.8.2}\protected@file@percent }
\newlabel{subsec:fpg-example}{{5.8.2}{38}{Esempio di FP-Growth}{subsection.5.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Costruzione dell'FP-tree: a sinistra transazioni riordinate; a destra l'albero con prefissi condivisi e contatori aggiornati.\relax }}{38}{figure.caption.50}\protected@file@percent }
\newlabel{fig:fp-growth-complete}{{5.2}{38}{Costruzione dell'FP-tree: a sinistra transazioni riordinate; a destra l'albero con prefissi condivisi e contatori aggiornati.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {paragraph}{Visita per pattern-growth.}{38}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Espansione di un item $x$.}{38}{section*.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Header table e node-link per l'item $p$: la base condizionale di $p$ si ottiene seguendo i link e risalendo verso la radice.\relax }}{39}{figure.caption.53}\protected@file@percent }
\newlabel{fig:fp-growth-links}{{5.3}{39}{Header table e node-link per l'item $p$: la base condizionale di $p$ si ottiene seguendo i link e risalendo verso la radice.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {paragraph}{Esempio 1: item $p$.}{39}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 2: item $m$.}{39}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 3: item $b$.}{39}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Confronto: FP-Growth vs Apriori}{39}{section.5.9}\protected@file@percent }
\newlabel{subsec:confronto-fp-apriori}{{5.9}{39}{Confronto: FP-Growth vs Apriori}{section.5.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Confronto sintetico tra Apriori e FP-Growth.\relax }}{39}{table.caption.57}\protected@file@percent }
\newlabel{tab:apriori-vs-fpgrowth}{{5.1}{39}{Confronto sintetico tra Apriori e FP-Growth.\relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Clustering}{41}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Spazi metrici e distanze}{41}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Spazio euclideo}{41}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Centroide.}{42}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Spazi non euclidei}{42}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Algoritmi di clustering}{43}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Tipi di clustering}{43}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Bontà di un algoritmo}{43}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Curse of dimensionality}{44}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Ortogonalità dei vettori}{44}{subsection.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Clustering Gerarchico}{45}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Distanze tra cluster}{45}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Dendrogramma}{45}{subsection.6.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Esempi grafici delle diverse nozioni di distanza tra cluster.\relax }}{46}{figure.caption.59}\protected@file@percent }
\newlabel{fig:cluster_distances}{{6.1}{46}{Esempi grafici delle diverse nozioni di distanza tra cluster.\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A sinistra: punti nel piano con centroidi (triangoli) e cerchi che schematizzano la coesione dei gruppi; i colori indicano i cluster. A destra: dendrogramma agglomerativo che mostra l'ordine di fusione e l'altezza (distanza di linkage). Un taglio orizzontale del dendrogramma determina il numero di cluster.\relax }}{46}{figure.caption.60}\protected@file@percent }
\newlabel{fig:dendograms}{{6.2}{46}{A sinistra: punti nel piano con centroidi (triangoli) e cerchi che schematizzano la coesione dei gruppi; i colori indicano i cluster. A destra: dendrogramma agglomerativo che mostra l'ordine di fusione e l'altezza (distanza di linkage). Un taglio orizzontale del dendrogramma determina il numero di cluster.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Clustering divisivo}{47}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Complessità computazionale}{47}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Clustering partizionale: K-means}{47}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Scelta greedy dei centroidi iniziali}{48}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Funzione obiettivo}{48}{subsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Scelta del numero di cluster}{48}{subsection.6.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Metodo elbow.}{48}{section*.61}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Metodo (\emph  {elbow}). Si traccia la distanza media dal centroide (o WCSS/n) al variare di $k$; il valore “ottimo” è nel punto di flesso, dove l'aumento di $k$ porta benefici marginali trascurabili.\relax }}{49}{figure.caption.62}\protected@file@percent }
\newlabel{fig:elbow}{{6.3}{49}{Metodo (\emph {elbow}). Si traccia la distanza media dal centroide (o WCSS/n) al variare di $k$; il valore “ottimo” è nel punto di flesso, dove l'aumento di $k$ porta benefici marginali trascurabili.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {paragraph}{Metodo silhouette.}{49}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Complessità computazionale}{50}{subsection.6.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}K-means su Big data}{51}{subsection.6.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Clustering per densità: DBSCAN}{51}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}DBSCAN}{51}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definizione di cluster in DBSCAN.}{51}{section*.65}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Relazioni di raggiungibilità e connessione per densità in DBSCAN (parametri $\varepsilon $ e \texttt  {minPts}).\relax }}{52}{figure.caption.64}\protected@file@percent }
\newlabel{fig:dbscan_reachability}{{6.4}{52}{Relazioni di raggiungibilità e connessione per densità in DBSCAN (parametri $\varepsilon $ e \texttt {minPts}).\relax }{figure.caption.64}{}}
\@writefile{toc}{\contentsline {paragraph}{Algoritmo DBSCAN.}{52}{section*.66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces $k$-distance plot: il gomito suggerisce il valore di $\varepsilon $.\relax }}{53}{figure.caption.67}\protected@file@percent }
\newlabel{fig:kdist}{{6.5}{53}{$k$-distance plot: il gomito suggerisce il valore di $\varepsilon $.\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {paragraph}{Complessità computazionale.}{53}{section*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}OPTICS}{53}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Distanza di raggiungibilità e area localizzata.}{53}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reachability plot.}{54}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estrazione dei cluster.}{54}{section*.72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces OPTICS: esempio di \emph  {reachability plot}. Le zone basse (valli) indicano cluster densi; le zone alte (creste) indicano separazioni. (immagine da libro/slide)\relax }}{55}{figure.caption.71}\protected@file@percent }
\newlabel{fig:optics-reach}{{6.6}{55}{OPTICS: esempio di \emph {reachability plot}. Le zone basse (valli) indicano cluster densi; le zone alte (creste) indicano separazioni. (immagine da libro/slide)\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces OPTICS. (1) A sinistra: punti nel piano e, sotto, \emph  {reachability plot}; le valli (segmenti colorati) corrispondono a regioni dense/cluster, mentre la linea orizzontale tratteggiata indica una soglia $\epsilon $ che produce un taglio in stile DBSCAN. Le linee tratteggiate collegano ogni gruppo nel piano al suo intervallo nel plot. (2) A destra: regola \emph  {steep down/up} per l'estrazione automatica dei cluster dal reachability plot (si entra quando la reachability scende bruscamente e si esce quando risale).\relax }}{55}{figure.caption.73}\protected@file@percent }
\newlabel{fig:optics-cluster-extraction}{{6.7}{55}{OPTICS. (1) A sinistra: punti nel piano e, sotto, \emph {reachability plot}; le valli (segmenti colorati) corrispondono a regioni dense/cluster, mentre la linea orizzontale tratteggiata indica una soglia $\epsilon $ che produce un taglio in stile DBSCAN. Le linee tratteggiate collegano ogni gruppo nel piano al suo intervallo nel plot. (2) A destra: regola \emph {steep down/up} per l'estrazione automatica dei cluster dal reachability plot (si entra quando la reachability scende bruscamente e si esce quando risale).\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {paragraph}{Complessità computazionale.}{56}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}HDBSCAN}{56}{subsection.6.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Esempio di clustering con HDBSCAN. \relax }}{56}{figure.caption.75}\protected@file@percent }
\newlabel{fig:hdbscan-example}{{6.8}{56}{Esempio di clustering con HDBSCAN. \relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {paragraph}{Cluster in HDBSCAN.}{56}{section*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estrazione dei cluster stabili.}{57}{section*.77}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces HDBSCAN. Sinistra: MST costruito sulla \emph  {mutual reachability distance} (archi più pesanti vengono tagliati al crescere di \(\lambda \)). Destra: \emph  {condensed tree}; in evidenza i cluster scelti massimizzando la stabilità.\relax }}{58}{figure.caption.78}\protected@file@percent }
\newlabel{fig:hdbscan-figs}{{6.9}{58}{HDBSCAN. Sinistra: MST costruito sulla \emph {mutual reachability distance} (archi più pesanti vengono tagliati al crescere di \(\lambda \)). Destra: \emph {condensed tree}; in evidenza i cluster scelti massimizzando la stabilità.\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Classificazione}{59}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:classificazione}{{7}{59}{Classificazione}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduzione}{59}{section.7.1}\protected@file@percent }
\newlabel{sec:intro-class}{{7.1}{59}{Introduzione}{section.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Predizione (regressione).}{59}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Schema generale di un classificatore}{59}{subsection.7.1.1}\protected@file@percent }
\newlabel{subsec:schema-class}{{7.1.1}{59}{Schema generale di un classificatore}{subsection.7.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Overfitting.}{59}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Requisiti desiderabili}{59}{subsection.7.1.2}\protected@file@percent }
\newlabel{subsec:req}{{7.1.2}{59}{Requisiti desiderabili}{subsection.7.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Schema a blocchi di un classificatore: addestramento, validazione e uso.\relax }}{60}{figure.caption.81}\protected@file@percent }
\newlabel{fig:schema-class}{{7.1}{60}{Schema a blocchi di un classificatore: addestramento, validazione e uso.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Alberi decisionali}{60}{section.7.2}\protected@file@percent }
\newlabel{sec:trees}{{7.2}{60}{Alberi decisionali}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Classificazione tramite albero}{60}{subsection.7.2.1}\protected@file@percent }
\newlabel{subsec:tree-class}{{7.2.1}{60}{Classificazione tramite albero}{subsection.7.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Dataset \emph  {weather} (a sinistra) e albero decisionale appreso (a destra). La tabella contiene 14 esempi con quattro attributi descrittivi (\texttt  {Outlook}, \texttt  {Temperature}, \texttt  {Humidity}, \texttt  {Windy}) e la classe binaria \texttt  {P/N}. L’albero (stile ID3/C4.5) sceglie come radice \texttt  {Outlook}; il ramo \texttt  {overcast} porta direttamente alla classe \texttt  {P}, mentre per \texttt  {sunny} si testa \texttt  {Humidity} e per \texttt  {rain} si testa \texttt  {Windy}. L’esempio illustra il passaggio da dati tabellari a regole interpretabili.\relax }}{60}{figure.caption.82}\protected@file@percent }
\newlabel{fig:weather-tree}{{7.2}{60}{Dataset \emph {weather} (a sinistra) e albero decisionale appreso (a destra). La tabella contiene 14 esempi con quattro attributi descrittivi (\texttt {Outlook}, \texttt {Temperature}, \texttt {Humidity}, \texttt {Windy}) e la classe binaria \texttt {P/N}. L’albero (stile ID3/C4.5) sceglie come radice \texttt {Outlook}; il ramo \texttt {overcast} porta direttamente alla classe \texttt {P}, mentre per \texttt {sunny} si testa \texttt {Humidity} e per \texttt {rain} si testa \texttt {Windy}. L’esempio illustra il passaggio da dati tabellari a regole interpretabili.\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Costruzione top–down}{60}{subsection.7.2.2}\protected@file@percent }
\newlabel{subsec:topdown}{{7.2.2}{60}{Costruzione top–down}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Pruning.}{61}{section*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Splitting degli attributi}{61}{subsection.7.2.3}\protected@file@percent }
\newlabel{subsec:splitting}{{7.2.3}{61}{Splitting degli attributi}{subsection.7.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Scelta dell’attributo e strategia greedy}{61}{subsection.7.2.4}\protected@file@percent }
\newlabel{subsec:greedy}{{7.2.4}{61}{Scelta dell’attributo e strategia greedy}{subsection.7.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Misure di goodness}{61}{section.7.3}\protected@file@percent }
\newlabel{sec:goodness}{{7.3}{61}{Misure di goodness}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Information Gain (ID3)}{61}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:ig}{{7.3.1}{61}{Information Gain (ID3)}{subsection.7.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Idea.}{61}{section*.84}\protected@file@percent }
\newlabel{par:ig-example}{{7.3.1}{62}{Esempio e limitazioni}{section*.85}{}}
\@writefile{toc}{\contentsline {paragraph}{Limite noto.}{62}{section*.86}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Esempio di scelta della radice con Information Gain sul dataset “weather”.\relax }}{62}{figure.caption.87}\protected@file@percent }
\newlabel{fig:ig-weather}{{7.3}{62}{Esempio di scelta della radice con Information Gain sul dataset “weather”.\relax }{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Gain Ratio (C4.5)}{62}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:gain-ratio}{{7.3.2}{62}{Gain Ratio (C4.5)}{subsection.7.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Selezione in C4.5.}{63}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nota pratica (attributi continui).}{63}{section*.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Gini Index (CART)}{63}{subsection.7.3.3}\protected@file@percent }
\newlabel{subsec:gini}{{7.3.3}{63}{Gini Index (CART)}{subsection.7.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Pruning degli alberi}{63}{subsection.7.3.4}\protected@file@percent }
\newlabel{subsec:pruning}{{7.3.4}{63}{Pruning degli alberi}{subsection.7.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Che cosa misurano.}{64}{section*.91}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decisione di pruning.}{64}{section*.92}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Errore prima e dopo lo split.}{65}{section*.94}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Indice di costo–complessità per lo split.}{65}{section*.95}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Pruning: confronto errore stimato del sottoalbero vs foglia (C4.5) e principio costo–complessità (CART).\relax }}{65}{figure.caption.96}\protected@file@percent }
\newlabel{fig:pruning}{{7.4}{65}{Pruning: confronto errore stimato del sottoalbero vs foglia (C4.5) e principio costo–complessità (CART).\relax }{figure.caption.96}{}}
\@writefile{toc}{\contentsline {paragraph}{Pro/contro degli alberi decisionali.}{65}{section*.97}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Classificatori generativi}{65}{section.7.4}\protected@file@percent }
\newlabel{sec:generative}{{7.4}{65}{Classificatori generativi}{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Teorema di Bayes e regola di decisione}{66}{subsection.7.4.1}\protected@file@percent }
\newlabel{subsec:bayes-rule}{{7.4.1}{66}{Teorema di Bayes e regola di decisione}{subsection.7.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Naive Bayes}{66}{subsection.7.4.2}\protected@file@percent }
\newlabel{subsec:naive-bayes}{{7.4.2}{66}{Naive Bayes}{subsection.7.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Idea.}{66}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regola di decisione (MAP, in scala logaritmica).}{66}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stima essenziale delle probabilità.}{66}{section*.100}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vantaggi e svantaggi.}{66}{section*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Reti Bayesiane}{67}{subsection.7.4.3}\protected@file@percent }
\newlabel{subsec:bayesnet}{{7.4.3}{67}{Reti Bayesiane}{subsection.7.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Bayes net Sprinkler–Rain–Grass}}{67}{figure.caption.102}\protected@file@percent }
\newlabel{fig:bayes-net}{{7.5}{67}{Bayes net Sprinkler–Rain–Grass}{figure.caption.102}{}}
\@writefile{toc}{\contentsline {paragraph}{Uso per la classificazione.}{67}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Classificatori discriminativi}{67}{section.7.5}\protected@file@percent }
\newlabel{sec:discriminativi}{{7.5}{67}{Classificatori discriminativi}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Classificazione lineare e non lineare}{68}{subsection.7.5.1}\protected@file@percent }
\newlabel{subsec:lin-nonlin}{{7.5.1}{68}{Classificazione lineare e non lineare}{subsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Perceptron}{68}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:perceptron}{{7.5.2}{68}{Perceptron}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Definizione.}{68}{section*.104}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regola di aggiornamento.}{68}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Proprietà.}{68}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{68}{section*.107}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One–Vs–One (OVO).}{69}{section*.108}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One–Vs–All (OVA).}{69}{section*.109}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Support Vector Machines (SVM)}{70}{subsection.7.5.3}\protected@file@percent }
\newlabel{subsec:svm}{{7.5.3}{70}{Support Vector Machines (SVM)}{subsection.7.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Esempio di Support Vector Machine che massimizza il margine tra due classi. I punti cerchiati sono i support vectors che definiscono l'iperpiano ottimale.\relax }}{70}{figure.caption.110}\protected@file@percent }
\newlabel{fig:svm-margin}{{7.6}{70}{Esempio di Support Vector Machine che massimizza il margine tra due classi. I punti cerchiati sono i support vectors che definiscono l'iperpiano ottimale.\relax }{figure.caption.110}{}}
\@writefile{toc}{\contentsline {paragraph}{Formulazione del problema.}{70}{section*.111}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Iperpiani con pesi $\mathbf  {w}$ normalizzati per margine unitario.\relax }}{71}{figure.caption.112}\protected@file@percent }
\newlabel{fig:svm-margin-normalized}{{7.7}{71}{Iperpiani con pesi $\mathbf {w}$ normalizzati per margine unitario.\relax }{figure.caption.112}{}}
\@writefile{toc}{\contentsline {paragraph}{SVM Soft margin.}{72}{section*.113}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Kernel Trick.}{73}{section*.114}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVM Multi-classe.}{74}{section*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Apprendimento Lazy}{74}{section.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}K-Nearest Neighbor}{74}{subsection.7.6.1}\protected@file@percent }
\newlabel{subsec:knn}{{7.6.1}{74}{K-Nearest Neighbor}{subsection.7.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Esempio di classificazione con K-NN (k=5). Il punto da classificare (in verde) è circondato dal cerchio che indica i $k$ vicini più prossimi: tra i 5 vicini ci sono tre triangoli rossi e due quadrati blu, quindi per maggioranza il punto viene assegnato alla classe rossa.\relax }}{74}{figure.caption.116}\protected@file@percent }
\newlabel{fig:knn-example}{{7.8}{74}{Esempio di classificazione con K-NN (k=5). Il punto da classificare (in verde) è circondato dal cerchio che indica i $k$ vicini più prossimi: tra i 5 vicini ci sono tre triangoli rossi e due quadrati blu, quindi per maggioranza il punto viene assegnato alla classe rossa.\relax }{figure.caption.116}{}}
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{75}{section*.117}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Varianti.}{75}{section*.118}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Ensemble Learning}{75}{section.7.7}\protected@file@percent }
\newlabel{sec:ensemble}{{7.7}{75}{Ensemble Learning}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Bagging}{75}{subsection.7.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{75}{section*.119}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Forest.}{75}{section*.120}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Schema di Random Forest: dai dati di training $(X)$ si costruiscono $k$ alberi decisionali $T_1(X,\alpha _1)$, $T_2(X,\alpha _2)$, $\dots  $, $T_k(X,\alpha _k)$, ciascuno con parametri $\alpha _i$ diversi. La predizione finale è ottenuta combinando i risultati attraverso voting o averaging ($\DOTSB \sum@ \slimits@ T_i(X,\alpha _i)$).\relax }}{76}{figure.caption.121}\protected@file@percent }
\newlabel{fig:random-forest}{{7.9}{76}{Schema di Random Forest: dai dati di training $(X)$ si costruiscono $k$ alberi decisionali $T_1(X,\alpha _1)$, $T_2(X,\alpha _2)$, $\dots $, $T_k(X,\alpha _k)$, ciascuno con parametri $\alpha _i$ diversi. La predizione finale è ottenuta combinando i risultati attraverso voting o averaging ($\sum T_i(X,\alpha _i)$).\relax }{figure.caption.121}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Boosting}{76}{subsection.7.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{76}{section*.122}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Schema di Boosting: ogni classificatore viene addestrato su dati pesati in base agli errori dei modelli precedenti. I dati mal classificati ricevono peso maggiore, così i modelli successivi si concentrano sulle istanze più difficili. Le predizioni finali sono combinate in un ensemble ponderato.\relax }}{77}{figure.caption.123}\protected@file@percent }
\newlabel{fig:boosting-example}{{7.10}{77}{Schema di Boosting: ogni classificatore viene addestrato su dati pesati in base agli errori dei modelli precedenti. I dati mal classificati ricevono peso maggiore, così i modelli successivi si concentrano sulle istanze più difficili. Le predizioni finali sono combinate in un ensemble ponderato.\relax }{figure.caption.123}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Adaboost}{77}{subsection.7.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gini Index Pesato.}{77}{section*.124}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Peso dello stump.}{78}{section*.125}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aggiornamento dei pesi.}{78}{section*.126}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Andamento dell'incremento e del decremento dei pesi delle tuple per Adaboost.\relax }}{78}{figure.caption.127}\protected@file@percent }
\newlabel{fig:adaboost-weight-changes}{{7.11}{78}{Andamento dell'incremento e del decremento dei pesi delle tuple per Adaboost.\relax }{figure.caption.127}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Validazione di un classificatore}{79}{section.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Matrice di confusione}{79}{subsection.7.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Esempio di matrice di confusione per un classificatore a tre classi (Gatto, Cane, Coniglio): le righe indicano le classi reali, le colonne le classi predette. I numeri nelle celle sono le frequenze assolute; le somme marginali mostrano il totale per riga/colonna. Un buon classificatore presenta valori elevati sulla diagonale principale (corretta assegnazione).\relax }}{79}{figure.caption.129}\protected@file@percent }
\newlabel{fig:confusion-matrix}{{7.12}{79}{Esempio di matrice di confusione per un classificatore a tre classi (Gatto, Cane, Coniglio): le righe indicano le classi reali, le colonne le classi predette. I numeri nelle celle sono le frequenze assolute; le somme marginali mostrano il totale per riga/colonna. Un buon classificatore presenta valori elevati sulla diagonale principale (corretta assegnazione).\relax }{figure.caption.129}{}}
\@writefile{toc}{\contentsline {paragraph}{Misure di accuratezza con due classi.}{79}{section*.130}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Misure di accuratezza (due classi).}{79}{section*.131}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Soglia discriminativa in un classificatore binario}{80}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Curva ROC}{80}{subsection.7.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Curva di Precision-Recall}{80}{subsection.7.8.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Esempio di curva ROC al variare della soglia $\sigma $.\relax }}{81}{figure.caption.132}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}Validazione di un classificatore}{81}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Cenni di Regressione}{83}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Regressione lineare semplice}{83}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Formulazione del modello}{83}{subsection.8.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Stima dei parametri}{84}{subsection.8.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Interpretazione geoemetrica}{84}{subsection.8.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Regressione lineare multipla}{84}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Formulazione del modello}{84}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Stima dei parametri}{85}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Interpretazione geometrica}{85}{subsection.8.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Regressione non lineare}{85}{section.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Regressione logistica}{85}{section.8.4}\protected@file@percent }
\abx@aux@cite{0}{Leskovec2014MMDS}
\abx@aux@segm{0}{0}{Leskovec2014MMDS}
\abx@aux@cite{0}{James2021ISL2}
\abx@aux@segm{0}{0}{James2021ISL2}
\abx@aux@cite{0}{Hastie2009ESL2}
\abx@aux@segm{0}{0}{Hastie2009ESL2}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Regressione logistica binaria semplice}{86}{subsection.8.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stima dei parametri.}{86}{section*.133}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Subgraph Matching}{87}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Isomorfismo di Grafi}{87}{section.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Esempio di subgraph mapping: il grafo di query (a sinistra) viene trovato all'interno del grafo di dati (a destra).\relax }}{87}{figure.caption.135}\protected@file@percent }
\newlabel{fig:subgraph_mapping_example}{{9.1}{87}{Esempio di subgraph mapping: il grafo di query (a sinistra) viene trovato all'interno del grafo di dati (a destra).\relax }{figure.caption.135}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Automorfismo}{87}{subsection.9.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Esempi di \emph  {automorfismi} del grafo \(G\): a sinistra il grafo originale; al centro un automorfismo che scambia i vertici \(2\) e \(3\) mantenendo \(1\) fisso; a destra un secondo automorfismo che permuta i vertici come indicato dalle frecce tratteggiate. In entrambi i casi la struttura di adiacenza è preservata.\relax }}{88}{figure.caption.136}\protected@file@percent }
\newlabel{fig:automorphism_example}{{9.2}{88}{Esempi di \emph {automorfismi} del grafo \(G\): a sinistra il grafo originale; al centro un automorfismo che scambia i vertici \(2\) e \(3\) mantenendo \(1\) fisso; a destra un secondo automorfismo che permuta i vertici come indicato dalle frecce tratteggiate. In entrambi i casi la struttura di adiacenza è preservata.\relax }{figure.caption.136}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Operazione di subgraph matching}{88}{section.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Esempio di \emph  {subgraph matching}: il grafo \emph  {Query} (in alto a sinistra) viene ricercato all'interno del grafo \emph  {Target} (in alto a destra). In basso sono mostrati i possibili match del sottografo query all'interno del grafo target, evidenziati dai riquadri rossi.\relax }}{88}{figure.caption.137}\protected@file@percent }
\newlabel{fig:subgraph_matching_example}{{9.3}{88}{Esempio di \emph {subgraph matching}: il grafo \emph {Query} (in alto a sinistra) viene ricercato all'interno del grafo \emph {Target} (in alto a destra). In basso sono mostrati i possibili match del sottografo query all'interno del grafo target, evidenziati dai riquadri rossi.\relax }{figure.caption.137}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Complessità computazionale}{89}{section.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Algoritmi di subgraph matching}{89}{section.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Soluzione Bruteforce}{89}{subsection.9.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Ricerca \emph  {brute force} per \emph  {subgraph matching}: in alto i grafi \(G_a\) (query) e \(G_b\) (target); sotto l'albero di ricerca che esplora tutte le corrispondenze possibili tra vertici (es.\ \((1,4)\), \((1,5)\), \((1,6)\), \((1,7)\)). Le foglie evidenziate indicano i mapping completi che preservano le adiacenze (match isomorfi).\relax }}{89}{figure.caption.138}\protected@file@percent }
\newlabel{fig:bruteforce_subgraph_matching}{{9.4}{89}{Ricerca \emph {brute force} per \emph {subgraph matching}: in alto i grafi \(G_a\) (query) e \(G_b\) (target); sotto l'albero di ricerca che esplora tutte le corrispondenze possibili tra vertici (es.\ \((1,4)\), \((1,5)\), \((1,6)\), \((1,7)\)). Le foglie evidenziate indicano i mapping completi che preservano le adiacenze (match isomorfi).\relax }{figure.caption.138}{}}
\@writefile{toc}{\contentsline {paragraph}{Esempio di backtracking.}{90}{section*.139}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Esempio di \emph  {backtracking} con pruning nel \emph  {subgraph matching}. A sinistra il grafo \emph  {query} e il grafo \emph  {target}; a destra l'albero di ricerca. Il ramo che mappa \((3\!\to \!c)\) viene potato perché viola un vincolo di grado (\(\deg (3)=3>\deg (c)=2\)), quindi non può condurre a una soluzione.\relax }}{90}{figure.caption.140}\protected@file@percent }
\newlabel{fig:backtracking_subgraph_matching}{{9.5}{90}{Esempio di \emph {backtracking} con pruning nel \emph {subgraph matching}. A sinistra il grafo \emph {query} e il grafo \emph {target}; a destra l'albero di ricerca. Il ramo che mappa \((3\!\to \!c)\) viene potato perché viola un vincolo di grado (\(\deg (3)=3>\deg (c)=2\)), quindi non può condurre a una soluzione.\relax }{figure.caption.140}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Algoritmo di Ullmann}{90}{subsection.9.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Algoritmo di Ullmann: a sinistra il grafo \emph  {query} \(Q\) e il grafo \emph  {target} \(T\); a destra l'albero di ricerca con le corrispondenze provate. I nodi cerchiati in rosso indicano scelte scartate tramite \emph  {pruning}; il ramo con \((3,7)\) completa un mapping valido.\relax }}{91}{figure.caption.141}\protected@file@percent }
\newlabel{fig:ullmann_algorithm_example}{{9.6}{91}{Algoritmo di Ullmann: a sinistra il grafo \emph {query} \(Q\) e il grafo \emph {target} \(T\); a destra l'albero di ricerca con le corrispondenze provate. I nodi cerchiati in rosso indicano scelte scartate tramite \emph {pruning}; il ramo con \((3,7)\) completa un mapping valido.\relax }{figure.caption.141}{}}
\@writefile{toc}{\contentsline {paragraph}{Esempio della figura \ref {fig:ullmann_algorithm_example}.}{91}{section*.142}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Algoritmo VF}{91}{subsection.9.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{92}{section*.143}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regola di fattibilità per grafi indiretti.}{92}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regola di fattibilità per grafi diretti.}{92}{section*.146}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Rappresentazione grafica dei sei insiemi utilizzati dall'algoritmo VF per verificare la regola di fattibilità durante la costruzione incrementale della mappatura tra nodi del grafo query $Q$ e nodi del grafo target $T$. La figura mostra, per uno stato $s$ dell'algoritmo, la suddivisione di entrambi i grafi nei tre insiemi fondamentali: i nodi già mappati ($\text  {Match}_Q(s)$ e $\text  {Match}_T(s)$), i nodi adiacenti a quelli mappati ($\text  {Term}_Q(s)$ e $\text  {Term}_T(s)$) e i nodi non ancora considerati ($\text  {Rem}_Q(s)$ e $\text  {Rem}_T(s)$). Le frecce tratteggiate indicano le possibili connessioni tra nodi dei diversi insiemi, mentre le frecce orizzontali centrali rappresentano la mappatura parziale $M(s)$ costruita dallo stato corrente. Questa struttura consente di verificare la regola di fattibilità: ogni nuova coppia $(q,t)$ può essere aggiunta alla mappatura se preserva le adiacenze verso i nodi già mappati, se esistono sufficienti nodi adiacenti disponibili (look-ahead a un livello) e se esistono sufficienti nodi non ancora considerati per supportare eventuali mappature future (look-ahead a due livelli).\relax }}{93}{figure.caption.145}\protected@file@percent }
\newlabel{fig:vf_algorithm_example}{{9.7}{93}{Rappresentazione grafica dei sei insiemi utilizzati dall'algoritmo VF per verificare la regola di fattibilità durante la costruzione incrementale della mappatura tra nodi del grafo query $Q$ e nodi del grafo target $T$. La figura mostra, per uno stato $s$ dell'algoritmo, la suddivisione di entrambi i grafi nei tre insiemi fondamentali: i nodi già mappati ($\text {Match}_Q(s)$ e $\text {Match}_T(s)$), i nodi adiacenti a quelli mappati ($\text {Term}_Q(s)$ e $\text {Term}_T(s)$) e i nodi non ancora considerati ($\text {Rem}_Q(s)$ e $\text {Rem}_T(s)$). Le frecce tratteggiate indicano le possibili connessioni tra nodi dei diversi insiemi, mentre le frecce orizzontali centrali rappresentano la mappatura parziale $M(s)$ costruita dallo stato corrente. Questa struttura consente di verificare la regola di fattibilità: ogni nuova coppia $(q,t)$ può essere aggiunta alla mappatura se preserva le adiacenze verso i nodi già mappati, se esistono sufficienti nodi adiacenti disponibili (look-ahead a un livello) e se esistono sufficienti nodi non ancora considerati per supportare eventuali mappature future (look-ahead a due livelli).\relax }{figure.caption.145}{}}
\@writefile{toc}{\contentsline {paragraph}{Complessità e considerazioni}{94}{section*.147}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Algoritmo VF2}{94}{subsection.9.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}Algoritmo RI}{95}{subsection.9.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ordinamento dei nodi.}{95}{section*.148}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Grafo non diretto di esempio (11 nodi, 0-10), usato per illustrare l'ordinamento dei nodi della query nell'algoritmo RI.\relax }}{96}{figure.caption.150}\protected@file@percent }
\newlabel{fig:ri_algorithm_example}{{9.8}{96}{Grafo non diretto di esempio (11 nodi, 0-10), usato per illustrare l'ordinamento dei nodi della query nell'algoritmo RI.\relax }{figure.caption.150}{}}
\@writefile{toc}{\contentsline {paragraph}{Passo $i=1$.}{96}{section*.151}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Passo $i=2$.}{96}{section*.152}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tabella finale}{97}{section*.154}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.6}RI-DS}{97}{subsection.9.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Subgraph Matching in Database di Grafi}{97}{section.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Indicizzazione}{98}{subsection.9.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Features dei grafi}{98}{section.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio di filtraggio tramite profili di feature.}{98}{figure.caption.156}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Indicizzazione \emph  {feature-based} per il filtraggio dei candidati. A sinistra: dalla query \(Q\) si estraggono piccole caratteristiche (feature) e si costruisce il profilo di frequenza \(F_Q\). A destra: per ogni grafo del database (es.\ \(G\)) è precomputato il profilo \(F_G\).\relax }}{99}{figure.caption.156}\protected@file@percent }
\newlabel{fig:feature_indexing_example}{{9.9}{99}{Indicizzazione \emph {feature-based} per il filtraggio dei candidati. A sinistra: dalla query \(Q\) si estraggono piccole caratteristiche (feature) e si costruisce il profilo di frequenza \(F_Q\). A destra: per ogni grafo del database (es.\ \(G\)) è precomputato il profilo \(F_G\).\relax }{figure.caption.156}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Schema di subgraph matching in database di grafi}{99}{subsection.9.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}Indicizzazione inversa}{100}{subsection.9.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.3}Algoritmo SING}{100}{subsection.9.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Query \(Q\) (angolo con nodo \(A\) adiacente a \(B\) e \(C\)) e due grafi candidati. Nel grafo \(g_1\) il pattern è presente come \emph  {subgrafo non indotto}, ma non come \emph  {indotto} a causa dell'arco diagonale aggiuntivo; nel grafo \(g_2\) il pattern è presente anche come \emph  {subgrafo indotto}.\relax }}{100}{figure.caption.157}\protected@file@percent }
\newlabel{fig:sing_algorithm_example}{{9.10}{100}{Query \(Q\) (angolo con nodo \(A\) adiacente a \(B\) e \(C\)) e due grafi candidati. Nel grafo \(g_1\) il pattern è presente come \emph {subgrafo non indotto}, ma non come \emph {indotto} a causa dell'arco diagonale aggiuntivo; nel grafo \(g_2\) il pattern è presente anche come \emph {subgrafo indotto}.\relax }{figure.caption.157}{}}
\@writefile{toc}{\contentsline {paragraph}{Processamento della query.}{100}{section*.159}\protected@file@percent }
\abx@aux@cite{0}{Cordella1999VF}
\abx@aux@segm{0}{0}{Cordella1999VF}
\abx@aux@cite{0}{Cordella2004VF2}
\abx@aux@segm{0}{0}{Cordella2004VF2}
\abx@aux@cite{0}{Bonnici2013RI}
\abx@aux@segm{0}{0}{Bonnici2013RI}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Indicizzazione inversa globale e locale per l'algoritmo SING. A sinistra: indice inverso globale che associa ad ogni feature a lista dei grafi che la contengono e il conteggio delle occorrenze. A destra: indice inverso locale per il grafo \(g_1\), dove per ogni feature è indicato un vettore binario che mostra i nodi di partenza della feature in \(g_1\).\relax }}{101}{figure.caption.158}\protected@file@percent }
\newlabel{fig:sing_indexing_example}{{9.11}{101}{Indicizzazione inversa globale e locale per l'algoritmo SING. A sinistra: indice inverso globale che associa ad ogni feature a lista dei grafi che la contengono e il conteggio delle occorrenze. A destra: indice inverso locale per il grafo \(g_1\), dove per ogni feature è indicato un vettore binario che mostra i nodi di partenza della feature in \(g_1\).\relax }{figure.caption.158}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Subgraph Matching di Grafi Frequenti}{103}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Esempio di estrazione di sotto-grafi frequenti da un database di grafi.\relax }}{103}{figure.caption.161}\protected@file@percent }
\newlabel{fig:frequent-subgraph-matching}{{10.1}{103}{Esempio di estrazione di sotto-grafi frequenti da un database di grafi.\relax }{figure.caption.161}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Esempio di estrazione di sotto-grafi frequenti basata sulla frequenza nei grafi del database.\relax }}{104}{figure.caption.162}\protected@file@percent }
\newlabel{fig:frequent-subgraph-frequency}{{10.2}{104}{Esempio di estrazione di sotto-grafi frequenti basata sulla frequenza nei grafi del database.\relax }{figure.caption.162}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Algoritmo FSG}{104}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Regola Apriori per sotto-grafi}{104}{subsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Join tra sotto-grafi}{105}{subsection.10.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Join tra due sottografi che differiscono per un nodo.\relax }}{105}{figure.caption.164}\protected@file@percent }
\newlabel{fig:fsg-join-1}{{10.3}{105}{Join tra due sottografi che differiscono per un nodo.\relax }{figure.caption.164}{}}
\@writefile{toc}{\contentsline {paragraph}{Scenario 1: i due sottografi differiscono per un nodo.}{105}{figure.caption.164}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scenario 2: il grafo \emph  {core} ha più automorfismi.}{105}{figure.caption.166}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Join tra due sottografi con grafo core avente più automorfismi.\relax }}{106}{figure.caption.166}\protected@file@percent }
\newlabel{fig:fsg-join-2}{{10.4}{106}{Join tra due sottografi con grafo core avente più automorfismi.\relax }{figure.caption.166}{}}
\@writefile{toc}{\contentsline {paragraph}{Scenario 3: i sottografi candidati hanno più grafi \emph  {core} in comune.}{106}{figure.caption.168}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Join tra due sottografi con più grafi core in comune.\relax }}{107}{figure.caption.168}\protected@file@percent }
\newlabel{fig:fsg-join-3}{{10.5}{107}{Join tra due sottografi con più grafi core in comune.\relax }{figure.caption.168}{}}
\@writefile{toc}{\contentsline {paragraph}{Caso generale.}{108}{section*.169}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Procedura dell'algoritmo}{108}{subsection.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Generazione dei candidati}{108}{subsection.10.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Stringa di adiacenza}{109}{subsection.10.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Esempio di stringa di adiacenza di un grafo.\relax }}{109}{figure.caption.170}\protected@file@percent }
\newlabel{fig:adjacency-string-example}{{10.6}{109}{Esempio di stringa di adiacenza di un grafo.\relax }{figure.caption.170}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Forma canonica}{109}{subsection.10.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algoritmo.}{109}{section*.171}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{109}{section*.172}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Esempio di calcolo della forma canonica di un grafo.\relax }}{110}{figure.caption.173}\protected@file@percent }
\newlabel{fig:canonical-form-example}{{10.7}{110}{Esempio di calcolo della forma canonica di un grafo.\relax }{figure.caption.173}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.7}Verifica della regola Apriori}{110}{subsection.10.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Algoritmo gSpan}{111}{section.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Visita DFS}{111}{subsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Albero DFS.}{111}{section*.175}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Codifica DFS}{111}{subsection.10.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Esempio di visita in profondità (DFS) su un grafo diretto. I numeri all'interno dei nodi indicano i tempi di scoperta e di completamento (d/f) di ciascun vertice. I nodi colorati in grigio o nero rappresentano rispettivamente i vertici scoperti e completati, mentre le frecce tratteggiate indicano archi di ritorno o di attraversamento classificati durante l'esecuzione.\relax }}{112}{figure.caption.174}\protected@file@percent }
\newlabel{fig:dfs_visit_example}{{10.8}{112}{Esempio di visita in profondità (DFS) su un grafo diretto. I numeri all'interno dei nodi indicano i tempi di scoperta e di completamento (d/f) di ciascun vertice. I nodi colorati in grigio o nero rappresentano rispettivamente i vertici scoperti e completati, mentre le frecce tratteggiate indicano archi di ritorno o di attraversamento classificati durante l'esecuzione.\relax }{figure.caption.174}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces A sinistra il grafo di partenza; a destra alcuni alberi di visita in profondità (DFS) ottenuti con diversi ordini di esplorazione. Gli archi forward (dell'albero) sono in nero continuo, mentre gli archi backward sono tratteggiati, come indicato in legenda.\relax }}{112}{figure.caption.176}\protected@file@percent }
\newlabel{fig:dfs_tree_example}{{10.9}{112}{A sinistra il grafo di partenza; a destra alcuni alberi di visita in profondità (DFS) ottenuti con diversi ordini di esplorazione. Gli archi forward (dell'albero) sono in nero continuo, mentre gli archi backward sono tratteggiati, come indicato in legenda.\relax }{figure.caption.176}{}}
\@writefile{toc}{\contentsline {paragraph}{Costruzione della codifica DFS.}{113}{section*.177}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Codifica degli archi per i tre schemi (b)~$\alpha $, (c)~$\beta $ e (d)~$\gamma $.\relax }}{113}{table.caption.178}\protected@file@percent }
\newlabel{tab:dfs_code_example}{{10.1}{113}{Codifica degli archi per i tre schemi (b)~$\alpha $, (c)~$\beta $ e (d)~$\gamma $.\relax }{table.caption.178}{}}
\@writefile{toc}{\contentsline {paragraph}{Codice DFS minimo.}{113}{section*.179}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{DFS Code Tree.}{113}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Generazione dei candidati}{114}{subsection.10.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Esempio di generazione dei candidati in \textsc  {gSpan}. (a) Sottografo frequente di partenza. I candidati con $k\!+\!1$ archi si ottengono aggiungendo un solo arco secondo la regola del \emph  {cammino più a destra} del codice DFS minimo: (b-d) aggiunte \emph  {backward} da un nodo del cammino più a destra verso un antenato; (e-f) aggiunte \emph  {forward} dal nodo foglia del cammino più a destra verso un nuovo nodo. Gli archi tratteggiati indicano l'arco aggiunto. La riga inferiore (b.0-b.3, e.0-e.2) illustra le varianti non ridondanti prodotte per ciascuna estensione rispettando l'ordine del codice DFS minimo.\relax }}{114}{figure.caption.181}\protected@file@percent }
\newlabel{fig:gspan_candidate_generation}{{10.10}{114}{Esempio di generazione dei candidati in \textsc {gSpan}. (a) Sottografo frequente di partenza. I candidati con $k\!+\!1$ archi si ottengono aggiungendo un solo arco secondo la regola del \emph {cammino più a destra} del codice DFS minimo: (b-d) aggiunte \emph {backward} da un nodo del cammino più a destra verso un antenato; (e-f) aggiunte \emph {forward} dal nodo foglia del cammino più a destra verso un nuovo nodo. Gli archi tratteggiati indicano l'arco aggiunto. La riga inferiore (b.0-b.3, e.0-e.2) illustra le varianti non ridondanti prodotte per ciascuna estensione rispettando l'ordine del codice DFS minimo.\relax }{figure.caption.181}{}}
\abx@aux@cite{0}{CookHolder2006}
\abx@aux@segm{0}{0}{CookHolder2006}
\@writefile{toc}{\contentsline {paragraph}{Pruning dello spazio di ricerca.}{115}{section*.182}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Elementi di Reti neurali}{117}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Confronto tra un neurone biologico (a sinistra) e un neurone artificiale (a destra). Nel neurone biologico, il segnale si propaga dai dendriti, attraverso il soma e lungo l'assone fino ai terminali sinaptici. Nel neurone artificiale, gli ingressi $x_i$ vengono pesati con i corrispondenti pesi $w_i$, sommati e combinati con un termine di bias; il risultato $z_j$ viene poi trasformato da una funzione di attivazione per generare l'uscita del neurone.\relax }}{117}{figure.caption.184}\protected@file@percent }
\newlabel{fig:nn_neuron_comparison}{{11.1}{117}{Confronto tra un neurone biologico (a sinistra) e un neurone artificiale (a destra). Nel neurone biologico, il segnale si propaga dai dendriti, attraverso il soma e lungo l'assone fino ai terminali sinaptici. Nel neurone artificiale, gli ingressi $x_i$ vengono pesati con i corrispondenti pesi $w_i$, sommati e combinati con un termine di bias; il risultato $z_j$ viene poi trasformato da una funzione di attivazione per generare l'uscita del neurone.\relax }{figure.caption.184}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Strati}{118}{section.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Esempio di una rete neurale con 1 input layers, 4 hidden layers e e 1 output layer.\relax }}{118}{figure.caption.185}\protected@file@percent }
\newlabel{fig:neural_network_example}{{11.2}{118}{Esempio di una rete neurale con 1 input layers, 4 hidden layers e e 1 output layer.\relax }{figure.caption.185}{}}
\@writefile{toc}{\contentsline {paragraph}{Deep neural network.}{118}{section*.186}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Connessioni tra layer}{119}{subsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Progettare una rete neurale}{119}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Funzioni di attivazione}{119}{section.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Proprietà.}{120}{section*.187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Funzione step}{120}{subsection.11.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Grafico della funzione step.\relax }}{120}{figure.caption.188}\protected@file@percent }
\newlabel{fig:step_function}{{11.3}{120}{Grafico della funzione step.\relax }{figure.caption.188}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Esempio di un perceptron con 4 input, pesi associati e bias. L'output viene calcolato applicando la funzione step alla somma pesata degli input più il bias.\relax }}{121}{figure.caption.189}\protected@file@percent }
\newlabel{fig:perceptron}{{11.4}{121}{Esempio di un perceptron con 4 input, pesi associati e bias. L'output viene calcolato applicando la funzione step alla somma pesata degli input più il bias.\relax }{figure.caption.189}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Funzione logistica}{121}{subsection.11.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces Funzione sigmoide $\sigma (x)$ (blu) e sua derivata $\sigma '(x)$ (rosso tratteggiato). La derivata raggiunge il massimo in \(x=0\) (\(0{.}25\)); per \(|x|\gg 0\) la sigmoide satura e il gradiente tende a zero.\relax }}{121}{figure.caption.190}\protected@file@percent }
\newlabel{fig:sigmoid-derivative}{{11.5}{121}{Funzione sigmoide $\sigma (x)$ (blu) e sua derivata $\sigma '(x)$ (rosso tratteggiato). La derivata raggiunge il massimo in \(x=0\) (\(0{.}25\)); per \(|x|\gg 0\) la sigmoide satura e il gradiente tende a zero.\relax }{figure.caption.190}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}Tangente iperbolica}{122}{subsection.11.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces Funzione tangente iperbolica $\tanh (x)$: attivazione dispari, centrata in \(0\) con range \((-1,1)\). Satura verso \(\pm 1\) per \(|x|\) grandi; derivata $tanh'(x)$ massima in \(x=0\), utile per ridurre il bias shift rispetto alla sigmoide.\relax }}{122}{figure.caption.191}\protected@file@percent }
\newlabel{fig:tanh-derivative}{{11.6}{122}{Funzione tangente iperbolica $\tanh (x)$: attivazione dispari, centrata in \(0\) con range \((-1,1)\). Satura verso \(\pm 1\) per \(|x|\) grandi; derivata $tanh'(x)$ massima in \(x=0\), utile per ridurre il bias shift rispetto alla sigmoide.\relax }{figure.caption.191}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Funzione softmax}{122}{subsection.11.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}ReLU: Rectified Linear Unit}{123}{subsection.11.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variante ELU.}{124}{section*.192}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces Grafici della funzione ReLU (rosso), della sua variante Leaky ReLU (verde) che permette un piccolo gradiente per input negativi, e della ELU (blu) che introduce una componente esponenziale per input negativi.\relax }}{124}{figure.caption.193}\protected@file@percent }
\newlabel{fig:relu_function}{{11.7}{124}{Grafici della funzione ReLU (rosso), della sua variante Leaky ReLU (verde) che permette un piccolo gradiente per input negativi, e della ELU (blu) che introduce una componente esponenziale per input negativi.\relax }{figure.caption.193}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Funzioni Loss}{124}{section.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Regression Loss}{124}{subsection.11.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces Confronto tra la squared error loss (tratteggiata, crescita quadratica) e la Huber loss (linea continua, crescita lineare oltre la soglia): la Huber loss penalizza meno fortemente gli errori molto grandi, risultando più robusta agli outlier.\relax }}{125}{figure.caption.194}\protected@file@percent }
\newlabel{fig:regression_loss}{{11.8}{125}{Confronto tra la squared error loss (tratteggiata, crescita quadratica) e la Huber loss (linea continua, crescita lineare oltre la soglia): la Huber loss penalizza meno fortemente gli errori molto grandi, risultando più robusta agli outlier.\relax }{figure.caption.194}{}}
\@writefile{toc}{\contentsline {paragraph}{MSE: Mean Squared Error.}{125}{section*.195}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}Classification Loss}{126}{subsection.11.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entropia.}{126}{section*.196}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Entropia incrociata.}{126}{section*.197}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Training di una rete neurale}{127}{section.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}Ottimizzazione dei pesi}{127}{subsection.11.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}Metodo di discesa del gradiente}{127}{subsection.11.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Andamento della funzione di loss al variare del \emph  {learning rate}. Un valore troppo elevato può causare instabilità o divergenza dell'addestramento, mentre un valore troppo basso rallenta significativamente la convergenza. Un \emph  {learning rate} ottimale permette una discesa rapida e stabile verso il minimo.\relax }}{128}{figure.caption.198}\protected@file@percent }
\newlabel{fig:eta_examples}{{11.9}{128}{Andamento della funzione di loss al variare del \emph {learning rate}. Un valore troppo elevato può causare instabilità o divergenza dell'addestramento, mentre un valore troppo basso rallenta significativamente la convergenza. Un \emph {learning rate} ottimale permette una discesa rapida e stabile verso il minimo.\relax }{figure.caption.198}{}}
\@writefile{toc}{\contentsline {paragraph}{Matrice Jacobiana.}{129}{section*.199}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochastic Gradient Descent.}{129}{section*.200}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}Esempio di computazione}{129}{subsection.11.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Grafo computazionale di una rete neurale feed-forward a singolo strato, con ingresso $\mathbf  {x}$, pesi $W$, bias $b$, attivazione $\sigma $ e perdita MSE rispetto al target $\hat  {y}$.\relax }}{130}{figure.caption.201}\protected@file@percent }
\newlabel{fig:computation_graph_nn}{{11.10}{130}{Grafo computazionale di una rete neurale feed-forward a singolo strato, con ingresso $\mathbf {x}$, pesi $W$, bias $b$, attivazione $\sigma $ e perdita MSE rispetto al target $\hat {y}$.\relax }{figure.caption.201}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.4}Backpropagation}{130}{subsection.11.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regola della catena.}{130}{section*.202}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{131}{section*.203}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Grafo di backpropagation per la rete neurale in figura \ref {fig:computation_graph_nn}.\relax }}{131}{figure.caption.204}\protected@file@percent }
\newlabel{fig:backpropagation_example}{{11.11}{131}{Grafo di backpropagation per la rete neurale in figura \ref {fig:computation_graph_nn}.\relax }{figure.caption.204}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}Tecniche di Regolarizzazione}{133}{section.11.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}Regolarizzazione L1 e L2}{133}{subsection.11.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}Dropout}{134}{subsection.11.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Esempio di applicazione del dropout in una rete neurale: durante l'addestramento, alcuni neuroni (in grigio) vengono "spenti" casualmente, costringendo la rete a non dipendere troppo da singoli neuroni e a imparare rappresentazioni più robuste. Durante il test, tutti i neuroni sono attivi, ma i pesi vengono scalati per tenere conto del dropout.\relax }}{134}{figure.caption.205}\protected@file@percent }
\newlabel{fig:dropout}{{11.12}{134}{Esempio di applicazione del dropout in una rete neurale: durante l'addestramento, alcuni neuroni (in grigio) vengono "spenti" casualmente, costringendo la rete a non dipendere troppo da singoli neuroni e a imparare rappresentazioni più robuste. Durante il test, tutti i neuroni sono attivi, ma i pesi vengono scalati per tenere conto del dropout.\relax }{figure.caption.205}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Early Stopping}{134}{subsection.11.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Aumento del training set}{134}{subsection.11.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.7}Tipi di reti neurali}{135}{section.11.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Feed-Forward Networks}{135}{subsection.11.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}Reti Neurali Convoluzionali}{135}{subsection.11.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces Schema di una rete neurale convoluzionale per la classificazione di immagini: dall'input grezzo si susseguono layer di convoluzione + ReLU e pooling per l'estrazione gerarchica delle feature, seguiti da livelli fully connected e da un output softmax che produce le probabilità per ciascuna classe (es.\ car, truck, van, bicycle).\relax }}{135}{figure.caption.206}\protected@file@percent }
\newlabel{fig:cnn_architecture}{{11.13}{135}{Schema di una rete neurale convoluzionale per la classificazione di immagini: dall'input grezzo si susseguono layer di convoluzione + ReLU e pooling per l'estrazione gerarchica delle feature, seguiti da livelli fully connected e da un output softmax che produce le probabilità per ciascuna classe (es.\ car, truck, van, bicycle).\relax }{figure.caption.206}{}}
\@writefile{toc}{\contentsline {paragraph}{Fotorecettori.}{136}{section*.207}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layer.}{136}{section*.208}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stride.}{136}{section*.209}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Zero Padding.}{136}{section*.210}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pooling Layer.}{136}{section*.211}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CNN su immagini a colori.}{137}{section*.212}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Rete neurali di grafi}{137}{subsection.11.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Schema generale di una rete neurale su grafi. (1) Un blocco \emph  {permutation equivariant} propaga le informazioni sul grafo e aggiorna le rappresentazioni dei nodi; (2) un'operazione di \emph  {local pooling} aggrega le caratteristiche su un sottografo rilevante; (3) un \emph  {global pooling} produce un'unica rappresentazione vettoriale dell'intero grafo.\relax }}{138}{figure.caption.213}\protected@file@percent }
\newlabel{fig:graph_neural_network}{{11.14}{138}{Schema generale di una rete neurale su grafi. (1) Un blocco \emph {permutation equivariant} propaga le informazioni sul grafo e aggiorna le rappresentazioni dei nodi; (2) un'operazione di \emph {local pooling} aggrega le caratteristiche su un sottografo rilevante; (3) un \emph {global pooling} produce un'unica rappresentazione vettoriale dell'intero grafo.\relax }{figure.caption.213}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.4}Reti Neurali Ricorrenti}{138}{subsection.11.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces A sinistra, l'unità base di una rete neurale ricorrente (RNN), in cui lo stato nascosto $\mathbf  {s}$ viene aggiornato combinando l'input corrente $\mathbf  {x}$ con lo stato precedente tramite le matrici di pesi $\mathbf  {U}$, $\mathbf  {W}$ e genera l'output $\mathbf  {y}$ tramite $\mathbf  {V}$. A destra, la stessa RNN “unrolled” per $n$ passi temporali, con la catena di stati $\mathbf  {s}_1,\dots  ,\mathbf  {s}_n$ e dei corrispondenti input $\mathbf  {x}_1,\dots  ,\mathbf  {x}_n$ e output $\mathbf  {y}_1,\dots  ,\mathbf  {y}_n$.\relax }}{138}{figure.caption.214}\protected@file@percent }
\newlabel{fig:rnn_example}{{11.15}{138}{A sinistra, l'unità base di una rete neurale ricorrente (RNN), in cui lo stato nascosto $\mathbf {s}$ viene aggiornato combinando l'input corrente $\mathbf {x}$ con lo stato precedente tramite le matrici di pesi $\mathbf {U}$, $\mathbf {W}$ e genera l'output $\mathbf {y}$ tramite $\mathbf {V}$. A destra, la stessa RNN “unrolled” per $n$ passi temporali, con la catena di stati $\mathbf {s}_1,\dots ,\mathbf {s}_n$ e dei corrispondenti input $\mathbf {x}_1,\dots ,\mathbf {x}_n$ e output $\mathbf {y}_1,\dots ,\mathbf {y}_n$.\relax }{figure.caption.214}{}}
\@writefile{toc}{\contentsline {paragraph}{Input di una RNN.}{138}{section*.215}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.5}LSTM: Long Short-Term Memory}{139}{subsection.11.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces Struttura interna di una cella LSTM. La cella utilizza tre gate principali — \emph  {forget gate}, \emph  {input gate} e \emph  {output gate} — per controllare in modo dinamico il flusso di informazione nello stato interno \(C_t\) e nello stato nascosto \(H_t\). Le attivazioni sigmoide (\(\sigma \)) e tangente iperbolica (\(\tanh \)) regolano rispettivamente la selezione e la trasformazione dei valori.\relax }}{140}{figure.caption.216}\protected@file@percent }
\newlabel{fig:lstm_example}{{11.16}{140}{Struttura interna di una cella LSTM. La cella utilizza tre gate principali — \emph {forget gate}, \emph {input gate} e \emph {output gate} — per controllare in modo dinamico il flusso di informazione nello stato interno \(C_t\) e nello stato nascosto \(H_t\). Le attivazioni sigmoide (\(\sigma \)) e tangente iperbolica (\(\tanh \)) regolano rispettivamente la selezione e la trasformazione dei valori.\relax }{figure.caption.216}{}}
\@writefile{toc}{\contentsline {paragraph}{Aggiornamento della cella di memoria.}{140}{section*.217}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.6}Autoencoder}{142}{subsection.11.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Varianti di autoencoder.}{142}{section*.218}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.7}Variational Autoencoder}{142}{subsection.11.7.7}\protected@file@percent }
\abx@aux@cite{0}{Leskovec2014MMDS}
\abx@aux@segm{0}{0}{Leskovec2014MMDS}
\@writefile{toc}{\contentsline {paragraph}{Loss function dei VAE.}{143}{section*.219}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applicazione di VAE.}{143}{section*.220}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Introduzione a Transformer, LLM e Vector Databases}{145}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Transformer}{145}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Seq2seq}{145}{subsection.12.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitazioni delle RNN.}{145}{section*.223}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Schema di un modello \emph  {encoder--decoder} basato su reti neurali ricorrenti. L'\textbf  {encoder} riceve in ingresso una sequenza $x^{(1)}, x^{(2)}, \ldots  , x^{(T)}$ e la comprime in una rappresentazione latente finale. Il \textbf  {decoder} utilizza tale rappresentazione per generare iterativamente la sequenza di output $y^{(1)}, y^{(2)}, \ldots  , y^{(T')}$, producendo a ogni passo un simbolo di uscita fino al token di fine sequenza (\texttt  {<End>}). Le frecce ricorsive indicano la dipendenza temporale tra gli stati consecutivi.\relax }}{146}{figure.caption.222}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Architettura Encoder - Decoder}{146}{subsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.3}Tokenizzazione ed Embedding}{146}{subsection.12.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Architettura encoder--decoder di un Transformer.\relax }}{147}{figure.caption.224}\protected@file@percent }
\newlabel{fig:transformer-architecture}{{12.2}{147}{Architettura encoder--decoder di un Transformer.\relax }{figure.caption.224}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.4}Encoding posizionale}{147}{subsection.12.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.5}Meccanismo di Self-Attention}{148}{subsection.12.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces Schema del meccanismo di \emph  {self-attention} per una singola testa di attenzione. A partire dagli embedding di input, il modello costruisce tre rappresentazioni ($Q$, $K$ e $V$) tramite proiezioni lineari. Le similarità tra query e key determinano i pesi di attenzione, che vengono normalizzati con una softmax. Tali pesi sono poi utilizzati per combinare i value, producendo embedding finali arricchiti dal contesto della sequenza.\relax }}{148}{figure.caption.225}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single-head vs Multi-head attention.}{149}{section*.226}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}LLM: Large Language Models}{150}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Caratteristiche principali}{150}{subsection.12.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Applicazioni}{150}{subsection.12.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Temperatura.}{150}{section*.227}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Pre-Training e Fine-Tuning}{151}{subsection.12.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}Prompt}{151}{subsection.12.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.5}RAG: Retrieval-Augmented Generation}{151}{subsection.12.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{151}{section*.228}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces Schema del processo di \emph  {Retrieval-Augmented Generation} (RAG). A partire da un prompt e una query, il sistema recupera informazioni rilevanti da fonti di conoscenza esterne, le integra nel contesto e le fornisce al modello di linguaggio, che genera una risposta finale più informata e coerente.\relax }}{152}{figure.caption.229}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Vector Databases}{152}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Splitting in chunk}{152}{subsection.12.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}Memorizzazione nel database vettoriale}{153}{subsection.12.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.3}Graph-RAG}{153}{subsection.12.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Approfondimenti}{155}{part.3}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{71DEABCA5EA0D6AAC9CB3A7A445FCF5D}
\abx@aux@defaultrefcontext{0}{Barabasi2016NetworkScience}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Bonnici2013RI}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{CookHolder2006}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Cordella2004VF2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Cordella1999VF}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Hastie2009ESL2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{James2021ISL2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Leskovec2014MMDS}{nty/global//global/global/global}
\gdef \@abspage@last{165}
