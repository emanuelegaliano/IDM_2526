\chapter{Clustering}
Il clustering è una tecnica di apprendimento \textbf{non supervisionato} che mira a \textbf{raggruppare} un insieme di oggetti in modo che gli oggetti all'interno dello stesso gruppo (o cluster) siano più simili tra loro rispetto a quelli di altri gruppi. Questa tecnica è utile per scoprire strutture nascoste nei dati e per ridurre la dimensionalità.

\section{Spazi metrici e distanze}
Per applicare il clustering, è necessario definire una \textbf{metrica} che misura la \textbf{distanza} tra gli oggetti. Gli oggetti da raggruppare sono, infatti, punti appartenenti a un certo spazio metrico $S$ dove è possibile definire una funzione di distanza $D: S \times S \to \mathbb{R}^+$ che soddisfi le seguenti proprietà:
\begin{enumerate}
    \item \textbf{Non negatività}: $D(x, y) \geq 0$ per ogni $x, y \in S$.
    \item \textbf{Simmetria}: $D(x, y) = D(y, x)$ per ogni $x, y \in S$.
    \item \textbf{Disuguaglianza triangolare}: $D(x, z) \leq D(x, y) + D(y, z)$ per ogni $x, y, z \in S$.
\end{enumerate}

\subsection{Spazio euclideo}
Un esempio di spazio metrico è lo spazio euclideo $\mathbb{R}^n$ con la metrica euclidea, definita come:
\[
D_2(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2} 
\]
dove $x = (x_1, x_2, \ldots, x_n)$ e $y = (y_1, y_2, \ldots, y_n)$ sono due punti in $\mathbb{R}^n$ (si usa $D_2$ per indicare che utilizza la norma $L_2$).


Un problema della distanza euclidea è che utilizza la norma $L_2$, che può non essere adatta per tutti i tipi di dati, specialmente quando le variabili hanno scale diverse o quando i dati contengono outlier. In questi casi, si possono considerare altre metriche come la distanza di Manhattan (con norma $L_1$):
\[
D_1(x, y) = \sum_{i=1}^n |x_i - y_i|
\]

Altre distanze in spazi euclidei includono:
\begin{align*}
&D_r = \left( \sum_{i=1}^n |x_i - y_i|^r \right)^{1/r} &\text{(norma $L_r$)} \\
&D_\infty = \max_{i} |x_i - y_i| &\text{(norma $L_\infty$)} \\
&D_{cos} = 1 - \frac{x \cdot y}{\|x\| \|y\|} &\text{(distanza coseno)}
\end{align*}

\noindent
dove in particolare la distanza coseno è utile per misurare la similarità tra vettori in spazi ad alta dimensione, come nel caso di documenti rappresentati da vettori di frequenze di parole.

\paragraph{Centroide.}
Negli spazi euclidei viene utilizzato il concetto di \textbf{centroide} per rappresentare un cluster. Il centroide di un insieme di punti $X = \{x_1, x_2, \ldots, x_k\}$ è definito come:
\[
c(X) = \frac{1}{k} \sum_{i=1}^k x_i
\]
ovvero $c(X)$ è il punto medio di tutti i punti nel cluster. Il centroide minimizza la somma delle distanze quadrate dai punti del cluster, rendendolo una rappresentazione efficace della posizione centrale del cluster.

\subsection{Spazi non euclidei}
Negli spazi non euclidei il concetto di centroide può non essere definito o utile. In questi casi, si possono utilizzare altre rappresentazioni per i cluster, come il \textbf{medoide}, che è un punto reale del dataset che minimizza la somma delle distanze ai punti del cluster:
\[
m(X) = \arg\min_{x \in X} \sum_{y \in X} D(x, y)
\]
Il medoide è particolarmente utile in spazi dove non ha senso calcolare un centroide, come in spazi discreti o quando i dati sono categoriali. Esempi di distanze non euclidee sono:
\begin{description}
    \item[Distanza di Edit.] - La distanza di edit misura quanto è necessario modificare una stringa per trasformarla in un'altra, considerando operazioni come inserimenti, cancellazioni e sostituzioni di caratteri. Ad esempio partendo da una stringa $A$ e trasformandola in una stringa $B$, la distanza di edit può essere calcolata come il numero minimo di operazioni necessarie per ottenere $B$ da $A$.
    \item[Distanza di Hamming.] - La distanza di Hamming conta il numero di posizioni in cui due stringhe di uguale lunghezza differiscono, ed è utile per dati binari o categoriali. Ad esempio, per le stringhe $A = 10101$ e $B = 10011$, la distanza di Hamming è 2, poiché differiscono nelle posizioni 2 e 4.
    \item[Distanza di Jaccard.] - La distanza di Jaccard misura la dissimilarità tra due insiemi, definita come il rapporto tra l'intersezione e l'unione degli insiemi:
    \[D_J(A, B) = 1 - \frac{|A \cap B|}{|A \cup B|}\]
    Questa distanza è particolarmente utile per dati categoriali o binari, come nel caso di documenti rappresentati da insiemi di parole. Ad esempio, per gli insiemi $A = \{1, 2, 3\}$ e $B = \{2, 3, 4\}$, la distanza di Jaccard è:
    \[D_J(A, B) = 1 - \frac{|\{2, 3\}|}{|\{1, 2, 3, 4\}|} = 1 - \frac{2}{4} = 0.5\]
\end{description}

\section{Algoritmi di clustering}
Gli algoritmi di clustering possono essere classificati in base al tipo di metrica utilizzata e alla loro strategia di raggruppamento. 

\subsection{Tipi di clustering}
Una tassonomia comune include:
\begin{description}
    \item[Clustering gerarchico o agglomerativo.] - Costruisce una gerarchia di cluster, che può essere rappresentata come un dendrogramma. Gli algoritmi più comuni sono l'algoritmo di Agglomerative Nesting (AGNES) e il Divisive Analysis (DIANA). Questi algoritmi iniziano con ogni punto come un cluster separato e successivamente uniscono i cluster più vicini fino a ottenere un unico cluster o fino a raggiungere un numero desiderato di cluster.
    \item[Clustering partizionale.] - Questi algoritmi cercano di partizionare i dati in un numero fisso di cluster, come il K-means e il K-medoids. Il K-means cerca di minimizzare la somma delle distanze quadrate tra i punti e i loro centroidi, mentre il K-medoids utilizza i medoidi come rappresentanti dei cluster.
    \item[Clustering per densità.] - Identifica cluster come aree di alta densità separate da aree di bassa densità. L'algoritmo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) è un esempio di questo tipo di clustering, che richiede due parametri: il raggio di ricerca (eps) e il numero minimo di punti in un cluster (minPts). DBSCAN è particolarmente utile per identificare cluster di forma arbitraria e per gestire il rumore nei dati.
\end{description} 

\subsection{Bontà di un algoritmo}
La bontà di un algoritmo di clustering può essere valutata attraverso diversi fattori:
\begin{itemize}
    \item Scalabilità: l'algoritmo deve essere in grado di gestire grandi dataset senza un aumento esponenziale del tempo di esecuzione.
    \item Robustezza: l'algoritmo deve essere in grado di gestire rumore e outlier nei dati senza influenzare significativamente i risultati del clustering.
    \item Interpretabilità: i risultati del clustering devono essere facilmente interpretabili e utili per l'analisi dei dati.
    \item Stabilità: l'algoritmo dovrebbe produrre risultati simili su esecuzioni multiple con lo stesso dataset, a meno che non vengano introdotte variazioni significative nei dati.
    \item Flessibilità: l'algoritmo dovrebbe essere in grado di gestire diversi tipi di dati, come dati numerici, categoriali o testuali, e dovrebbe essere in grado di utilizzare diverse metriche di distanza a seconda delle esigenze del problema.
    \item Insensibilità: l'algoritmo non deve essere troppo sensibili all'aggiunta di nuovi dati, in modo che i cluster rimangano stabili anche con l'introduzione di nuovi punti.
\end{itemize}

\subsection{Curse of dimensionality}
Uno dei problemi comuni che il clustering deve affrontare è la \textbf{curse of dimensionality}, che si riferisce al fenomeno in cui l'aumento del numero di dimensioni (caratteristiche) rende difficile la misurazione delle distanze tra i punti. In spazi ad alta dimensione, i punti tendono a diventare equidistanti, rendendo difficile distinguere tra cluster. Per convincersi di questo problema, si può usare la distanza euclidea tra due punti: per un certo $n$ molto grande e per punti casuali $x$ e $y$, la distanza tende a essere molto simile per tutti i punti:
\begin{itemize}
    \item $D_2(x, y)$ ha un'alta probabilità di essere almeno pari a $1$, perché la somma di molti termini positivi tende a essere grande creando un \textbf{limite inferiore}.
    \item Invece è improbabile che $D_2(x, y)$ sia molto grande, perché la probabilità che tutti i termini della somma siano grandi è bassa, creando un \textbf{limite superiore}.
\end{itemize}

Da questo, possiamo dire che in alta dimensione, tutte le distanze tendono a concentrarsi intorno a un valore medio:
\[
\underbrace{1}_{\text{limite inferiore}} \ \lesssim\ D(\mathbf{x},\mathbf{y})\ \lesssim\ \underbrace{\sqrt{n}}_{\text{limite superiore}}
\]

\subsection{Ortogonalità dei vettori}
Un altro aspetto della curse of dimensionality è l'ortogonalità dei vettori in spazi ad alta dimensione. In uno spazio euclideo di dimensione elevata, la maggior parte dei vettori tende ad essere quasi ortogonale tra loro. Questo significa che l'angolo tra due vettori casuali tende a essere vicino a 90 gradi, rendendo difficile trovare direzioni significative nei dati. Questo fenomeno può complicare ulteriormente il processo di clustering, poiché i cluster potrebbero non essere ben definiti in termini di direzioni nei dati.

Per convincersi di questo, si può considerare la distanza del coseno tra due vettori casuali in uno spazio di dimensione elevata. La distanza del coseno tende a essere vicina a 1, indicando che i vettori sono quasi ortogonali:
\[
D_{cos}(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|} \approx 1
\]

Questo perché il denominatore è formato da quantità positive, mentre il numeratore (il prodotto scalare) tende a essere piccolo in confronto, poiché le componenti dei vettori si annullano a vicenda in alta dimensione. Si può dimostrare che, al crescere di $n$, il denominatore cresca linearmente mentre il numeratore assuma un valore atteso 0 con una deviazione standard pari a $\sqrt{n}$. Pertanto, il rapporto tende a 0 e la distanza del coseno tende a 1.

\section{Clustering Gerarchico}
Il clustering gerarchico è una tecnica di clustering che costruisce una gerarchia di cluster, rappresentata come un \textbf{dendrogramma}: un albero che mostra le relazioni tra i cluster a diversi livelli di granularità. Esistono due approcci principali al clustering gerarchico: l'approccio agglomerativo e l'approccio divisivo.

\subsection{Distanze tra cluster}
Per considerare due cluster vicini si devono utilizzare delle metriche:
\begin{itemize}
    \item In uno spazio euclideo si considera il \emph{centroide} di ogni cluster e si calcola la distanza tra i centroidi.
    \item In spazi non euclidei si considera il \emph{medoide} di ogni cluster e si calcola la distanza tra i medoidi.
\end{itemize}

Uno dei problemi di utilizzare il centroide o il medoide è che non sempre rappresentano bene la forma del cluster, specialmente se i cluster hanno forme complesse o non sono convessi. Per questo motivo, si possono utilizzare altre strategie per misurare la distanza tra cluster:

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.65\textwidth}
\vspace{0pt}
\begin{itemize}[leftmargin=1.2em]
  \item \textbf{Single-link}: $\min\{D(x,y): x\in C_i,\ y\in C_j\}$ (tende a catene). Questo metodo può portare a cluster allungati e poco compatti, poiché si basa solo sulla distanza minima tra i punti dei cluster, permettendo a punti distanti di essere raggruppati insieme se esiste una catena di punti vicini.
  \item \textbf{Complete-link}: $\max\{D(x,y): x\in C_i,\ y\in C_j\}$ (favorisce cluster compatti). Questo metodo tende a creare cluster più compatti e sferici, poiché considera la distanza massima tra i punti dei cluster, evitando che punti lontani vengano raggruppati insieme.
  \item \textbf{Average-link}: media delle distanze su tutte le coppie $x\in C_i,\,y\in C_j$ (compromesso). Questo metodo bilancia le caratteristiche di single-link e complete-link, considerando la distanza media tra tutti i punti dei cluster, risultando in cluster più equilibrati.
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[t]{0.27\textwidth}
\vspace{0pt}
\centering
\includegraphics[width=\linewidth]{images/cluster_distances.png}
\caption{Esempi grafici delle diverse nozioni di distanza tra cluster.}
\label{fig:cluster_distances}
\end{minipage}
\end{figure}

\subsection{Dendrogramma}
Un dendrogramma è una rappresentazione grafica della gerarchia dei cluster ottenuta tramite il clustering gerarchico. Ogni foglia del dendrogramma rappresenta un punto dati, mentre i nodi interni rappresentano i cluster formati unendo i punti o i cluster più vicini. L'altezza di un nodo nel dendrogramma indica la distanza tra i cluster uniti in quel punto.

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{images/dendograms.png}
  \caption{A sinistra: punti nel piano con centroidi (triangoli) e cerchi che schematizzano la coesione dei gruppi; i colori indicano i cluster. A destra: dendrogramma agglomerativo che mostra l'ordine di fusione e l'altezza (distanza di linkage). Un taglio orizzontale del dendrogramma determina il numero di cluster.}
  \label{fig:dendograms}
\end{figure}

Il dendrogramma consente di visualizzare come i cluster si formano a diversi livelli di distanza, permettendo di scegliere il numero di cluster desiderato effettuando un taglio orizzontale al livello appropriato. Ad esempio, tagliando il dendrogramma a un'altezza specifica, si possono ottenere un certo numero di cluster, come mostrato nella Figura \ref{fig:dendograms}.

Può essere tuttavia utilizzato come \textbf{criterio di arresto} per gli algoritmi agglomerativi: si può decidere di fermarsi quando la distanza tra i cluster da unire supera una certa soglia, evitando così di dover specificare a priori il numero di cluster desiderato.

\subsection{Clustering divisivo}
Uno dei problemi che il clustering agglomerativo può incontrare è la \textbf{scelta del numero di cluster}: spesso non è noto a priori quanti cluster esistano nei dati, e scegliere un numero errato può portare a risultati di clustering subottimali. 

Il clustering \emph{divisivo} affronta questo problema iniziando con tutti i punti in un unico cluster e suddividendolo iterativamente in cluster più piccoli (fino a raggiungere un criterio di arresto). Questo approccio consente di esplorare la struttura dei dati in modo più flessibile, poiché non richiede la specifica del numero di cluster iniziale.

\subsection{Complessità computazionale}
Al primo passo si valuta la distanza per ogni coppia di cluster e si sceglie la migliore: costo $\Theta(n^2)$.
Dopo ogni fusione i cluster diminuiscono di uno, quindi i passi successivi costano, nell'ordine,
$(n-1)^2,(n-2)^2,\dots,2^2$.
\[
T_{\text{naive}}
=\sum_{k=2}^{n} k^{2}
=\frac{n(n+1)(2n+1)}{6}-1
=\Theta(n^{3}).
\]

Utilizzando però delle \textbf{code di priorità} per memorizzare le distanze tra i cluster e ottenere il minimimo in $O(1)$, si può ridurre la complessità a $\Theta(n^2 \log n)$:
\[
T_{\text{optimized}}
=\sum_{k=2}^{n} \left( O(\log k) + O(k) \right)
=O(n^2 \log n).
\]

\section{Clustering partizionale: K-means}
Il clustering partizionale è un'alternativa più efficiente rispetto al clustering gerarchico, specialmente per grandi dataset. Questi algoritmi cercano di partizionare i dati in un numero fisso di cluster, ottimizzando una funzione obiettivo che misura la qualità del clustering.

Il clustering partizionale più noto è il K-means, che mira a minimizzare la somma delle distanze quadrate tra i punti e i loro centroidi. L'algoritmo assume di conoscere a priori il numero di cluster $k$ e procede iterativamente attraverso i seguenti passi:
\begin{enumerate}
    \item Si scelgono inizialmente $k$ punti che abbiano probabilità alta di essere ben distribuiti (ad esempio, selezionandoli casualmente dal dataset) come centroidi iniziali.
    \item Si assegna ogni punto al cluster il cui centroide è più vicino, utilizzando la distanza euclidea.
    \item Si ricalcolano i centroidi di ogni cluster come la media dei punti assegnati a quel cluster.
    \item Si ripetono i passi 2 e 3 fino a quando i centroidi non cambiano più significativamente o fino a raggiungere un numero massimo di iterazioni.
\end{enumerate}

\subsection{Scelta greedy dei centroidi iniziali}
Un modo semplice per scegliere i centroidi iniziali è selezionarli casualmente dal dataset. Tuttavia, questa scelta può portare a risultati subottimali se i centroidi iniziali non sono ben distribuiti. Un modo ottimale è utilizzare una scelta \emph{greedy}\footnote{Un algoritmo greedy è un algoritmo che prende decisioni localmente ottimali in ogni passo con l'aspettativa di trovare una soluzione globale ottimale.} che massimizzi la distanza tra i centroidi iniziali:
\begin{enumerate}
    \item Si sceglie il primo centroide casualmente dal dataset $c_1$ e lo si aggiunge all'insieme $S$ dei punti scelti.
    \item Si aggiunge a $S$ il punto $x \in X \setminus S$ ($X$ è il dataset originale) che massimizza la distanza minima da tutti i punti già scelti:
    \[
    c_{i} = \arg\max_{x \in X \setminus S} \min_{y \in S} D(x, y)
    \]
    \item Si ripete il passo 2 fino a quando non si sono scelti $k$ centroidi.
\end{enumerate}

\subsection{Funzione obiettivo}
Per arrestare l'algoritmo K-means, si può utilizzare una funzione obiettivo che misura la qualità del clustering. Questa funzione può essere definita a paartire dalla somma delle distanze quadrate tra i punti e i loro centroidi:
\[
E = \sum_{i=1}^k \sum_{x \in C_i} D_2(x, c_i)^2
\]
dove $C_i$ è il cluster $i$ e $c_i$ è il centroide del cluster $i$. L'algoritmo K-means mira a minimizzare questa funzione obiettivo, e si può arrestare quando la variazione di $E$ tra due iterazioni consecutive è inferiore a una soglia predefinita.

\subsection{Scelta del numero di cluster}
Uno dei problemi del clustering partizionali è la scelta del numero di cluster $k$. Generalmente non si conoscono a priori il numero di cluster nei dati, e scegliere un valore errato può portare a risultati di clustering subottimali.

Si può quindi considerare il valore di $k$ come un \textbf{iperparametro}\footnote{Un parametro il cui valore è fissato prima dell'addestramento e non viene appreso dal modello.} da ottimizzare. 

\paragraph{Metodo elbow.}
Un metodo comune per scegliere il numero di cluster è il \textbf{metodo elbow}, che consiste nel calcolare la funzione obiettivo $E$ per diversi valori di $k$ e tracciare un grafico di $E$ in funzione di $k$. Si cerca quindi un punto nel grafico dove la diminuzione di $E$ inizia a rallentare, formando una "gomito" (elbow). Questo punto indica un buon compromesso tra la qualità del clustering e la complessità del modello.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/elbow-k.png}
  \caption{Metodo (\emph{elbow}). Si traccia la distanza media dal centroide (o WCSS/n) al variare di $k$; il valore “ottimo” è nel punto di flesso, dove l'aumento di $k$ porta benefici marginali trascurabili.}
  \label{fig:elbow}
\end{figure}

Dalla figura \ref{fig:elbow} notiamo che la funzione:
\[
W(k) = \frac{1}{n} \sum_{i=1}^k \sum_{x \in C_i} D_2(x, c_i)^2
\]
decresce al crescere di $k$, ma il tasso di decrescita diminuisce. Si sceglie quindi il valore di $k$ in corrispondenza del punto di flesso della curva, dove l'aggiunta di ulteriori cluster non porta a una riduzione significativa della somma delle distanze quadrate. Questo può essere anche individuato matematicamente supponendo che tra due valori $x$ e $y$ del parametro $k$ ci sia una differenza \emph{non trascurabile} nella distanza media dai centroidi:
\begin{enumerate}
    \item Si prende il valore medio $z = (x + y) / 2$ e si effettua il clustering per $k = z$.
    \item Se il valore della distanza media dai centroidi per $k = z$ è vicino a quello per $k = x$, allora si sceglie $k = x$, altrimenti si sceglie $k = y$.
    \item Si ripetono i passi 1 e 2 fino a quando non si trova il valore ottimale di $k$.
\end{enumerate}

\paragraph{Metodo silhouette.}
Un altro metodo per scegliere il numero di cluster è il \textbf{metodo silhouette}, che valuta la qualità del clustering calcolando un indice di silhouette per ogni punto. L'indice di silhouette misura \emph{quanto} un punto \emph{è ben assegnato al suo cluster} rispetto agli altri cluster. Per un punto $x_i$ appartenente al cluster $C_i$, si definiscono:
\begin{itemize}
    \item $a(i)$: la distanza media tra $x_i$ e tutti gli altri punti nel suo cluster $C_i$ (coesione):
    \[
    a(i) = \frac{1}{|C_i| - 1} \sum_{x_j \in C_i, j \neq i} D(x_i, x_j)
    \]
    \item $b(i)$: la distanza media tra $x_i$ e tutti i punti nel cluster più vicino a $C_i$ (separazione):
    \[
    b(i) = \min_{j \neq i} \frac{1}{|C_j|} \sum_{x_k \in C_j} D(x_i, x_k)
    \]
\end{itemize}

L'indice di silhouette $s(i)$ per il punto $x_i$ è quindi definito come:
\[
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\]

Questo valore è compreso in $[-1, 1]$, dove un valore vicino a 1 indica che il punto è ben assegnato al suo cluster, un valore vicino a 0 indica che il punto è al confine tra due cluster, e un valore negativo indica che il punto potrebbe essere stato assegnato al cluster sbagliato. 

Possiamo analizzare anche le funzioni $a(i), b(i)$:
\begin{itemize}
    \item Se $a(i) \ll b(i)$, allora $s(i) \approx 1$: il punto è ben assegnato al suo cluster. Questo perché la distanza media all'interno del cluster è molto più piccola rispetto alla distanza media al cluster più vicino.
    \item Se $a(i) \approx b(i)$, allora $s(i) \approx 0$: il punto è al confine tra due cluster. In questo caso, la distanza media all'interno del cluster è simile a quella al cluster più vicino, indicando che il punto non è chiaramente associato a un cluster specifico.
    \item Se $a(i) \gg b(i)$, allora $s(i) \approx -1$: il punto potrebbe essere stato assegnato al cluster sbagliato. Questo accade quando la distanza media all'interno del cluster è maggiore rispetto alla distanza al cluster più vicino, suggerendo che il punto sarebbe meglio posizionato in un altro cluster.
\end{itemize}

Grazie all'indice di silhouette, possiamo definire un problema di ottimizzazione per scegliere il numero di cluster $k$:
\[
k* = \arg\max_{k} \frac{1}{n} \sum_{i=1}^n s(i)
\]
Ovvero, scegliamo il numero di cluster che massimizza la media degli indici di silhouette su tutti i punti del dataset. Questo approccio consente di valutare la qualità del clustering in modo più dettagliato rispetto al metodo elbow, poiché tiene conto della coesione e della separazione dei cluster.

\subsection{Complessità computazionale}
Dato $t$ il numero di iterazioni dell'algoritmo K-means, $k$ il numero di cluster ed $n$ il numero di elementi del dataset, la complessità computazionale dell'algoritmo K-means è:
\[
T = O(t \cdot k \cdot n)
\]
Questo perché in ogni iterazione si devono assegnare tutti i $n$ punti ai $k$ cluster (costo $O(k \cdot n)$) e poi ricalcolare i centroidi (costo $O(n)$, trascurabile rispetto al costo di assegnazione).

\subsection{K-means su Big data}
Per clusterizzare grosse quantità di dati in spazi con elevato numero di dimensioni, che non possonoo risiedere in memoria principale, si utilizzano opportune varianti del $k$-means. Queste varianti, BFR e CURE, utilizzano una serie di statistiche e di valori per rappresentare in modo compatto i cluster e ottimizzare l'uso della RAM (gestendo anche gli outlier).