\chapter{Elementi di Reti neurali}
Una rete neurale è un modello computazionale ispirato alla struttura e al funzionamento del cervello umano. È composta da unità chiamate neuroni artificiali, organizzati in strati (layer), che elaborano informazioni attraverso connessioni ponderate. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/nn_neuron_comparison.png}
    \caption{Confronto tra un neurone biologico (a sinistra) e un neurone artificiale (a destra). 
    Nel neurone biologico, il segnale si propaga dai dendriti, attraverso il soma e lungo l'assone fino ai terminali sinaptici. 
    Nel neurone artificiale, gli ingressi $x_i$ vengono pesati con i corrispondenti pesi $w_i$, sommati e combinati con un termine di bias; 
    il risultato $z_j$ viene poi trasformato da una funzione di attivazione per generare l'uscita del neurone.}
    \label{fig:nn_neuron_comparison}
\end{figure}

\noindent
La struttura di una rete neurale è generalmente definita come una sequenza di layer:
\begin{itemize}
    \item \textbf{Input layer}: (strato di ingresso) riceve i dati grezzi e li trasmette agli strati successivi.
    \item \textbf{Hidden layers}: (strati nascosti) elaborano le informazioni ricevute dall'input layer attraverso una serie di trasformazioni non lineari.
    \item \textbf{Output layer}: (strato di uscita) produce il risultato finale della rete neurale, come una classificazione o una previsione.
\end{itemize}

\section{Strati}
Ogni strato di una rete neurale è costituito da un insieme di neuroni artificiali (esempio in figura \ref{fig:neural_network_example}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/neural_network_example.png}
    \caption{Esempio di una rete neurale con 1 input layers, 4 hidden layers e e 1 output layer.}
    \label{fig:neural_network_example}
\end{figure}

Ogni \emph{hidden layer} è costituito da \textbf{nodi} che prendono in input \emph{valori pesati} proveniente dallo strato precedente, li elaborano e producono un valore di output che verrà pesato ad uno o più nodi del layer successivo. L'output layer è costituito da uno o più nodi che restituiscono in output un valore.

\paragraph{Deep neural network.}
Si parla di \emph{deep neural network} (DNN) quando una rete neurale possiede più di un hidden layer. Le DNN sono in grado di apprendere rappresentazioni più complesse dei dati rispetto alle reti neurali con un solo hidden layer, permettendo di risolvere problemi più sofisticati.

In generale le reti neurali lavorano con strutture dati chiamate \textbf{tensori}\footnote{Un tensore è una struttura dati multidimensionale che generalizza i concetti di scalare (0D), vettore (1D) e matrice (2D) a dimensioni superiori. I tensori sono fondamentali nell'ambito del machine learning e delle reti neurali, poiché consentono di rappresentare e manipolare dati complessi in modo efficiente.}.

\subsection{Connessioni tra layer}
La rete neurale in figura \ref{fig:neural_network_example} è un esempio di rete \textbf{densa}, poiché ogni nodo riceve tutti gli output dai nodi del layer precedente. Altre tipologie tra layer sono:
\begin{description}
    \item[Random]: fissato un certo $m$, ogni nodo riceve output solamente da $m$ nodi random del precedente layer.
    \item[Pooled]: i nodi di un layer sono partizionati in $k$ cluster. Il layer successivo sarà formato da $k$ nodi, uno per ogni cluster. Il nodo associato al cluster $C$ riceverà solo gli output dei nodi del layer precedente appartenenti a tale certo cluster.
    \item[Convoluzionale]: ogni nodo di un layer è connesso solo a un sottoinsieme di nodi del layer precedente, definiti da una \emph{finestra di convoluzione} che si sposta lungo l'input. Questo tipo di connessione è particolarmente utile per l'elaborazione di dati strutturati, come immagini o segnali audio.
\end{description}

\section{Progettare una rete neurale}
Quando si addestra una rete neurale bisogna stabilire: 
\begin{itemize}
    \item Quanti layer nascosti definire.
    \item Quanto nodi in ciascun layer.
    \item Come connettere nodi di layer consecutivi.
    \item Quale funzione di attivazione scegliere per ogni layer.
\end{itemize}

Definita la struttura, il modello dweve essere addestrato su un training set per calcolare i valori ottimali dei pesi grazie a una \textbf{funzione di costo} (o \emph{loss function}) che misura l'errore tra le predizioni della rete e i valori reali. L'addestramento avviene tramite algoritmi di ottimizzazione come la \emph{discesa del gradiente} (gradient descent) e il \emph{backpropagation}, che aggiornano i pesi per minimizzare la funzione di costo.

Generalmente si va per tentativi, provando diverse architetture della rete partendo da un caso semplice, che non può dare magari buoni risultati che aiuta tuttavia  capire meglio il problema e le caratteristiche dei dati.

\section{Funzioni di attivazione}
La funzione di attivazione di un neurone artificiale è quella funzione $F$ che determina l'output in base all'input ricevuto. Guardando la figura \ref{fig:nn_neuron_comparison}, l'input del neurone artificiale è dato dalla somma pesata degli ingressi più un termine di bias:
\[
z_j = \sum_{i} w_{ij} x_i + b_j
\]

È importante notare che tutti i nodi di uno stesso layer utilizzano la stessa funzione di attivazione.

\paragraph{Proprietà.}
Le funzioni di attivazione devono possedere alcune proprietà:
\begin{itemize}
    \item Devono essere \textbf{differenziabile} e \textbf{continua} per permettere l'addestramento della rete tramite algoritmi di ottimizzazione basati sul calcolo del gradiente.
    \item La derivata della funzione non deve \emph{saturare}: ovvero non deve avvicinarsi a zero per valori estremi dell'input, altrimenti il processo di apprendimento diventa inefficace (in generale si ha uno stallo nell'aggiornamento dei pesi).
    \item Allo stesso modo, la derivata non deve "esplodere", ovvero non deve tendere a infinito, poiché ciò può causare instabilità nell'addestramento (numerica, in questo caso, nella ricerca dei pesi ottimali).
\end{itemize}

\subsection{Funzione step}
La funzione step (o funzione a gradino) è una funzione di attivazione semplice che restituisce 0 per input negativi e 1 per input positivi. È definita come:
\[
F(z) = \begin{cases}
0 & \text{se } z < 0 \\
1 & \text{se } z \geq 0
\end{cases}
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images/step_function.png}
    \caption{Grafico della funzione step.}
    \label{fig:step_function}
\end{figure}

La funzione step è utile per modelli di classificazione binaria, ma non è differenziabile nel punto $z=0$, il che limita la sua efficacia nell'addestramento delle reti neurali tramite metodi basati sul gradiente. Un modello che usa questa funzione è il \emph{perceptron}: un semplice modello di rete neurale con un singolo layer di nodi che utilizza la funzione step per prendere decisioni binarie.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/perceptron.png}
    \caption{Esempio di un perceptron con 4 input, pesi associati e bias. L'output viene calcolato applicando la funzione step alla somma pesata degli input più il bias.}
    \label{fig:perceptron}
\end{figure}

\subsection{Funzione logistica}
Un altro tipo di funzione di attivazione è la funzione logistica (o sigmoide), definita come:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

La funzione sigmoide è utilizzata spesso in reti neurali per problemi di classificazione binaria, poiché mappa qualsiasi input reale in un intervallo compreso tra 0 e 1, interpretabile come una probabilità. La funzione logistica rispecchia le proprietà richieste per una funzione di attivazione, essendo differenziabile e continua:
\[
\sigma'(x) = \sigma(x)(1 - \sigma(x))
\]
tuttavia, la sua derivata può saturare per valori estremi di $x$, rallentando l'apprendimento in reti profonde.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sigmoid_function.png}
  \caption{Funzione sigmoide $\sigma(x)$ (blu) e sua derivata $\sigma'(x)$ (rosso tratteggiato). La derivata raggiunge il massimo in \(x=0\) (\(0{.}25\)); per \(|x|\gg 0\) la sigmoide satura e il gradiente tende a zero.}
  \label{fig:sigmoid-derivative}
\end{figure}

\subsection{Tangente iperbolica}
Un'altra funzione di attivazione molto simile alla sigmoide è la tangente iperbolica ($\tanh$). Questa funzione mappa gli input reali nell'intervallo tra -1 e 1, ed è definita come:
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

La tangente iperbolica è spesso preferita alla sigmoide perché la sua uscita è centrata intorno a zero, il che può facilitare l'apprendimento in alcune reti neurali. Questo si può notare riscrivendo la funzione in termini della sigmoide:
\[
\tanh(x) = 2\sigma(2x) - 1
\]
La derivata della tangente iperbolica è:
\[
\tanh'(x) = 1 - \tanh^2(x)
\]
Anche la tangente iperbolica può soffrire di saturazione per valori estremi di $x$, simile alla sigmoide, ma la sua uscita centrata intorno a zero può aiutare a mitigare questo problema in alcune situazioni.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tanh_function.png}
    \caption{Funzione tangente iperbolica $\tanh(x)$: attivazione dispari, centrata in \(0\) con range \((-1,1)\). Satura verso \(\pm1\) per \(|x|\) grandi; derivata $tanh'(x)$ massima in \(x=0\), utile per ridurre il bias shift rispetto alla sigmoide.}
    \label{fig:tanh-derivative}
\end{figure}

\subsection{Funzione softmax}
A differenza della funzione sigmoide, che opera su un unico valore e ritorna un output tra 0 e 1, la funzione softmax agisce sull'intero vettore di output di un layer, trasformandolo in una distribuzione di probabilità. Sia $x = (x_1, x_2, \ldots, x_n)$ il vettore di input, la funzione softmax è definita come:
\[
\mu(x) = (\mu(x_1), \mu(x_2), \ldots, \mu(x_n)) \quad \text{dove} \quad \mu(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]

Si può dimostrare che restituisce una distribuzione di probabilità, sommando i singoli valori della funzione e ottenendo 1:
\[
\sum_{i=1}^{n} \mu(x_i) = \sum_{i=1}^{n} \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = 1
\]

La funzione softmax è comunemente utilizzata nell'output layer di reti neurali per problemi di classificazione multi-classe, dove ogni output rappresenta la probabilità che l'input appartenga a una delle classi possibili.

Dalla formula si evince che il denominatore è una somma di esponenziali, quindi se gli input variano in un range ampio, allora anche gli esponenziali varieranno in un range ampio. Ciò può causare problemi numerici, come overflow o underflow, durante il calcolo della funzione softmax. Possiamo però sfruttare una proprietà del softmax, ovvero l'\textbf{iperparametrizzazione}: diversi valori di input possono produrre lo stesso output. In particolare, possiamo sottrarre un valore costante $c$ da tutti gli input senza modificare l'output della funzione softmax:
\[
\mu(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = \frac{e^{x_i - c}}{\sum_{j=1}^{n} e^{x_j - c}}
\]
Se scegliamo $c = \max_{j}(x_j)$, allora il massimo degli input diventa 0, evitando problemi di overflow negli esponenziali. Inoltre, il denominatore è una somma di valori tra 0 e 1, riducendo il rischio di underflow.

\subsection{ReLU: Rectified Linear Unit}
La funzione ReLU (Rectified Linear Unit) è una funzione di attivazione che prende spunto dai \emph{raddrizzatori} a singola semionda utilizzati in elettronica: essi trasformano un segnale alternato in un segnale unidirezionale (sempre positivo o sempre negativo). La funzione ReLU è definita come:
\[
\text{ReLU}(x) = \max(0, x) = 
\begin{cases}
0 & \text{se } x < 0 \\
x & \text{se } x \geq 0
\end{cases}
\]

Questa funzione genera un output pari a zero per input negativi e un output lineare (uguale all'input) per input positivi. 

Nella pratica, la funzione ReLU non satura mai per valori positivi di $x$, il che aiuta a mitigare il problema del gradiente che scompare (vanishing gradient) durante l'addestramento delle reti neurali profonde. Tuttavia, per input negativi, la derivata della ReLU è zero, il che può portare al problema dei "neuroni morti" (dead neurons), dove alcuni neuroni non si attivano mai durante l'addestramento.

\paragraph{Variante ELU.}
Per affrontare e risolvere il problema dei neuroni morti, esistono delle varianti della ReLU, come la Leaky ReLU e la Exponential Linear Unit (ELU). La ELU è definita come:
\[
\text{ELU}(x) = 
\begin{cases}
x & \text{se } x \geq 0 \\
\alpha (e^{x} - 1) & \text{se } x < 0
\end{cases}
\]
dove $\alpha$ è un iperparametro positivo che controlla il valore di saturazione per input negativi. La ELU permette un piccolo gradiente per input negativi, riducendo il rischio di neuroni morti e migliorando l'apprendimento della rete.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/relu_function.png}
    \caption{Grafici della funzione ReLU (rosso), della sua variante Leaky ReLU (verde) che permette un piccolo gradiente per input negativi, e della ELU (blu) che introduce una componente esponenziale per input negativi.}
    \label{fig:relu_function}
\end{figure}

\section{Funzioni Loss}
Il problema nelle reti neurali è lo stesso che si ha in modelli più piccoli come la classificazione o la regressione: serve definire un modo di misurare l'errore tra le predizioni del modello e i valori reali, così che si possa addestrare il modello per minimizzare tale errore. Questa misurazione dell'errore è definita tramite una \textbf{funzione di costo} (o \emph{loss function}).

La loss function è generalmente applicata sui pesi della rete neurale generando \textbf{pesi ottimali} per i nodi della rete. L'addestramento della rete avviene tramite algoritmi di ottimizzazione come la \emph{discesa del gradiente} (gradient descent) e il \emph{backpropagation}, che aggiornano i pesi per minimizzare la funzione di costo.

\subsection{Regression Loss}
In un problema di regressione, l'obiettivo è prevedere un valore continuo. Ipotizziamo quindi una regression loss function che misuri l'errore tra il valore predetto $\hat{y}$ e il valore reale $y$. Chiamiamo questa funzione $L$:
\[
L(\hat{y}, y) = (\hat{y} - y)^2
\]
Già questa funzione, tuttavia, presenta un problema: per valori di errore molto grandi, la funzione cresce in modo quadratico, il che può portare a problemi numerici durante l'addestramento della rete. Una soluzione è utilizzare la funzione di \textbf{Huber loss}, che combina la perdita quadratica per piccoli errori e la perdita lineare per grandi errori:
\[
L_\delta(\hat{y}, y) =
\begin{cases}
(y - \hat{y})^2 & \text{se } |y - \hat{y}| \leq \delta \\
2 \delta |y - \hat{y}| - \frac{1}{2} \delta ^2 & \text{se } |y - \hat{y}| > \delta
\end{cases}
\]
dove $\delta$ è un iperparametro che determina il punto di transizione tra la perdita quadratica e lineare.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/regression_loss.png}
    \caption{Confronto tra la squared error loss (tratteggiata, crescita quadratica) e la Huber loss (linea continua, crescita lineare oltre la soglia): la Huber loss penalizza meno fortemente gli errori molto grandi, risultando più robusta agli outlier.}
    \label{fig:regression_loss}
\end{figure}

\paragraph{MSE: Mean Squared Error.}
Una funzione di costo comunemente usata nei problemi di regressione è il \textbf{Mean Squared Error} (MSE), che calcola la media degli errori quadratici tra le predizioni e i valori reali su un dataset:
\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
\]
dove $N$ è il numero di campioni nel dataset, $\hat{y}_i$ è la predizione per il campione $i$ e $y_i$ è il valore reale corrispondente. Un problema del MSE è che penalizza fortemente gli errori grandi, il che può essere problematico in presenza di outlier nei dati. Si può risolvere utilizzando la radice quadrata dell'MSE, chiamata \textbf{Root Mean Squared Error} (RMSE):
\[
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2}
\]
La RMSE ha la stessa unità di misura delle predizioni e dei valori reali, rendendo più intuitiva l'interpretazione dell'errore medio.

\subsection{Classification Loss}
Nei problemi di classificazione, l'obiettivo è assegnare un'etichetta discreta a ciascun input. Il problema in questo caso, diverso da quello di regressione, è che l'output della rete neurale spesso non è un'etichetta diretta, ma una distribuzione di probabilità sulle possibili classi (ad esempio tramite la funzione softmax). Per misurare l'errore tra la distribuzione predetta e la distribuzione reale (spesso rappresentata come un vettore one-hot), si utilizzano funzioni di costo specifiche per la classificazione.

Consideriamo un problema di classificazione e siano $C_1, C_2, \ldots, C_n$ le classi possibili. Supponiamo che il training set sia formato da coppie $(x, p)$ dove $x$ è il vettore di input e $p = (p_1, p_2, \ldots, p_n)$ è una distribuzione di probabilità dove $p_i$ indica la probabilità che $x$ appartenga alla classe $C_i$. Supponiamo che la rete neurale produca in output una distribuzione di probabilità $q = (q_1, q_2, \ldots, q_n)$, dove $q_i$ è la probabilità predetta per la classe $C_i$, possiamo definire la funzione di costo come una misura della differenza tra le distribuzioni $p$ e $q$.

\paragraph{Entropia.} Per misurare la differenza tra due distribuzioni di probabilità è possibile utilizzare l'\textbf{entropia}, in quanto essa misura l'incertezza associata a una distribuzione di probabilità.

Il \emph{Teorema di Shannon} afferma che, in un sistema di codifica ottimale, il numero di bit per simbolo necessario per rappresentare un messaggio è pari all'entropia della sorgente di informazione che genera il messaggio:
\[
H(p) = - \sum_{i=1}^{n} p_i \log(p_i)
\]
dove $-\log p_i$ rappresenta il numero di bit necessari per codificare l'evento $C_i$ con probabilità $p_i$, misura comunemente nota come \textbf{self-information}.

In altre parole, l'entropia fornisce un limite inferiore alla quantità di informazione necessaria per codificare i dati in modo efficiente.

\paragraph{Entropia incrociata.}
Supponendo di voler cambiare lo schema di codifica, utilizzando una distribuzione di probabilità $q$ diversa dalla distribuzione reale $p$. QUestso implica che per il nuovo schema occorrono $ - \log q_i$ bit per codificare l'evento $C_i$. La quantità media di bit necessari per codificare un messaggio generato dalla distribuzione $p$ utilizzando lo schema di codifica basato su $q$ è data dall'\textbf{entropia incrociata} (cross-entropy):
\[
C(p || q) = - \sum_{i=1}^{n} p_i \log(q_i)
\]

Possiamo utilizzare l'entropia incrociata per misurare la differenza tra la distribuzione reale $p$ e la distribuzione predetta $q$ dalla rete neurale. Questa differenza viene chiamata \textbf{Kullback-Leibler divergence} (o KL divergence):
\[
KL(p || q) = C(p || q) - H(p) = \sum_{i=1}^{n} p_i \log\left(\frac{p_i}{q_i}\right)
\]
e, generalmente, $C(p || q) \geq H(p)$, con uguaglianza se e solo se $p = q$.

Minimizzare la KL divergence equivale a minimizzare l'entropia incrociata, poiché l'entropia $H(p)$ è costante rispetto a $q$. Quindi, possiamo utilizzare l'entropia incrociata come funzione di costo per addestrare la rete neurale:
\[
L(p, q) = - \sum_{i=1}^{n} p_i \log(q_i)
\]

Nella pratica però, poiché minimizzare la divergenza KL equivale a minimizzare l'entropia incrociata, si utilizza direttamente quest'ultima come funzione di costo per addestrare la rete neurale.

\section{Training di una rete neurale}
In un problema di regressione lineare, l'obiettivo è trovare i pesi dell'iperpiano che riescono ad approssimare i dati. Per le reti neurale il ragionamento è lo stesso ma su interpretazioni geometriche più complesse. 

\subsection{Ottimizzazione dei pesi}
L'addestramento di una rete neurale consiste nel trovare i pesi che \emph{minimizzano} la funzione di loss su un training set di dati. Per minimizzare i pesi, si può utilizzare il metodo della \textbf{discesa del gradiente} (gradient descent), che aggiorna iterativamente i pesi della rete nella direzione del gradiente negativo\footnote{Un gradiente negativo, infatti, indica la direzione di massima discesa della funzione ovvero il miglior modo per minimizzare la funzione.} della funzione di loss.

Sia $x = (x_1, x_2, \ldots, x_n)$ il vettore dei pesi della rete neurale e sia:
\[
f: \mathbb{R}^n \rightarrow \mathbb{R}
\]
la funzione di loss da minimizzare. Il gradiente di $f$ in un punto $x$ è il vettore delle derivate parziali di $f$ rispetto a ciascuna variabile:
\[
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
\]

\subsection{Metodo di discesa del gradiente}
L'idea di base della discesa del gradiente è quella di aggiornare iterativamente il vettore dei pesi $x$ nella direzione del gradiente negativo della funzione di loss. Dato uno stato corrente $x^{(t)}$, l'aggiornamento è:

\[
x^{(t+1)} = x^{(t)} - \eta \, \nabla f\bigl(x^{(t)}\bigr)
\]

dove $\eta > 0$ è il \emph{learning rate}, un parametro che controlla l'ampiezza del passo lungo la direzione di discesa.\footnote{Se $\eta$ è troppo grande, l'algoritmo può divergere; se è troppo piccolo, la convergenza è molto lenta.}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/eta_examples.jpg}
    \caption{Andamento della funzione di loss al variare del \emph{learning rate}. 
    Un valore troppo elevato può causare instabilità o divergenza dell'addestramento, 
    mentre un valore troppo basso rallenta significativamente la convergenza. 
    Un \emph{learning rate} ottimale permette una discesa rapida e stabile verso il minimo.}
    \label{fig:eta_examples}
\end{figure}

Fin qui abbiamo considerato $f$ come una funzione scalare di un vettore di pesi $x \in \mathbb{R}^n$. Nelle reti neurali, però, la funzione di loss dipende da $x$ in modo indiretto: i pesi determinano prima le \emph{attivazioni} interne dei neuroni e, tramite queste, l'output finale della rete. Questo significa che la funzione di loss $f$ può essere vista come una funzione composta:
\[
f(x) = \mathcal{L}\bigl( \hat{y}, y \bigr),
\]
dove $\hat{y}$ è l'output della rete neurale (la predizione) e $y$ è l'etichetta corretta. L'output $\hat{y}$ dipende dai pesi $x$ e dall'input $u$ della rete:
\[
\hat{y} = F_x(u),
\]
dove $F_x$ rappresenta la funzione computata dalla rete neurale con pesi $x$. Possiamo quindi vedere formalmente la rete neurale come una funzione vettoriale parametrica:
\[
F_x : \mathbb{R}^d \to \mathbb{R}^k, \qquad \hat{y} = F_x(u),
\]
che mappa l'input $u$ nello spazio delle predizioni $\hat{y}$. La funzione di loss si può allora scrivere come
\[
f(x) = \mathcal{L}\bigl( F_x(u), y \bigr).
\]

Per calcolare il gradiente $\nabla f(x)$ in modo efficiente è fondamentale applicare la \emph{regola della catena}\footnote{La regola della catena permette di calcolare la derivata di una funzione composta come prodotto delle derivate delle funzioni componenti.} del calcolo differenziale in forma vettoriale. In questo contesto entra in gioco la \textbf{matrice Jacobiana}.

\paragraph{Matrice Jacobiana.}
Consideriamo una funzione vettoriale
\[
g : \mathbb{R}^n \to \mathbb{R}^m, \qquad g(x) = 
\begin{pmatrix}
g_1(x) \\
\vdots \\
g_m(x)
\end{pmatrix}.
\]
La \emph{matrice Jacobiana} di $g$ nel punto $x$ è la matrice $m \times n$ delle derivate parziali:
\[
J_g(x) = 
\begin{pmatrix}
\dfrac{\partial g_1}{\partial x_1} & \dfrac{\partial g_1}{\partial x_2} & \cdots & \dfrac{\partial g_1}{\partial x_n} \\
\dfrac{\partial g_2}{\partial x_1} & \dfrac{\partial g_2}{\partial x_2} & \cdots & \dfrac{\partial g_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\dfrac{\partial g_m}{\partial x_1} & \dfrac{\partial g_m}{\partial x_2} & \cdots & \dfrac{\partial g_m}{\partial x_n}
\end{pmatrix}.
\]
La Jacobiana è dunque la generalizzazione matriciale del concetto di derivata: mentre il gradiente $\nabla f(x)$ descrive come una funzione \emph{scalare} varia al variare delle sue variabili, la Jacobiana $J_g(x)$ descrive come variano simultaneamente tutte le componenti di una funzione \emph{vettoriale} $g$.

\paragraph{Stochastic Gradient Descent.}
Nelle reti neurali, il calcolo esatto del gradiente $\nabla f(x)$ su tutto il training set può essere computazionalmente costoso, specialmente per dataset di grandi dimensioni. Per ovviare a questo problema, si utilizza spesso una variante chiamata \textbf{Stochastic Gradient Descent} (SGD). Invece di calcolare il gradiente su tutto il dataset, l'SGD calcola una stima del gradiente utilizzando un singolo campione (o un piccolo batch di campioni) alla volta scelto casualmente.

\subsection{Esempio di computazione}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/computation_graph_nn.png}
    \caption{Grafo computazionale di una rete neurale feed-forward a singolo strato, con ingresso $\mathbf{x}$, pesi $W$, bias $b$, attivazione $\sigma$ e perdita MSE rispetto al target $\hat{y}$.}
    \label{fig:computation_graph_nn}
\end{figure}

Ipotizziamo di voler computare la rete neurale del grafo in figura \ref{fig:computation_graph_nn}. La rete prende in input un vettore $\mathbf{x} \in \mathbb{R}^n$ a cui moltiplica un vettore di pesi $W$ calcolando $\mathbf{u}$:
\[
\mathbf{u} = W \cdot \mathbf{x}
\]

Dopo aver calcolato il vettore $\mathbf{u}$ bisogna aggiungere il vettore di bias $b$ trovando $\mathbf{v}$:
\[
\mathbf{v} = \mathbf{u} + b
\]

Una volta calcolato il vettore $v$ viene applicata la funzione di attivazione per computare la predizione, una sigmoide in questo caso, restituendo $\hat{y}$:
\[
\hat{y} = \sigma(v) = \sigma(Wx + b)
\]

E per valutare la bontà di questa rete si utilizza la funzione di Loss tra le predizioni e le etichette vere, MSE in questo caso, per tirare fuori $L$:
\[
L = MSE(y, \hat{y})
\]

\subsection{Backpropagation}
La sola definizione di una funzione di loss e della discesa del gradiente non è ancora sufficiente per addestrare in modo efficiente una rete neurale profonda. 
Il problema principale è che la loss $L$ dipende dai pesi di tutti i layer solo \emph{indirettamente}, attraverso una lunga composizione di trasformazioni lineari e non lineari. 
Per poter aggiornare ogni parametro è quindi necessario calcolare, per ciascun peso $w$, la derivata $\frac{\partial L}{\partial w}$.

La backpropagation risolve il cosiddetto \emph{credit assignment problem}: dato l'errore complessivo $L$ osservato in uscita, stabilisce ''di chi è la colpa'' nei vari layer interni, assegnando a ciascun neurone un contributo di errore proporzionale alla sua responsabilità. 

\paragraph{Regola della catena.} 
Per fare questo, viene sfruttata la \textbf{regola della catena} (chain rule) del calcolo differenziale: se $y = g(x), z = f(y) = f(g(x))$, allora la derivata di $z$ rispetto a $x$ è:
\[
\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
\]

\noindent
E questa regola può essere applicata anche su funzioni a più variabili: se 
\begin{align*}
y &= (y_1, y_2, \dots, y_n) \\
z &= f(y) = f(y_1, y_2, \dots, y_n) \\
&= f(g_1(x), g_2(x), \dots, g_n(x)), \quad \forall i \in \{1, n\}
\end{align*}
allora:
\[
\frac{dz}{dx} = \sum_{i=1}^n \frac{\partial{z} \cdot dy_i}{\partial{y_i} \cdot dx}
\] 

Questa regola può essere applicata anche a funzioni a più variabili vettore\footnote{Una variabile vettore è una variabile che può assumere valori rappresentati come vettori, ovvero insiemi ordinati di numeri. Ad esempio, un vettore in uno spazio tridimensionale può essere rappresentato come \((x, y, z)\), dove \(x\), \(y\) e \(z\) sono le componenti del vettore lungo ciascuna delle tre dimensioni.} come nel caso delle reti neurali, utilizzando gradienti e matrici jacobiane. Se $y = g(x)$ e $z = f(y) = f(g(x))$ allora:
\[
\nabla_x z = J_x(y)^\top \cdot \nabla_y z
\]
dove $J_x(y)$ è la matrice Jacobiana di $g$ calcolata in $x$.

Questo permette di calcolare efficientemente le derivate parziali della funzione di loss rispetto a tutti i pesi della rete neurale, propagando l'errore all'indietro attraverso la rete, dal layer di output fino ai layer di input. In questo modo, ogni peso viene aggiornato in base al suo contributo all'errore complessivo, permettendo alla rete di apprendere dai dati in modo efficace.

\paragraph{Esempio.}
Ipotizziamo di voler applicare la backpropagation alla rete neurale del grafo in figura~\ref{fig:computation_graph_nn}. Indichiamo con
\[
g(z) = \nabla_z L
\]
il gradiente della loss rispetto alla variabile $z$. In questo esempio $y$ è l'output della rete e $\hat{y}$ il vettore target.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images/computation_graph_nn_backpropagation.png}
    \caption{Grafo di backpropagation per la rete neurale in figura \ref{fig:computation_graph_nn}.}
    \label{fig:backpropagation_example}
\end{figure}

\noindent
Per applicare la backpropagation, si procede come segue:
\begin{enumerate}
    \item Si parte dall'ultimo nodo del grafo, la funzione di loss (MSE), e si calcola la derivata della loss rispetto all'output della rete neurale $y$. Per una singola istanza
    \[
        L = \|y - \hat{y}\|^2 = \sum_i (y_i - \hat{y}_i)^2,
    \]
    da cui, derivando rispetto a $y$,
    \[
        g(y) = \nabla_{y} L = \frac{\partial L}{\partial y} = 2\,(y - \hat{y}).
    \]

    \item Si propaga quindi l'errore all'indietro attraverso la funzione di attivazione $y = \sigma(v)$, applicata componente per componente. La Jacobiana di $y$ rispetto a $v$ è diagonale:
    \[
        J_{v}(y) = \operatorname{diag}\bigl(y \circ (1-y)\bigr),
    \]
    e la regola della catena in forma vettoriale dà
    \[
        g(v) = J_{v}(y)\, g(y).
    \]
    Poiché $J_{v}(y)$ è diagonale, questa moltiplicazione corrisponde a un prodotto elemento per elemento:
    \[
        g(v)_i = y_i(1-y_i)\, g(y)_i
        \qquad\text{ovvero}\qquad
        g(v) = \bigl(y \circ (1-y)\bigr) \circ g(y).
    \]

    \item Consideriamo ora il nodo di somma $v = u + b$. Ogni componente soddisfa $v_i = u_i + b_i$, quindi le Jacobiane rispetto a $u$ e $b$ sono matrici identità:
    \[
        J_{u}(v) = I, \qquad J_{b}(v) = I.
    \]
    Applicando di nuovo la regola della catena otteniamo
    \[
        g(u) = J_{u}(v)\, g(v) = g(v), \qquad
        g(b) = J_{b}(v)\, g(v) = g(v).
    \]
    Nel grafo di backpropagation (figura \ref{fig:backpropagation_example}) questo corrisponde alle due frecce che partono da $g(v)$ e arrivano a $g(u)$ e $g(b)$: lo stesso segnale di errore viene copiato sui due rami.

    \item Infine propaghiamo l'errore fino ai pesi $W$, passando per il nodo di prodotto $u = W x$. Scriviamo $W$ per righe come
    \[
        W =
        \begin{pmatrix}
            w_1^{\top} \\
            \vdots \\
            w_n^{\top}
        \end{pmatrix},
        \qquad
        u_i = w_i^{\top} x.
    \]
    Per ogni riga $w_i$ la Jacobiana di $u$ rispetto a $w_i$ ha tutti zeri tranne nella posizione relativa a $u_i$, dove compare $x$; di conseguenza
    \[
        g(w_i) = J_{w_i}(u)\, g(u) = g(u_i)\, x.
    \]
    Mettendo insieme tutte le righe otteniamo la forma compatta
    \[
        g(W) = g(u)\, x^{\top},
    \]
    che corrisponde al nodo $g(W)$ nella figura \ref{fig:backpropagation_example}: il gradiente rispetto all'intera matrice dei pesi è il prodotto esterno tra il vettore di errori sul layer e il vettore di input $x$.
\end{enumerate}

\section{Tecniche di Regolarizzazione}
Un problema che può nascere da modelli troppo complessi è l'\textbf{overfitting}, ovvero la capacità del modello di adattarsi troppo bene ai dati di training, perdendo la capacità di generalizzare a dati nuovi. Per risolvere questo problema, si utilizza la \emph{regolarizzazione}, ovvero l'aggiunta di un termine di penalizzazione alla funzione di loss che scoraggia soluzioni troppo complesse. Questo funziona perché questo termine va a penalizzare i pesi troppo grandi, che spesso sono associati a modelli complessi.

\subsection{Regolarizzazione L1 e L2}
Le regolarizzazioni più comuni sono la \textbf{regolarizzazione L1} e la \textbf{regolarizzazione L2}:
\begin{description}
    \item[Regolarizzazione L1:] Aggiunge alla funzione di loss un termine proporzionale alla somma dei valori assoluti dei pesi:
    \[
    L_{\text{reg}} = \lambda \sum_{i} |w_i|
    \]
    dove $\lambda$ è un iperparametro che controlla l'importanza della regolarizzazione. La regolarizzazione L1 tende a produrre modelli più semplici, in quanto favorisce soluzioni con molti pesi esattamente uguali a zero, portando a una forma di selezione delle caratteristiche (feature selection).
    \item[Regolarizzazione L2:] Aggiunge alla funzione di loss un termine proporzionale alla somma dei quadrati dei pesi:
    \[
    L_{\text{reg}} = \lambda \sum_{i} w_i^2
    \]
    Anche in questo caso, $\lambda$ controlla l'importanza della regolarizzazione. La regolarizzazione L2 tende a produrre modelli con pesi più piccoli e distribuiti, riducendo la complessità del modello senza eliminare completamente nessun peso.  
\end{description}

\subsection{Dropout}
Un'altra tecnica di regolarizzazione molto efficace è il \textbf{dropout}. Durante l'addestramento, il dropout consiste nel "spegnere" casualmente una frazione dei neuroni in ogni layer della rete. Questo impedisce alla rete di dipendere troppo da singoli neuroni, costringendola a imparare rappresentazioni più robuste e generalizzabili. Durante la fase di test, invece, tutti i neuroni sono attivi, ma i pesi vengono scalati per tenere conto del fatto che durante l'addestramento alcuni neuroni erano stati spenti.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dropout.png}
    \caption{Esempio di applicazione del dropout in una rete neurale: durante l'addestramento, alcuni neuroni (in grigio) vengono "spenti" casualmente, costringendo la rete a non dipendere troppo da singoli neuroni e a imparare rappresentazioni più robuste. Durante il test, tutti i neuroni sono attivi, ma i pesi vengono scalati per tenere conto del dropout.}
    \label{fig:dropout}
\end{figure}

\subsection{Early Stopping}
Un problema che può sopraggiungere durante il training è trovare un minimo locale per la funzione di loss che non generalizza bene sui dati di test. Una tecnica semplice ma efficace per evitare questo problema è l'\textbf{early stopping}. Durante l'addestramento, si monitora la performance della rete su un set di validazione separato dal training set. Se la performance sul set di validazione inizia a peggiorare (ad esempio, se la loss aumenta o l'accuratezza diminuisce), si interrompe l'addestramento, anche se la performance sul training set continua a migliorare. In questo modo, si evita di addestrare troppo a lungo la rete, prevenendo l'overfitting. Questo può essere effettuato salvando i pesi della rete ogni volta che la performance sul set di validazione migliora, e ripristinando i pesi migliori alla fine dell'addestramento.

Il numero di epoche senza miglioramento $\alpha$ prima di fermare l'addestramento è un iperparametro che può essere regolato in base al problema specifico.

\subsection{Aumento del training set}
Altri metodi per ridurre l'overfitting includono l'\textbf{aumento del training set} (data augmentation), che consiste nel generare nuovi dati di training a partire dai dati esistenti tramite trasformazioni come rotazioni, traslazioni, zoom, ecc. Questo aiuta la rete a imparare a riconoscere pattern più generali e a non dipendere troppo da caratteristiche specifiche dei dati originali. 

Per esempio, se una rete neurale è addestrata per classificare immagini si potrebbe incrementare il training set ruotando le immagini o distorcendole leggermente, in modo che la rete impari a riconoscere gli oggetti indipendentemente dalla loro posizione o orientamento.

\section{Tipi di reti neurali}
Generalmente le reti neurali possono essere classificate in base alla loro architettura e al tipo di dati che elaborano. Infatti, una specifica architettura può essere più adatta per un certo tipo di problema rispetto ad un'altra.

\subsection{Feed-Forward Networks}
Le \textbf{Feed-Forward Networks} (FFN) sono il tipo più semplice di reti neurali, in cui l'informazione fluisce in una sola direzione, dall'input all'output, senza cicli o connessioni ricorrenti. Queste reti sono composte da strati di neuroni, dove ogni neurone in uno strato è connesso a tutti i neuroni dello strato successivo. Le FFN sono comunemente utilizzate per problemi di classificazione e regressione.

\subsection{Reti Neurali Convoluzionali}
Le \textbf{Reti Neurali Convoluzionali} (Convolutional Neural Networks, CNN) sono progettate per elaborare dati con una struttura a griglia, come le immagini. Le CNN utilizzano operazioni di convoluzione per estrarre caratteristiche locali dai dati, sfruttando la correlazione spaziale tra i pixel. Queste reti sono particolarmente efficaci per compiti di visione artificiale, come il riconoscimento di immagini e la classificazione.

La loro architettura tipica alterna layer \textbf{convoluzionali}, che applicano filtri per estrarre caratteristiche, e layer di \textbf{pooling}, che riducono la dimensionalità dei dati mantenendo le informazioni più rilevanti.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/convolutional_nn_architecture.png}
    \caption{Schema di una rete neurale convoluzionale per la classificazione di immagini: 
    dall'input grezzo si susseguono layer di convoluzione + ReLU e pooling per l'estrazione gerarchica delle feature, 
    seguiti da livelli fully connected e da un output softmax che produce le probabilità per ciascuna classe (es.\ car, truck, van, bicycle).}
    \label{fig:cnn_architecture}
\end{figure}

\paragraph{Fotorecettori.}
Il primo layer coglie le informazion che rappresentano i pixel essenziali delle immagini (come in dei fotorecettori nell'occhio umano), ovvero i \textbf{contorni}.. Il riconoscimento dei contorni non dipende dal punto di osservazione, quindi i filtri convoluzionali sono in grado di catturare queste caratteristiche indipendentemente dalla loro posizione nell'immagine.

\paragraph{Convolutional Layer.}
I \textbf{convolutional layer} sono composti da un insieme di filtri (o kernel) che vengono applicati all'immagine di input tramite l'operazione di convoluzione. Ogni filtro è una piccola matrice di pesi che scorre sull'immagine, calcolando prodotti scalari tra i pesi del filtro e i pixel dell'immagine. Questo processo produce delle mappe di attivazione che evidenziano la presenza di specifiche caratteristiche nell'immagine, come bordi, angoli o texture.

In particolare, lavorano su un quadrato $k \times k$ di nodi nella griglia del layer precedente (dove $k$ è la dimensione del filtro). Per esempio, prendendo un pixel $x_{ij}$ il filtro viene applicato al quadrato in cui il pixel in alto a sinistra è $x_{ij}$:
\[
z_{ij} = \sum_{i=0}^k \sum_{j=0}^k w_{ij} \cdot x_{i+i, j+j} + b
\]
dove $w_{ij}$ sono i pesi del filtro e $b$ è il bias associato al filtro.

\paragraph{Stride.}
Per controllare la dimensione dell'output, si può utilizzare il parametro \textbf{stride}, che indica di quanti pixel spostarsi orizzontalmente e verticalmente dopo aver applicato il filtro. Un valore di stride maggiore di 1 riduce la dimensione dell'output, mentre uno stride di 1 mantiene la stessa dimensione (a meno di padding).

Ipotizziamo di avere un'immagine di dimensione $W \times H$ e un filtro di dimensione $k \times k$ con uno stride di $s$. La dimensione dell'output dopo l'applicazione del filtro sarà:
\[
W_{out} = \frac{W - k}{s} + 1, \quad H_{out} = \frac{H - k}{s} + 1
\]

\paragraph{Zero Padding.}
Per evitare che le dimensioni dell'output si riducano troppo rapidamente con l'applicazione di più layer convoluzionali, si può utilizzare il \textbf{zero padding}, che consiste nell'aggiungere un bordo di zeri attorno all'immagine di input. Questo permette di mantenere le dimensioni dell'output più vicine a quelle dell'input.

\paragraph{Pooling Layer.}
I \textbf{pooling layer} sono utilizzati per ridurre la dimensionalità delle mappe di attivazione prodotte dai convolutional layer, mantenendo le informazioni più rilevanti. Scorre in larghezza e in lunghezza (di uno step stride) una finestra di dimensione $p \times p$ e ne calcola un valore rappresentativo, come il massimo (max pooling) o la media (average pooling) dei valori all'interno della finestra. Questo processo aiuta a ridurre il numero di parametri e a prevenire l'overfitting.

\paragraph{CNN su immagini a colori.}
Nel caso di un'immagine a colori, l'input è costituito da una matrice $W \times H \times 3$, dove i tre canali rappresentano i valori di rosso, verde e blu (RGB) per ciascun pixel. I filtri convoluzionali in questo caso hanno una profondità di 3, in modo da poter operare su tutti e tre i canali contemporaneamente. Durante la convoluzione, ogni filtro produce una mappa di attivazione che combina le informazioni provenienti dai tre canali, permettendo alla rete di apprendere caratteristiche complesse che coinvolgono tutte le componenti cromatiche dell'immagine.

Tutti i nodi di uno stesso layer convoluzionale condividono gli stessi pesi, in modo da rilevare la stessa caratteristica in diverse posizioni dell'immagine. Questo riduce significativamente il numero di parametri della rete rispetto a una rete fully connected, rendendo l'addestramento più efficiente e meno soggetto a overfitting.

\subsection{Rete neurali di grafi}
Le reti neurali di grafi (Graph Neural Network, GNN) hanno un funzionamento simile alle CNN ma sono progettate per lavorare su dati rappresentabili tramite grafi. La GNN produce per ogni nodo un vettore che rappresenta le informazioni principali e tali rappresentazioni possono essere poi sfruttate per risolvere i problemi di classificazioni di nodi e/o grafi oppure predizione di archi.

Le GNN si basano sul concetto di \textbf{messaggistica} (message passing), in cui i nodi comunicano tra loro per aggiornare le loro rappresentazioni in base alle informazioni dei nodi vicini. Questo processo avviene in più passaggi, in cui ogni nodo aggrega le informazioni dai suoi vicini e aggiorna la sua rappresentazione. La struttura del grafo e le connessioni tra i nodi influenzano profondamente il modo in cui le informazioni vengono propagate e aggregate.

Solitamente le GNN sono composte da 3 tipi di layer:
\begin{description}
    \item[Permutation Equivariant] - Questi layer sono progettati per essere invarianti alla permutazione dei nodi, ovvero l'ordine in cui i nodi vengono presentati non influisce sul risultato finale. Lavorano aggregando le informazioni dai nodi vicini e aggiornando le rappresentazioni in modo che siano indipendenti dall'ordine dei nodi.
    \item[Local Aggregation] - Questi layer aggregano le informazioni dai nodi vicini in modo locale, considerando solo i nodi direttamente connessi. Utilizzano funzioni di aggregazione come la somma, la media o il massimo per combinare le rappresentazioni dei nodi vicini.
    \item[Global Pooling] - Questi layer aggregano le informazioni da tutti i nodi del grafo per produrre una rappresentazione globale. Possono essere utilizzati per compiti di classificazione del grafo o per estrarre caratteristiche globali. Le funzioni di pooling comuni includono la somma, la media o il massimo delle rappresentazioni dei nodi.
\end{description}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/graph_neural_network.png}
    \caption{Schema generale di una rete neurale su grafi. 
    (1) Un blocco \emph{permutation equivariant} propaga le informazioni sul grafo e aggiorna le rappresentazioni dei nodi; 
    (2) un'operazione di \emph{local pooling} aggrega le caratteristiche su un sottografo rilevante; 
    (3) un \emph{global pooling} produce un'unica rappresentazione vettoriale dell'intero grafo.}
    \label{fig:graph_neural_network}
\end{figure}

\subsection{Reti Neurali Ricorrenti}
Le \textbf{Reti Neurali Ricorrenti} (Recurrent Neural Networks, RNN) sono progettate per elaborare dati sequenziali, come serie temporali o testo. Le RNN mantengono uno stato interno che viene aggiornato ad ogni passo della sequenza, permettendo alla rete di "ricordare" informazioni precedenti e di catturare dipendenze temporali nei dati.

Si parla di reti neurali \emph{ricorrenti} perché i neuroni in un layer possono avere connessioni che ritornano a se stessi o ad altri neuroni nello stesso layer, creando cicli nel grafo computazionale, chiamati \textbf{layer ricorrenti}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/rnn_example.png}
    \caption{A sinistra, l'unità base di una rete neurale ricorrente (RNN), in cui lo stato nascosto $\mathbf{s}$ viene aggiornato combinando l'input corrente $\mathbf{x}$ con lo stato precedente tramite le matrici di pesi $\mathbf{U}$, $\mathbf{W}$ e genera l'output $\mathbf{y}$ tramite $\mathbf{V}$. A destra, la stessa RNN “unrolled” per $n$ passi temporali, con la catena di stati $\mathbf{s}_1,\dots,\mathbf{s}_n$ e dei corrispondenti input $\mathbf{x}_1,\dots,\mathbf{x}_n$ e output $\mathbf{y}_1,\dots,\mathbf{y}_n$.}
    \label{fig:rnn_example}
\end{figure}

\paragraph{Input di una RNN.}
L'input di una RNN è una sequenza di vettori:
\[
x = (x_1, x_2, \ldots, x_T)
\]
dove ogni vettore $x_t$ rappresenta l'input al tempo $t$, consegnando una sequenza di output di vettori:
\[
y = (y_1, y_2, \ldots, y_T)
\]

I pesi della rete sono condivisi da tutti i nodi del layer ricorrente e sono rappresentati da tre matrici:

\begin{itemize}
    \item $\mathbf{U}$: matrice dei pesi che collega l'input $\mathbf{x}_t$ allo stato nascosto $\mathbf{s}_t$.
    \item $\mathbf{W}$: matrice dei pesi che collega lo stato nascosto precedente $\mathbf{s}_{t-1}$ allo stato nascosto corrente $\mathbf{s}_t$.
    \item $\mathbf{V}$: matrice dei pesi che collega lo stato nascosto corrente $\mathbf{s}_t$ all'output $\mathbf{y}_t$.
\end{itemize}

Chiamiamo $\mathbf{s}_t$ lo stato nascosto della RNN al tempo $t$.  
Lo stato nascosto viene aggiornato combinando l'input corrente $\mathbf{x}_t$ con lo stato precedente $\mathbf{s}_{t-1}$:

\[
\mathbf{s}_t = f(\mathbf{U}\,\mathbf{x}_t + \mathbf{W}\,\mathbf{s}_{t-1} + \mathbf{b})
\]

dove $f$ è una funzione di attivazione non lineare (ad esempio \textit{tanh} o ReLU) e $\mathbf{b}$ è il bias associato allo stato nascosto.

L'output al tempo $t$ si ottiene applicando la matrice $\mathbf{V}$ allo stato nascosto:
\[
\mathbf{y}_t = \mathbf{V}\,\mathbf{s}_t .
\]

Uno dei problemi principali di queste RNN sono le \textbf{sequenze di lunghezza variabile}. Per gestirlo, si possono utilizzare due tecniche:
\begin{description}
    \item[Zero Padding] - Come nel caso delle CNN, si possono aggiungere zeri alla fine delle sequenze più corte per uniformare la lunghezza di tutte le sequenze nel batch.
    \item[Bucketing] - Si possono raggruppare le sequenze in "bucket" in base alla loro lunghezza, in modo che tutte le sequenze in un bucket abbiano la stessa lunghezza. In questo modo, si può addestrare la RNN su ciascun bucket separatamente, evitando di dover gestire sequenze di lunghezza variabile all'interno dello stesso batch. 
\end{description}

Si possono anche \textbf{combinare} le due tecniche: si costruiscono diversi bucket che \emph{gestiscono} sequenze di lunghezza simile, e all'interno di ogni bucket si applica lo zero padding per \emph{uniformare} la lunghezza delle sequenze.

Un limite delle RNN, però, è l'apprendimento di relazioni "distanti", ovvero la difficoltà di catturare dipendenze a lungo termine nelle sequenze. Questo causa a livello di calcolo differenziale il problema del \textbf{vanishing gradient}, in cui i gradienti calcolati durante la backpropagation diventano molto piccoli, rendendo difficile l'aggiornamento efficace dei pesi associati a queste dipendenze lontane.

\section*{Riferimenti}
I riferimenti di questo capitolo includono:
\begin{itemize}
    \item Materiale visto a lezione.
    \item Capitolo 13 del libro \cite{Leskovec2014MMDS}.
\end{itemize}