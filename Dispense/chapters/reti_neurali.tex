\chapter{Elementi di Reti neurali}
Una rete neurale è un modello computazionale ispirato alla struttura e al funzionamento del cervello umano. È composta da unità chiamate neuroni artificiali, organizzati in strati (layer), che elaborano informazioni attraverso connessioni ponderate. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/nn_neuron_comparison.png}
    \caption{Confronto tra un neurone biologico (a sinistra) e un neurone artificiale (a destra). 
    Nel neurone biologico, il segnale si propaga dai dendriti, attraverso il soma e lungo l'assone fino ai terminali sinaptici. 
    Nel neurone artificiale, gli ingressi $x_i$ vengono pesati con i corrispondenti pesi $w_i$, sommati e combinati con un termine di bias; 
    il risultato $z_j$ viene poi trasformato da una funzione di attivazione per generare l'uscita del neurone.}
    \label{fig:nn_neuron_comparison}
\end{figure}

\noindent
La struttura di una rete neurale è generalmente definita come una sequenza di layer:
\begin{itemize}
    \item \textbf{Input layer}: (strato di ingresso) riceve i dati grezzi e li trasmette agli strati successivi.
    \item \textbf{Hidden layers}: (strati nascosti) elaborano le informazioni ricevute dall'input layer attraverso una serie di trasformazioni non lineari.
    \item \textbf{Output layer}: (strato di uscita) produce il risultato finale della rete neurale, come una classificazione o una previsione.
\end{itemize}

\section{Strati}
Ogni strato di una rete neurale è costituito da un insieme di neuroni artificiali (esempio in figura \ref{fig:neural_network_example}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/neural_network_example.png}
    \caption{Esempio di una rete neurale con 1 input layers, 4 hidden layers e e 1 output layer.}
    \label{fig:neural_network_example}
\end{figure}

Ogni \emph{hidden layer} è costituito da \textbf{nodi} che prendono in input \emph{valori pesati} proveniente dallo strato precedente, li elaborano e producono un valore di output che verrà pesato ad uno o più nodi del layer successivo. L'output layer è costituito da uno o più nodi che restituiscono in output un valore.

\paragraph{Deep neural network.}
Si parla di \emph{deep neural network} (DNN) quando una rete neurale possiede più di un hidden layer. Le DNN sono in grado di apprendere rappresentazioni più complesse dei dati rispetto alle reti neurali con un solo hidden layer, permettendo di risolvere problemi più sofisticati.

In generale le reti neurali lavorano con strutture dati chiamate \textbf{tensori}\footnote{Un tensore è una struttura dati multidimensionale che generalizza i concetti di scalare (0D), vettore (1D) e matrice (2D) a dimensioni superiori. I tensori sono fondamentali nell'ambito del machine learning e delle reti neurali, poiché consentono di rappresentare e manipolare dati complessi in modo efficiente.}.

\subsection{Connessioni tra layer}
La rete neurale in figura \ref{fig:neural_network_example} è un esempio di rete \textbf{densa}, poiché ogni nodo riceve tutti gli output dai nodi del layer precedente. Altre tipologie tra layer sono:
\begin{description}
    \item[Random]: fissato un certo $m$, ogni nodo riceve output solamente da $m$ nodi random del precedente layer.
    \item[Pooled]: i nodi di un layer sono partizionati in $k$ cluster. Il layer successivo sarà formato da $k$ nodi, uno per ogni cluster. Il nodo associato al cluster $C$ riceverà solo gli output dei nodi del layer precedente appartenenti a tale certo cluster.
    \item[Convoluzionale]: ogni nodo di un layer è connesso solo a un sottoinsieme di nodi del layer precedente, definiti da una \emph{finestra di convoluzione} che si sposta lungo l'input. Questo tipo di connessione è particolarmente utile per l'elaborazione di dati strutturati, come immagini o segnali audio.
\end{description}

\section{Progettare una rete neurale}
Quando si addestra una rete neurale bisogna stabilire: 
\begin{itemize}
    \item Quanti layer nascosti definire.
    \item Quanto nodi in ciascun layer.
    \item Come connettere nodi di layer consecutivi.
    \item Quale funzione di attivazione scegliere per ogni layer.
\end{itemize}

Definita la struttura, il modello dweve essere addestrato su un training set per calcolare i valori ottimali dei pesi grazie a una \textbf{funzione di costo} (o \emph{loss function}) che misura l'errore tra le predizioni della rete e i valori reali. L'addestramento avviene tramite algoritmi di ottimizzazione come la \emph{discesa del gradiente} (gradient descent) e il \emph{backpropagation}, che aggiornano i pesi per minimizzare la funzione di costo.

Generalmente si va per tentativi, provando diverse architetture della rete partendo da un caso semplice, che non può dare magari buoni risultati che aiuta tuttavia  capire meglio il problema e le caratteristiche dei dati.

\section{Funzioni di attivazione}
La funzione di attivazione di un neurone artificiale è quella funzione $F$ che determina l'output in base all'input ricevuto. Guardando la figura \ref{fig:nn_neuron_comparison}, l'input del neurone artificiale è dato dalla somma pesata degli ingressi più un termine di bias:
\[
z_j = \sum_{i} w_{ij} x_i + b_j
\]

È importante notare che tutti i nodi di uno stesso layer utilizzano la stessa funzione di attivazione.

\paragraph{Proprietà.}
Le funzioni di attivazione devono possedere alcune proprietà:
\begin{itemize}
    \item Devono essere \textbf{differenziabile} e \textbf{continua} per permettere l'addestramento della rete tramite algoritmi di ottimizzazione basati sul calcolo del gradiente.
    \item La derivata della funzione non deve \emph{saturare}: ovvero non deve avvicinarsi a zero per valori estremi dell'input, altrimenti il processo di apprendimento diventa inefficace (in generale si ha uno stallo nell'aggiornamento dei pesi).
    \item Allo stesso modo, la derivata non deve "esplodere", ovvero non deve tendere a infinito, poiché ciò può causare instabilità nell'addestramento (numerica, in questo caso, nella ricerca dei pesi ottimali).
\end{itemize}

\subsection{Funzione step}
La funzione step (o funzione a gradino) è una funzione di attivazione semplice che restituisce 0 per input negativi e 1 per input positivi. È definita come:
\[
F(z) = \begin{cases}
0 & \text{se } z < 0 \\
1 & \text{se } z \geq 0
\end{cases}
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{images/step_function.png}
    \caption{Grafico della funzione step.}
    \label{fig:step_function}
\end{figure}

La funzione step è utile per modelli di classificazione binaria, ma non è differenziabile nel punto $z=0$, il che limita la sua efficacia nell'addestramento delle reti neurali tramite metodi basati sul gradiente. Un modello che usa questa funzione è il \emph{perceptron}: un semplice modello di rete neurale con un singolo layer di nodi che utilizza la funzione step per prendere decisioni binarie.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/perceptron.png}
    \caption{Esempio di un perceptron con 4 input, pesi associati e bias. L'output viene calcolato applicando la funzione step alla somma pesata degli input più il bias.}
    \label{fig:perceptron}
\end{figure}

\subsection{Funzione logistica}
Un altro tipo di funzione di attivazione è la funzione logistica (o sigmoide), definita come:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

La funzione sigmoide è utilizzata spesso in reti neurali per problemi di classificazione binaria, poiché mappa qualsiasi input reale in un intervallo compreso tra 0 e 1, interpretabile come una probabilità. La funzione logistica rispecchia le proprietà richieste per una funzione di attivazione, essendo differenziabile e continua:
\[
\sigma'(x) = \sigma(x)(1 - \sigma(x))
\]
tuttavia, la sua derivata può saturare per valori estremi di $x$, rallentando l'apprendimento in reti profonde.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sigmoid_function.png}
  \caption{Funzione sigmoide $\sigma(x)$ (blu) e sua derivata $\sigma'(x)$ (rosso tratteggiato). La derivata raggiunge il massimo in \(x=0\) (\(0{.}25\)); per \(|x|\gg 0\) la sigmoide satura e il gradiente tende a zero.}
  \label{fig:sigmoid-derivative}
\end{figure}

\subsection{Tangente iperbolica}
Un'altra funzione di attivazione molto simile alla sigmoide è la tangente iperbolica ($\tanh$). Questa funzione mappa gli input reali nell'intervallo tra -1 e 1, ed è definita come:
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

La tangente iperbolica è spesso preferita alla sigmoide perché la sua uscita è centrata intorno a zero, il che può facilitare l'apprendimento in alcune reti neurali. Questo si può notare riscrivendo la funzione in termini della sigmoide:
\[
\tanh(x) = 2\sigma(2x) - 1
\]
La derivata della tangente iperbolica è:
\[
\tanh'(x) = 1 - \tanh^2(x)
\]
Anche la tangente iperbolica può soffrire di saturazione per valori estremi di $x$, simile alla sigmoide, ma la sua uscita centrata intorno a zero può aiutare a mitigare questo problema in alcune situazioni.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/tanh_function.png}
    \caption{Funzione tangente iperbolica $\tanh(x)$: attivazione dispari, centrata in \(0\) con range \((-1,1)\). Satura verso \(\pm1\) per \(|x|\) grandi; derivata $tanh'(x)$ massima in \(x=0\), utile per ridurre il bias shift rispetto alla sigmoide.}
    \label{fig:tanh-derivative}
\end{figure}

\subsection{Funzione softmax}
A differenza della funzione sigmoide, che opera su un unico valore e ritorna un output tra 0 e 1, la funzione softmax agisce sull'intero vettore di output di un layer, trasformandolo in una distribuzione di probabilità. Sia $x = (x_1, x_2, \ldots, x_n)$ il vettore di input, la funzione softmax è definita come:
\[
\mu(x) = (\mu(x_1), \mu(x_2), \ldots, \mu(x_n)) \quad \text{dove} \quad \mu(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]

Si può dimostrare che restituisce una distribuzione di probabilità, sommando i singoli valori della funzione e ottenendo 1:
\[
\sum_{i=1}^{n} \mu(x_i) = \sum_{i=1}^{n} \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = 1
\]

La funzione softmax è comunemente utilizzata nell'output layer di reti neurali per problemi di classificazione multi-classe, dove ogni output rappresenta la probabilità che l'input appartenga a una delle classi possibili.

Dalla formula si evince che il denominatore è una somma di esponenziali, quindi se gli input variano in un range ampio, allora anche gli esponenziali varieranno in un range ampio. Ciò può causare problemi numerici, come overflow o underflow, durante il calcolo della funzione softmax. Possiamo però sfruttare una proprietà del softmax, ovvero l'\textbf{iperparametrizzazione}: diversi valori di input possono produrre lo stesso output. In particolare, possiamo sottrarre un valore costante $c$ da tutti gli input senza modificare l'output della funzione softmax:
\[
\mu(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} = \frac{e^{x_i - c}}{\sum_{j=1}^{n} e^{x_j - c}}
\]
Se scegliamo $c = \max_{j}(x_j)$, allora il massimo degli input diventa 0, evitando problemi di overflow negli esponenziali. Inoltre, il denominatore è una somma di valori tra 0 e 1, riducendo il rischio di underflow.

\subsection{ReLU: Rectified Linear Unit}
La funzione ReLU (Rectified Linear Unit) è una funzione di attivazione che prende spunto dai \emph{raddrizzatori} a singola semionda utilizzati in elettronica: essi trasformano un segnale alternato in un segnale unidirezionale (sempre positivo o sempre negativo). La funzione ReLU è definita come:
\[
\text{ReLU}(x) = \max(0, x) = 
\begin{cases}
0 & \text{se } x < 0 \\
x & \text{se } x \geq 0
\end{cases}
\]

Questa funzione genera un output pari a zero per input negativi e un output lineare (uguale all'input) per input positivi. 

Nella pratica, la funzione ReLU non satura mai per valori positivi di $x$, il che aiuta a mitigare il problema del gradiente che scompare (vanishing gradient) durante l'addestramento delle reti neurali profonde. Tuttavia, per input negativi, la derivata della ReLU è zero, il che può portare al problema dei "neuroni morti" (dead neurons), dove alcuni neuroni non si attivano mai durante l'addestramento.

\paragraph{Variante ELU.}
Per affrontare e risolvere il problema dei neuroni morti, esistono delle varianti della ReLU, come la Leaky ReLU e la Exponential Linear Unit (ELU). La ELU è definita come:
\[
\text{ELU}(x) = 
\begin{cases}
x & \text{se } x \geq 0 \\
\alpha (e^{x} - 1) & \text{se } x < 0
\end{cases}
\]
dove $\alpha$ è un iperparametro positivo che controlla il valore di saturazione per input negativi. La ELU permette un piccolo gradiente per input negativi, riducendo il rischio di neuroni morti e migliorando l'apprendimento della rete.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/relu_function.png}
    \caption{Grafici della funzione ReLU (rosso), della sua variante Leaky ReLU (verde) che permette un piccolo gradiente per input negativi, e della ELU (blu) che introduce una componente esponenziale per input negativi.}
    \label{fig:relu_function}
\end{figure}

\section{Funzioni Loss}
Il problema nelle reti neurali è lo stesso che si ha in modelli più piccoli come la classificazione o la regressione: serve definire un modo di misurare l'errore tra le predizioni del modello e i valori reali, così che si possa addestrare il modello per minimizzare tale errore. Questa misurazione dell'errore è definita tramite una \textbf{funzione di costo} (o \emph{loss function}).

La loss function è generalmente applicata sui pesi della rete neurale generando \textbf{pesi ottimali} per i nodi della rete. L'addestramento della rete avviene tramite algoritmi di ottimizzazione come la \emph{discesa del gradiente} (gradient descent) e il \emph{backpropagation}, che aggiornano i pesi per minimizzare la funzione di costo.

\subsection{Regression Loss}
In un problema di regressione, l'obiettivo è prevedere un valore continuo. Ipotizziamo quindi una regression loss function che misuri l'errore tra il valore predetto $\hat{y}$ e il valore reale $y$. Chiamiamo questa funzione $L$:
\[
L(\hat{y}, y) = (\hat{y} - y)^2
\]
Già questa funzione, tuttavia, presenta un problema: per valori di errore molto grandi, la funzione cresce in modo quadratico, il che può portare a problemi numerici durante l'addestramento della rete. Una soluzione è utilizzare la funzione di \textbf{Huber loss}, che combina la perdita quadratica per piccoli errori e la perdita lineare per grandi errori:
\[
L_\delta(\hat{y}, y) =
\begin{cases}
(y - \hat{y})^2 & \text{se } |y - \hat{y}| \leq \delta \\
2 \delta |y - \hat{y}| - \frac{1}{2} \delta ^2 & \text{se } |y - \hat{y}| > \delta
\end{cases}
\]
dove $\delta$ è un iperparametro che determina il punto di transizione tra la perdita quadratica e lineare.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/regression_loss.png}
    \caption{Confronto tra la squared error loss (tratteggiata, crescita quadratica) e la Huber loss (linea continua, crescita lineare oltre la soglia): la Huber loss penalizza meno fortemente gli errori molto grandi, risultando più robusta agli outlier.}
    \label{fig:regression_loss}
\end{figure}

\paragraph{MSE: Mean Squared Error.}
Una funzione di costo comunemente usata nei problemi di regressione è il \textbf{Mean Squared Error} (MSE), che calcola la media degli errori quadratici tra le predizioni e i valori reali su un dataset:
\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
\]
dove $N$ è il numero di campioni nel dataset, $\hat{y}_i$ è la predizione per il campione $i$ e $y_i$ è il valore reale corrispondente. Un problema del MSE è che penalizza fortemente gli errori grandi, il che può essere problematico in presenza di outlier nei dati. Si può risolvere utilizzando la radice quadrata dell'MSE, chiamata \textbf{Root Mean Squared Error} (RMSE):
\[
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2}
\]
La RMSE ha la stessa unità di misura delle predizioni e dei valori reali, rendendo più intuitiva l'interpretazione dell'errore medio.

\subsection{Classification Loss}
Nei problemi di classificazione, l'obiettivo è assegnare un'etichetta discreta a ciascun input. Il problema in questo caso, diverso da quello di regressione, è che l'output della rete neurale spesso non è un'etichetta diretta, ma una distribuzione di probabilità sulle possibili classi (ad esempio tramite la funzione softmax). Per misurare l'errore tra la distribuzione predetta e la distribuzione reale (spesso rappresentata come un vettore one-hot), si utilizzano funzioni di costo specifiche per la classificazione.

Consideriamo un problema di classificazione e siano $C_1, C_2, \ldots, C_n$ le classi possibili. Supponiamo che il training set sia formato da coppie $(x, p)$ dove $x$ è il vettore di input e $p = (p_1, p_2, \ldots, p_n)$ è una distribuzione di probabilità dove $p_i$ indica la probabilità che $x$ appartenga alla classe $C_i$. Supponiamo che la rete neurale produca in output una distribuzione di probabilità $q = (q_1, q_2, \ldots, q_n)$, dove $q_i$ è la probabilità predetta per la classe $C_i$, possiamo definire la funzione di costo come una misura della differenza tra le distribuzioni $p$ e $q$.

\paragraph{Entropia.} Per misurare la differenza tra due distribuzioni di probabilità è possibile utilizzare l'\textbf{entropia}, in quanto essa misura l'incertezza associata a una distribuzione di probabilità.

Il \emph{Teorema di Shannon} afferma che, in un sistema di codifica ottimale, il numero di bit per simbolo necessario per rappresentare un messaggio è pari all'entropia della sorgente di informazione che genera il messaggio:
\[
H(p) = - \sum_{i=1}^{n} p_i \log(p_i)
\]
dove $-\log p_i$ rappresenta il numero di bit necessari per codificare l'evento $C_i$ con probabilità $p_i$, misura comunemente nota come \textbf{self-information}.

In altre parole, l'entropia fornisce un limite inferiore alla quantità di informazione necessaria per codificare i dati in modo efficiente.

\paragraph{Entropia incrociata.}
Supponendo di voler cambiare lo schema di codifica, utilizzando una distribuzione di probabilità $q$ diversa dalla distribuzione reale $p$. QUestso implica che per il nuovo schema occorrono $ - \log q_i$ bit per codificare l'evento $C_i$. La quantità media di bit necessari per codificare un messaggio generato dalla distribuzione $p$ utilizzando lo schema di codifica basato su $q$ è data dall'\textbf{entropia incrociata} (cross-entropy):
\[
C(p || q) = - \sum_{i=1}^{n} p_i \log(q_i)
\]

Possiamo utilizzare l'entropia incrociata per misurare la differenza tra la distribuzione reale $p$ e la distribuzione predetta $q$ dalla rete neurale. Questa differenza viene chiamata \textbf{Kullback-Leibler divergence} (o KL divergence):
\[
KL(p || q) = C(p || q) - H(p) = \sum_{i=1}^{n} p_i \log\left(\frac{p_i}{q_i}\right)
\]
e, generalmente, $C(p || q) \geq H(p)$, con uguaglianza se e solo se $p = q$.

Minimizzare la KL divergence equivale a minimizzare l'entropia incrociata, poiché l'entropia $H(p)$ è costante rispetto a $q$. Quindi, possiamo utilizzare l'entropia incrociata come funzione di costo per addestrare la rete neurale:
\[
L(p, q) = - \sum_{i=1}^{n} p_i \log(q_i)
\]

Nella pratica però, poiché minimizzare la divergenza KL equivale a minimizzare l'entropia incrociata, si utilizza direttamente quest'ultima come funzione di costo per addestrare la rete neurale.

\section{Training di una rete neurale}
In un problema di regressione lineare, l'obiettivo è trovare i pesi dell'iperpiano che riescono ad approssimare i dati. Per le reti neurale il ragionamento è lo stesso ma su interpretazioni geometriche più complesse. 

\subsection{Ottimizzazione dei pesi}
L'addestramento di una rete neurale consiste nel trovare i pesi che \emph{minimizzano} la funzione di loss su un training set di dati. Per minimizzare i pesi, si può utilizzare il metodo della \textbf{discesa del gradiente} (gradient descent), che aggiorna iterativamente i pesi della rete nella direzione del gradiente negativo\footnote{Un gradiente negativo, infatti, indica la direzione di massima discesa della funzione ovvero il miglior modo per minimizzare la funzione.} della funzione di loss.

Sia $x = (x_1, x_2, \ldots, x_n)$ il vettore dei pesi della rete neurale e sia:
\[
f: \mathbb{R}^n \rightarrow \mathbb{R}
\]
la funzione di loss da minimizzare. Il gradiente di $f$ in un punto $x$ è il vettore delle derivate parziali di $f$ rispetto a ciascuna variabile:
\[
\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)
\]

\subsection{Metodo di discesa del gradiente}
L'idea di base della discesa del gradiente è quella di aggiornare iterativamente il vettore dei pesi $x$ nella direzione del gradiente negativo della funzione di loss. Dato uno stato corrente $x^{(t)}$, l'aggiornamento è:

\[
x^{(t+1)} = x^{(t)} - \eta \, \nabla f\bigl(x^{(t)}\bigr)
\]

dove $\eta > 0$ è il \emph{learning rate}, un parametro che controlla l'ampiezza del passo lungo la direzione di discesa.\footnote{Se $\eta$ è troppo grande, l'algoritmo può divergere; se è troppo piccolo, la convergenza è molto lenta.}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/eta_examples.jpg}
    \caption{Andamento della funzione di loss al variare del \emph{learning rate}. 
    Un valore troppo elevato può causare instabilità o divergenza dell'addestramento, 
    mentre un valore troppo basso rallenta significativamente la convergenza. 
    Un \emph{learning rate} ottimale permette una discesa rapida e stabile verso il minimo.}
    \label{fig:eta_examples}
\end{figure}

Fin qui abbiamo considerato $f$ come una funzione scalare di un vettore di pesi $x \in \mathbb{R}^n$. Nelle reti neurali, però, la funzione di loss dipende da $x$ in modo indiretto: i pesi determinano prima le \emph{attivazioni} interne dei neuroni e, tramite queste, l'output finale della rete. Questo significa che la funzione di loss $f$ può essere vista come una funzione composta:
\[
f(x) = \mathcal{L}\bigl( \hat{y}, y \bigr),
\]
dove $\hat{y}$ è l'output della rete neurale (la predizione) e $y$ è l'etichetta corretta. L'output $\hat{y}$ dipende dai pesi $x$ e dall'input $u$ della rete:
\[
\hat{y} = F_x(u),
\]
dove $F_x$ rappresenta la funzione computata dalla rete neurale con pesi $x$. Possiamo quindi vedere formalmente la rete neurale come una funzione vettoriale parametrica:
\[
F_x : \mathbb{R}^d \to \mathbb{R}^k, \qquad \hat{y} = F_x(u),
\]
che mappa l'input $u$ nello spazio delle predizioni $\hat{y}$. La funzione di loss si può allora scrivere come
\[
f(x) = \mathcal{L}\bigl( F_x(u), y \bigr).
\]

Per calcolare il gradiente $\nabla f(x)$ in modo efficiente è fondamentale applicare la \emph{regola della catena}\footnote{La regola della catena permette di calcolare la derivata di una funzione composta come prodotto delle derivate delle funzioni componenti.} del calcolo differenziale in forma vettoriale. In questo contesto entra in gioco la \textbf{matrice Jacobiana}.

\paragraph{Matrice Jacobiana.}
Consideriamo una funzione vettoriale
\[
g : \mathbb{R}^n \to \mathbb{R}^m, \qquad g(x) = 
\begin{pmatrix}
g_1(x) \\
\vdots \\
g_m(x)
\end{pmatrix}.
\]
La \emph{matrice Jacobiana} di $g$ nel punto $x$ è la matrice $m \times n$ delle derivate parziali:
\[
J_g(x) = 
\begin{pmatrix}
\dfrac{\partial g_1}{\partial x_1} & \dfrac{\partial g_1}{\partial x_2} & \cdots & \dfrac{\partial g_1}{\partial x_n} \\
\dfrac{\partial g_2}{\partial x_1} & \dfrac{\partial g_2}{\partial x_2} & \cdots & \dfrac{\partial g_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\dfrac{\partial g_m}{\partial x_1} & \dfrac{\partial g_m}{\partial x_2} & \cdots & \dfrac{\partial g_m}{\partial x_n}
\end{pmatrix}.
\]
La Jacobiana è dunque la generalizzazione matriciale del concetto di derivata: mentre il gradiente $\nabla f(x)$ descrive come una funzione \emph{scalare} varia al variare delle sue variabili, la Jacobiana $J_g(x)$ descrive come variano simultaneamente tutte le componenti di una funzione \emph{vettoriale} $g$.

\paragraph{Stochastic Gradient Descent.}
Nelle reti neurali, il calcolo esatto del gradiente $\nabla f(x)$ su tutto il training set può essere computazionalmente costoso, specialmente per dataset di grandi dimensioni. Per ovviare a questo problema, si utilizza spesso una variante chiamata \textbf{Stochastic Gradient Descent} (SGD). Invece di calcolare il gradiente su tutto il dataset, l'SGD calcola una stima del gradiente utilizzando un singolo campione (o un piccolo batch di campioni) alla volta scelto casualmente.

\subsection{Backpropagation}


\section*{Riferimenti}
I riferimenti di questo capitolo includono:
\begin{itemize}
    \item Materiale visto a lezione.
    \item Capitolo 13 del libro \cite{Leskovec2014MMDS}.
\end{itemize}