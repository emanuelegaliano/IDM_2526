\chapter{Clustering}
Il clustering è una tecnica di apprendimento \textbf{non supervisionato} che mira a \textbf{raggruppare} un insieme di oggetti in modo che gli oggetti all'interno dello stesso gruppo (o cluster) siano più simili tra loro rispetto a quelli di altri gruppi. Questa tecnica è utile per scoprire strutture nascoste nei dati e per ridurre la dimensionalità.

\section{Spazi metrici e distanze}
Per applicare il clustering, è necessario definire una \textbf{metrica} che misura la \textbf{distanza} tra gli oggetti. Gli oggetti da raggruppare sono, infatti, punti appartenenti a un certo spazio metrico $S$ dove è possibile definire una funzione di distanza $D: S \times S \to \mathbb{R}^+$ che soddisfi le seguenti proprietà:
\begin{enumerate}
    \item \textbf{Non negatività}: $D(x, y) \geq 0$ per ogni $x, y \in S$.
    \item \textbf{Simmetria}: $D(x, y) = D(y, x)$ per ogni $x, y \in S$.
    \item \textbf{Disuguaglianza triangolare}: $D(x, z) \leq D(x, y) + D(y, z)$ per ogni $x, y, z \in S$.
\end{enumerate}

\subsection{Spazio euclideo}
Un esempio di spazio metrico è lo spazio euclideo $\mathbb{R}^n$ con la metrica euclidea, definita come:
\[
D_2(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2} 
\]
dove $x = (x_1, x_2, \ldots, x_n)$ e $y = (y_1, y_2, \ldots, y_n)$ sono due punti in $\mathbb{R}^n$ (si usa $D_2$ per indicare che utilizza la norma $L_2$).


Un problema della distanza euclidea è che utilizza la norma $L_2$, che può non essere adatta per tutti i tipi di dati, specialmente quando le variabili hanno scale diverse o quando i dati contengono outlier. In questi casi, si possono considerare altre metriche come la distanza di Manhattan (con norma $L_1$):
\[
D_1(x, y) = \sum_{i=1}^n |x_i - y_i|
\]

Altre distanze in spazi euclidei includono:
\begin{align*}
&D_r = \left( \sum_{i=1}^n |x_i - y_i|^r \right)^{1/r} &\text{(norma $L_r$)} \\
&D_\infty = \max_{i} |x_i - y_i| &\text{(norma $L_\infty$)} \\
&D_{cos} = 1 - \frac{x \cdot y}{\|x\| \|y\|} &\text{(distanza coseno)}
\end{align*}

\noindent
dove in particolare la distanza coseno è utile per misurare la similarità tra vettori in spazi ad alta dimensione, come nel caso di documenti rappresentati da vettori di frequenze di parole.

\paragraph{Centroide.}
Negli spazi euclidei viene utilizzato il concetto di \textbf{centroide} per rappresentare un cluster. Il centroide di un insieme di punti $X = \{x_1, x_2, \ldots, x_k\}$ è definito come:
\[
c(X) = \frac{1}{k} \sum_{i=1}^k x_i
\]
ovvero $c(X)$ è il punto medio di tutti i punti nel cluster. Il centroide minimizza la somma delle distanze quadrate dai punti del cluster, rendendolo una rappresentazione efficace della posizione centrale del cluster.

\subsection{Spazi non euclidei}
Negli spazi non euclidei il concetto di centroide può non essere definito o utile. In questi casi, si possono utilizzare altre rappresentazioni per i cluster, come il \textbf{medoide}, che è un punto reale del dataset che minimizza la somma delle distanze ai punti del cluster:
\[
m(X) = \arg\min_{x \in X} \sum_{y \in X} D(x, y)
\]
Il medoide è particolarmente utile in spazi dove non ha senso calcolare un centroide, come in spazi discreti o quando i dati sono categoriali. Esempi di distanze non euclidee sono:
\begin{description}
    \item[Distanza di Edit.] - La distanza di edit misura quanto è necessario modificare una stringa per trasformarla in un'altra, considerando operazioni come inserimenti, cancellazioni e sostituzioni di caratteri. Ad esempio partendo da una stringa $A$ e trasformandola in una stringa $B$, la distanza di edit può essere calcolata come il numero minimo di operazioni necessarie per ottenere $B$ da $A$.
    \item[Distanza di Hamming.] - La distanza di Hamming conta il numero di posizioni in cui due stringhe di uguale lunghezza differiscono, ed è utile per dati binari o categoriali. Ad esempio, per le stringhe $A = 10101$ e $B = 10011$, la distanza di Hamming è 2, poiché differiscono nelle posizioni 2 e 4.
    \item[Distanza di Jaccard.] - La distanza di Jaccard misura la dissimilarità tra due insiemi, definita come il rapporto tra l'intersezione e l'unione degli insiemi:
    \[D_J(A, B) = 1 - \frac{|A \cap B|}{|A \cup B|}\]
    Questa distanza è particolarmente utile per dati categoriali o binari, come nel caso di documenti rappresentati da insiemi di parole. Ad esempio, per gli insiemi $A = \{1, 2, 3\}$ e $B = \{2, 3, 4\}$, la distanza di Jaccard è:
    \[D_J(A, B) = 1 - \frac{|\{2, 3\}|}{|\{1, 2, 3, 4\}|} = 1 - \frac{2}{4} = 0.5\]
\end{description}

\section{Algoritmi di clustering}
Gli algoritmi di clustering possono essere classificati in base al tipo di metrica utilizzata e alla loro strategia di raggruppamento. 

\subsection{Tipi di clustering}
Una tassonomia comune include:
\begin{description}
    \item[Clustering gerarchico o agglomerativo.] - Costruisce una gerarchia di cluster, che può essere rappresentata come un dendrogramma. Gli algoritmi più comuni sono l'algoritmo di Agglomerative Nesting (AGNES) e il Divisive Analysis (DIANA). Questi algoritmi iniziano con ogni punto come un cluster separato e successivamente uniscono i cluster più vicini fino a ottenere un unico cluster o fino a raggiungere un numero desiderato di cluster.
    \item[Clustering partizionale.] - Questi algoritmi cercano di partizionare i dati in un numero fisso di cluster, come il K-means e il K-medoids. Il K-means cerca di minimizzare la somma delle distanze quadrate tra i punti e i loro centroidi, mentre il K-medoids utilizza i medoidi come rappresentanti dei cluster.
    \item[Clustering per densità.] - Identifica cluster come aree di alta densità separate da aree di bassa densità. L'algoritmo DBSCAN (Density-Based Spatial Clustering of Applications with Noise) è un esempio di questo tipo di clustering, che richiede due parametri: il raggio di ricerca (eps) e il numero minimo di punti in un cluster (minPts). DBSCAN è particolarmente utile per identificare cluster di forma arbitraria e per gestire il rumore nei dati.
\end{description} 

\subsection{Bontà di un algoritmo}
La bontà di un algoritmo di clustering può essere valutata attraverso diversi fattori:
\begin{itemize}
    \item Scalabilità: l'algoritmo deve essere in grado di gestire grandi dataset senza un aumento esponenziale del tempo di esecuzione.
    \item Robustezza: l'algoritmo deve essere in grado di gestire rumore e outlier nei dati senza influenzare significativamente i risultati del clustering.
    \item Interpretabilità: i risultati del clustering devono essere facilmente interpretabili e utili per l'analisi dei dati.
    \item Stabilità: l'algoritmo dovrebbe produrre risultati simili su esecuzioni multiple con lo stesso dataset, a meno che non vengano introdotte variazioni significative nei dati.
    \item Flessibilità: l'algoritmo dovrebbe essere in grado di gestire diversi tipi di dati, come dati numerici, categoriali o testuali, e dovrebbe essere in grado di utilizzare diverse metriche di distanza a seconda delle esigenze del problema.
    \item Insensibilità: l'algoritmo non deve essere troppo sensibili all'aggiunta di nuovi dati, in modo che i cluster rimangano stabili anche con l'introduzione di nuovi punti.
\end{itemize}

\subsection{Curse of dimensionality}
Uno dei problemi comuni che il clustering deve affrontare è la \textbf{curse of dimensionality}, che si riferisce al fenomeno in cui l'aumento del numero di dimensioni (caratteristiche) rende difficile la misurazione delle distanze tra i punti. In spazi ad alta dimensione, i punti tendono a diventare equidistanti, rendendo difficile distinguere tra cluster. Per convincersi di questo problema, si può usare la distanza euclidea tra due punti: per un certo $n$ molto grande e per punti casuali $x$ e $y$, la distanza tende a essere molto simile per tutti i punti:
\begin{itemize}
    \item $D_2(x, y)$ ha un'alta probabilità di essere almeno pari a $1$, perché la somma di molti termini positivi tende a essere grande creando un \textbf{limite inferiore}.
    \item Invece è improbabile che $D_2(x, y)$ sia molto grande, perché la probabilità che tutti i termini della somma siano grandi è bassa, creando un \textbf{limite superiore}.
\end{itemize}

Da questo, possiamo dire che in alta dimensione, tutte le distanze tendono a concentrarsi intorno a un valore medio:
\[
\underbrace{1}_{\text{limite inferiore}} \ \lesssim\ D(\mathbf{x},\mathbf{y})\ \lesssim\ \underbrace{\sqrt{n}}_{\text{limite superiore}}
\]

\subsection{Ortogonalità dei vettori}
Un altro aspetto della curse of dimensionality è l'ortogonalità dei vettori in spazi ad alta dimensione. In uno spazio euclideo di dimensione elevata, la maggior parte dei vettori tende ad essere quasi ortogonale tra loro. Questo significa che l'angolo tra due vettori casuali tende a essere vicino a 90 gradi, rendendo difficile trovare direzioni significative nei dati. Questo fenomeno può complicare ulteriormente il processo di clustering, poiché i cluster potrebbero non essere ben definiti in termini di direzioni nei dati.

Per convincersi di questo, si può considerare la distanza del coseno tra due vettori casuali in uno spazio di dimensione elevata. La distanza del coseno tende a essere vicina a 1, indicando che i vettori sono quasi ortogonali:
\[
D_{cos}(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|} \approx 1
\]

Questo perché il denominatore è formato da quantità positive, mentre il numeratore (il prodotto scalare) tende a essere piccolo in confronto, poiché le componenti dei vettori si annullano a vicenda in alta dimensione. Si può dimostrare che, al crescere di $n$, il denominatore cresca linearmente mentre il numeratore assuma un valore atteso 0 con una deviazione standard pari a $\sqrt{n}$. Pertanto, il rapporto tende a 0 e la distanza del coseno tende a 1.

\section{Clustering Gerarchico}
Il clustering gerarchico è una tecnica di clustering che costruisce una gerarchia di cluster, rappresentata come un \textbf{dendrogramma}: un albero che mostra le relazioni tra i cluster a diversi livelli di granularità. Esistono due approcci principali al clustering gerarchico: l'approccio agglomerativo e l'approccio divisivo.

\subsection{Distanze tra cluster}
Per considerare due cluster vicini si devono utilizzare delle metriche:
\begin{itemize}
    \item In uno spazio euclideo si considera il \emph{centroide} di ogni cluster e si calcola la distanza tra i centroidi.
    \item In spazi non euclidei si considera il \emph{medoide} di ogni cluster e si calcola la distanza tra i medoidi.
\end{itemize}

Uno dei problemi di utilizzare il centroide o il medoide è che non sempre rappresentano bene la forma del cluster, specialmente se i cluster hanno forme complesse o non sono convessi. Per questo motivo, si possono utilizzare altre strategie per misurare la distanza tra cluster:

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.65\textwidth}
\vspace{0pt}
\begin{itemize}[leftmargin=1.2em]
  \item \textbf{Single-link}: $\min\{D(x,y): x\in C_i,\ y\in C_j\}$ (tende a catene). Questo metodo può portare a cluster allungati e poco compatti, poiché si basa solo sulla distanza minima tra i punti dei cluster, permettendo a punti distanti di essere raggruppati insieme se esiste una catena di punti vicini.
  \item \textbf{Complete-link}: $\max\{D(x,y): x\in C_i,\ y\in C_j\}$ (favorisce cluster compatti). Questo metodo tende a creare cluster più compatti e sferici, poiché considera la distanza massima tra i punti dei cluster, evitando che punti lontani vengano raggruppati insieme.
  \item \textbf{Average-link}: media delle distanze su tutte le coppie $x\in C_i,\,y\in C_j$ (compromesso). Questo metodo bilancia le caratteristiche di single-link e complete-link, considerando la distanza media tra tutti i punti dei cluster, risultando in cluster più equilibrati.
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[t]{0.27\textwidth}
\vspace{0pt}
\centering
\includegraphics[width=\linewidth]{images/cluster_distances.png}
\caption{Esempi grafici delle diverse nozioni di distanza tra cluster.}
\label{fig:cluster_distances}
\end{minipage}
\end{figure}

\subsection{Dendrogramma}
Un dendrogramma è una rappresentazione grafica della gerarchia dei cluster ottenuta tramite il clustering gerarchico. Ogni foglia del dendrogramma rappresenta un punto dati, mentre i nodi interni rappresentano i cluster formati unendo i punti o i cluster più vicini. L'altezza di un nodo nel dendrogramma indica la distanza tra i cluster uniti in quel punto.

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{images/dendograms.png}
  \caption{A sinistra: punti nel piano con centroidi (triangoli) e cerchi che schematizzano la coesione dei gruppi; i colori indicano i cluster. A destra: dendrogramma agglomerativo che mostra l'ordine di fusione e l'altezza (distanza di linkage). Un taglio orizzontale del dendrogramma determina il numero di cluster.}
  \label{fig:dendograms}
\end{figure}

Il dendrogramma consente di visualizzare come i cluster si formano a diversi livelli di distanza, permettendo di scegliere il numero di cluster desiderato effettuando un taglio orizzontale al livello appropriato. Ad esempio, tagliando il dendrogramma a un'altezza specifica, si possono ottenere un certo numero di cluster, come mostrato nella Figura \ref{fig:dendograms}.

Può essere tuttavia utilizzato come \textbf{criterio di arresto} per gli algoritmi agglomerativi: si può decidere di fermarsi quando la distanza tra i cluster da unire supera una certa soglia, evitando così di dover specificare a priori il numero di cluster desiderato.

\subsection{Clustering divisivo}
Uno dei problemi che il clustering agglomerativo può incontrare è la \textbf{scelta del numero di cluster}: spesso non è noto a priori quanti cluster esistano nei dati, e scegliere un numero errato può portare a risultati di clustering subottimali. 

Il clustering \emph{divisivo} affronta questo problema iniziando con tutti i punti in un unico cluster e suddividendolo iterativamente in cluster più piccoli (fino a raggiungere un criterio di arresto). Questo approccio consente di esplorare la struttura dei dati in modo più flessibile, poiché non richiede la specifica del numero di cluster iniziale.

\subsection{Complessità computazionale}
Al primo passo si valuta la distanza per ogni coppia di cluster e si sceglie la migliore: costo $\Theta(n^2)$.
Dopo ogni fusione i cluster diminuiscono di uno, quindi i passi successivi costano, nell'ordine,
$(n-1)^2,(n-2)^2,\dots,2^2$.
\[
T_{\text{naive}}
=\sum_{k=2}^{n} k^{2}
=\frac{n(n+1)(2n+1)}{6}-1
=\Theta(n^{3}).
\]

Utilizzando però delle \textbf{code di priorità} per memorizzare le distanze tra i cluster e ottenere il minimimo in $O(1)$, si può ridurre la complessità a $\Theta(n^2 \log n)$:
\[
T_{\text{optimized}}
=\sum_{k=2}^{n} \left( O(\log k) + O(k) \right)
=O(n^2 \log n).
\]

\section{Clustering partizionale: K-means}
Il clustering partizionale è un'alternativa più efficiente rispetto al clustering gerarchico, specialmente per grandi dataset. Questi algoritmi cercano di partizionare i dati in un numero fisso di cluster, ottimizzando una funzione obiettivo che misura la qualità del clustering.

Il clustering partizionale più noto è il K-means, che mira a minimizzare la somma delle distanze quadrate tra i punti e i loro centroidi. L'algoritmo assume di conoscere a priori il numero di cluster $k$ e procede iterativamente attraverso i seguenti passi:
\begin{enumerate}
    \item Si scelgono inizialmente $k$ punti che abbiano probabilità alta di essere ben distribuiti (ad esempio, selezionandoli casualmente dal dataset) come centroidi iniziali.
    \item Si assegna ogni punto al cluster il cui centroide è più vicino, utilizzando la distanza euclidea.
    \item Si ricalcolano i centroidi di ogni cluster come la media dei punti assegnati a quel cluster.
    \item Si ripetono i passi 2 e 3 fino a quando i centroidi non cambiano più significativamente o fino a raggiungere un numero massimo di iterazioni.
\end{enumerate}

\subsection{Scelta greedy dei centroidi iniziali}
Un modo semplice per scegliere i centroidi iniziali è selezionarli casualmente dal dataset. Tuttavia, questa scelta può portare a risultati subottimali se i centroidi iniziali non sono ben distribuiti. Un modo ottimale è utilizzare una scelta \emph{greedy}\footnote{Un algoritmo greedy è un algoritmo che prende decisioni localmente ottimali in ogni passo con l'aspettativa di trovare una soluzione globale ottimale.} che massimizzi la distanza tra i centroidi iniziali:
\begin{enumerate}
    \item Si sceglie il primo centroide casualmente dal dataset $c_1$ e lo si aggiunge all'insieme $S$ dei punti scelti.
    \item Si aggiunge a $S$ il punto $x \in X \setminus S$ ($X$ è il dataset originale) che massimizza la distanza minima da tutti i punti già scelti:
    \[
    c_{i} = \arg\max_{x \in X \setminus S} \min_{y \in S} D(x, y)
    \]
    \item Si ripete il passo 2 fino a quando non si sono scelti $k$ centroidi.
\end{enumerate}

\subsection{Funzione obiettivo}
Per arrestare l'algoritmo K-means, si può utilizzare una funzione obiettivo che misura la qualità del clustering. Questa funzione può essere definita a paartire dalla somma delle distanze quadrate tra i punti e i loro centroidi:
\[
E = \sum_{i=1}^k \sum_{x \in C_i} D_2(x, c_i)^2
\]
dove $C_i$ è il cluster $i$ e $c_i$ è il centroide del cluster $i$. L'algoritmo K-means mira a minimizzare questa funzione obiettivo, e si può arrestare quando la variazione di $E$ tra due iterazioni consecutive è inferiore a una soglia predefinita.

\subsection{Scelta del numero di cluster}
Uno dei problemi del clustering partizionali è la scelta del numero di cluster $k$. Generalmente non si conoscono a priori il numero di cluster nei dati, e scegliere un valore errato può portare a risultati di clustering subottimali.

Si può quindi considerare il valore di $k$ come un \textbf{iperparametro}\footnote{Un parametro il cui valore è fissato prima dell'addestramento e non viene appreso dal modello.} da ottimizzare. 

\paragraph{Metodo elbow.}
Un metodo comune per scegliere il numero di cluster è il \textbf{metodo elbow}, che consiste nel calcolare la funzione obiettivo $E$ per diversi valori di $k$ e tracciare un grafico di $E$ in funzione di $k$. Si cerca quindi un punto nel grafico dove la diminuzione di $E$ inizia a rallentare, formando una "gomito" (elbow). Questo punto indica un buon compromesso tra la qualità del clustering e la complessità del modello.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/elbow-k.png}
  \caption{Metodo (\emph{elbow}). Si traccia la distanza media dal centroide (o WCSS/n) al variare di $k$; il valore “ottimo” è nel punto di flesso, dove l'aumento di $k$ porta benefici marginali trascurabili.}
  \label{fig:elbow}
\end{figure}

Dalla figura \ref{fig:elbow} notiamo che la funzione:
\[
W(k) = \frac{1}{n} \sum_{i=1}^k \sum_{x \in C_i} D_2(x, c_i)^2
\]
decresce al crescere di $k$, ma il tasso di decrescita diminuisce. Si sceglie quindi il valore di $k$ in corrispondenza del punto di flesso della curva, dove l'aggiunta di ulteriori cluster non porta a una riduzione significativa della somma delle distanze quadrate. Questo può essere anche individuato matematicamente supponendo che tra due valori $x$ e $y$ del parametro $k$ ci sia una differenza \emph{non trascurabile} nella distanza media dai centroidi:
\begin{enumerate}
    \item Si prende il valore medio $z = (x + y) / 2$ e si effettua il clustering per $k = z$.
    \item Se il valore della distanza media dai centroidi per $k = z$ è vicino a quello per $k = x$, allora si sceglie $k = x$, altrimenti si sceglie $k = y$.
    \item Si ripetono i passi 1 e 2 fino a quando non si trova il valore ottimale di $k$.
\end{enumerate}

\paragraph{Metodo silhouette.}
Un altro metodo per scegliere il numero di cluster è il \textbf{metodo silhouette}, che valuta la qualità del clustering calcolando un indice di silhouette per ogni punto. L'indice di silhouette misura \emph{quanto} un punto \emph{è ben assegnato al suo cluster} rispetto agli altri cluster. Per un punto $x_i$ appartenente al cluster $C_i$, si definiscono:
\begin{itemize}
    \item $a(i)$: la distanza media tra $x_i$ e tutti gli altri punti nel suo cluster $C_i$ (coesione):
    \[
    a(i) = \frac{1}{|C_i| - 1} \sum_{x_j \in C_i, j \neq i} D(x_i, x_j)
    \]
    \item $b(i)$: la distanza media tra $x_i$ e tutti i punti nel cluster più vicino a $C_i$ (separazione):
    \[
    b(i) = \min_{j \neq i} \frac{1}{|C_j|} \sum_{x_k \in C_j} D(x_i, x_k)
    \]
\end{itemize}

L'indice di silhouette $s(i)$ per il punto $x_i$ è quindi definito come:
\[
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\]

Questo valore è compreso in $[-1, 1]$, dove un valore vicino a 1 indica che il punto è ben assegnato al suo cluster, un valore vicino a 0 indica che il punto è al confine tra due cluster, e un valore negativo indica che il punto potrebbe essere stato assegnato al cluster sbagliato. 

Possiamo analizzare anche le funzioni $a(i), b(i)$:
\begin{itemize}
    \item Se $a(i) \ll b(i)$, allora $s(i) \approx 1$: il punto è ben assegnato al suo cluster. Questo perché la distanza media all'interno del cluster è molto più piccola rispetto alla distanza media al cluster più vicino.
    \item Se $a(i) \approx b(i)$, allora $s(i) \approx 0$: il punto è al confine tra due cluster. In questo caso, la distanza media all'interno del cluster è simile a quella al cluster più vicino, indicando che il punto non è chiaramente associato a un cluster specifico.
    \item Se $a(i) \gg b(i)$, allora $s(i) \approx -1$: il punto potrebbe essere stato assegnato al cluster sbagliato. Questo accade quando la distanza media all'interno del cluster è maggiore rispetto alla distanza al cluster più vicino, suggerendo che il punto sarebbe meglio posizionato in un altro cluster.
\end{itemize}

Grazie all'indice di silhouette, possiamo definire un problema di ottimizzazione per scegliere il numero di cluster $k$:
\[
k* = \arg\max_{k} \frac{1}{n} \sum_{i=1}^n s(i)
\]
Ovvero, scegliamo il numero di cluster che massimizza la media degli indici di silhouette su tutti i punti del dataset. Questo approccio consente di valutare la qualità del clustering in modo più dettagliato rispetto al metodo elbow, poiché tiene conto della coesione e della separazione dei cluster.

\subsection{Complessità computazionale}
Dato $t$ il numero di iterazioni dell'algoritmo K-means, $k$ il numero di cluster ed $n$ il numero di elementi del dataset, la complessità computazionale dell'algoritmo K-means è:
\[
T = O(t \cdot k \cdot n)
\]
Questo perché in ogni iterazione si devono assegnare tutti i $n$ punti ai $k$ cluster (costo $O(k \cdot n)$) e poi ricalcolare i centroidi (costo $O(n)$, trascurabile rispetto al costo di assegnazione).

\subsection{K-means su Big data}
Per clusterizzare grosse quantità di dati in spazi con elevato numero di dimensioni, che non possonoo risiedere in memoria principale, si utilizzano opportune varianti del $k$-means. Queste varianti, BFR e CURE, utilizzano una serie di statistiche e di valori per rappresentare in modo compatto i cluster e ottimizzare l'uso della RAM (gestendo anche gli outlier).

\section{Clustering per densità: DBSCAN}
Il clustering per densità definisce i cluster come \emph{regioni dello spazio a densità elevata} separate da regioni a densità bassa. A differenza dei metodi partizionali, non richiede di fissare a priori il numero di cluster $k$ e riesce a individuare cluster di \emph{forma arbitraria} (non necessariamente convessi). Inoltre, tratta in modo naturale il \emph{rumore} marcandolo come outlier.

\subsection{DBSCAN}
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) è un algoritmo di clustering per densità che identifica cluster come aree di alta densità separate da aree di bassa densità. In particolare una \emph{regione densa} è una regione contenente un numero di punti \emph{sufficientemente elevato} in un intorno dello spazio \emph{sufficientemente limitato}. DBSCAN funziona ad \textbf{intorni}\footnote{Un intorno di un punto $p$ è l'insieme di punti che si trovano entro una certa distanza $\varepsilon$ da $p$.}, che richiede due parametri:
\begin{itemize}
    \item $\varepsilon$ (eps): il raggio dell'intorno.
    \item $MinPts$: il numero minimo di punti richiesti per formare una regione densa.
\end{itemize}

\noindent
Da qui si possono definire tre tipi di punti, per un certo intorno $N_\varepsilon(Q)$\footnote{Intorno N di raggio $\varepsilon$ con centro Q}:
\begin{enumerate}
    \item \textbf{Punto $\mathbf{P}$ direttamente raggiungibile per densità da $\mathbf{Q}$}: se $P \in N_\varepsilon(Q)$ e $|N_\varepsilon(Q)| \geq MinPts$. Questo geometricamente, significa che $P$ si trova all'interno dell'intorno di $Q$ e che l'intorno di $Q$ contiene almeno $MinPts$ punti, indicando che $Q$ è in una regione densa.
    \item \textbf{Punto $\mathbf{P}$ raggiungibile per densità da $\mathbf{Q}$}: se esiste una catena di punti $P_1, P_2, \ldots, P_n$ tali che $P_1 = Q$, $P_n = P$ e ogni punto $P_{i+1}$ è direttamente raggiungibile per densità da $P_i$. Geometricamente, significa che si può raggiungere $P$ partendo da $Q$ attraversando una serie di punti, ciascuno dei quali si trova in una regione densa.
    \item \textbf{Punto $\mathbf{P}$ densamente connesso a $\mathbf{Q}$}: se esiste un punto $O$ tale che sia $P$ che $Q$ sono raggiungibili per densità da $O$. Geometricamente, significa che sia $P$ che $Q$ possono essere raggiunti partendo da un punto comune $O$ attraverso regioni dense.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/dbscan_directly_reachable.png}
  \caption{Directly density-reachable}
\end{subfigure}\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/dbscan_reachable.png}
  \caption{Density-reachable}
\end{subfigure}\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/dbscan_connected.png}
  \caption{Density-connected}
\end{subfigure}
\caption{Relazioni di raggiungibilità e connessione per densità in DBSCAN (parametri $\varepsilon$ e \texttt{minPts}).}
\label{fig:dbscan_reachability}
\end{figure}

\paragraph{Definizione di cluster in DBSCAN.}
Sulla base di queste definizione, DBSCAN definisce un cluster come un insieme massimale di punti densamente connessi. In altre parole, un cluster è formato da tutti i punti che possono essere raggiunti per densità l'uno dall'altro, partendo da un punto iniziale in una regione densa. Quindi fissati $\varepsilon$ e $\text{MinPts}$, se $D$ è il dataset iniziale, $C$ è un cluster se:
\begin{itemize}
    \item $\forall P, Q \in D$, se $P \in C$ e $Q$ è densamente connesso a $P$, allora $Q \in C$. Questo spiega la \textbf{massimalità} del cluster: se un punto $Q$ è densamente connesso a un punto $P$ già nel cluster, allora $Q$ deve essere incluso nel cluster.
    \item $\forall P, Q \in C$, $P$ è densamente connesso a $Q$. Questo spiega la \textbf{connettività} del cluster: ogni coppia di punti all'interno del cluster deve essere densamente connessa.
\end{itemize}

\paragraph{Algoritmo DBSCAN.}
Una volta definito il concetto di cluster, l'algoritmo DBSCAN procede come segue:
\begin{enumerate}
    \item Si inizia con un punto non visitato $P$ dal dataset $D$.
    \item Si calcola l'intorno $N_\varepsilon(P)$.
    \begin{enumerate}
        \item Se $|N_\varepsilon(P)| < MinPts$, allora $P$ è etichettato come rumore (outlier) e si passa al punto successivo.
        \item Altrimenti, si crea un nuovo cluster $C$ e si aggiunge $P$ a $C$.
    \end{enumerate}
    \item Si aggiunge $P$ e il nuovo intorno $N_\varepsilon(P)$ al cluster $C$. Ricorsivamente si aggiungono tutti i punti in $N_\varepsilon(P)$ che sono direttamente raggiungibili per densità da $P$.
    \item Si ripete il processo fino a quando tutti i punti del dataset sono stati visitati.
\end{enumerate}

Uno dei problemi che rimane da risolvere, però, è la \textbf{scelta dei parametri} $\varepsilon$ e $MinPts$. Generalmente se si lavora in un dataset $D$ di dimensione $m$ campioni e $n$ features si può scegliere $\text{MinPts} \ge n+1$ per garantire che un cluster abbia almeno una certa dimensione minima. Per scegliere $\varepsilon$, si può utilizzare il grafico delle distanze $k$-nearest neighbor (k-NN), tracciando la distanza del $k$-esimo vicino più prossimo per ogni punto del dataset e cercando un punto di flesso nel grafico che indichi un buon valore di $\varepsilon$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.68\textwidth]{images/kdistance_plot.png}
  \caption{$k$-distance plot: il gomito suggerisce il valore di $\varepsilon$.}
  \label{fig:kdist}
\end{figure}

\paragraph{Complessità computazionale.}
La complessità computazionale di DBSCAN dipende principalmente dal calcolo degli intorni per ogni punto del dataset. Se si utilizza una struttura dati efficiente come un albero KD o una griglia spaziale per accelerare la ricerca dei vicini, la complessità può essere ridotta a $O(n \log n)$, dove $n$ è il numero di punti nel dataset. Tuttavia, nella sua forma più semplice, senza strutture dati avanzate, la complessità è $O(n^2)$, poiché per ogni punto si deve calcolare l'intorno confrontandolo con tutti gli altri punti.

\subsection{OPTICS}
Uno dei problemi di DBSCAN è la difficoltà di scegliere i parametri $\varepsilon$ e $MinPts$, specialmente in dataset con densità variabile. OPTICS (Ordering Points To Identify the Clustering Structure) è un algoritmo di clustering per densità che affronta questo problema creando un ordinamento dei punti basato sulla loro densità locale, senza richiedere una scelta fissa di $\varepsilon$.

\paragraph{Distanza di raggiungibilità e area localizzata.}
OPTICS cerca di capire se due punti $A, B$ sono "abbastanza" vicini in termini di densità chiamandola \textbf{area localizzata} (\emph{core distance}) e \textbf{distanza di raggiungibilità} (\emph{reachability score}):
\begin{itemize}
    \item \textbf{Area localizzata} di un punto $A$ è la distanza tra $A$ e il suo $MinPts$-esimo vicino più prossimo. Se il numero di punti nell'intorno di $A$ è minore di $MinPts$, l'area localizzata è indefinita.
    \item \textbf{Distanza di raggiungibilità} di un punto $B$ da un punto $A$ è il massimo tra l'area localizzata di $A$ e la distanza tra $A$ e $B$:
    \[
    \text{reachability-distance}(A, B) = \max(\text{core-distance}(A), D(A, B))
    \]
\end{itemize}

\noindent
Per formalizzare questi concetti, definiamo due quantità per un punto $p$ e un intero $k$:
\begin{description}
  \item[\emph{Core-distance} di $p$:] è il raggio minimo che rende $p$ un punto \emph{core}. 
  In pratica è la distanza dal $k$-esimo vicino di $p$:
  \[
  \mathrm{core\_dist}_k(p)=d_k(p).
  \]
  Se $p$ ha meno di $k$ vicini, non è core e si pone $\mathrm{core\_dist}_k(p)=+\infty$ (non definita).
  
  \item[\emph{Reachability-distance} di $o$ da $p$:] misura quanto è “raggiungibile” $o$ partendo da $p$:
  \[
  \mathrm{reach\_dist}_k(o\,|\,p)=\max\{\mathrm{core\_dist}_k(p),\, D(p,o)\}.
  \]
\end{description}

Da queste definizioni si osserva che, se $p$ è un \emph{punto core}, allora la \emph{reachability-distance} di un punto $o$ a partire da $p$ è sempre definita ed è comunque \emph{non inferiore} alla $\mathrm{core\_dist}_k(p)$. Al contrario, se $p$ non è un punto core (cioè ha meno di $k$ vicini),
la reachability-distance di $o$ da $p$ non è definita.

Per chiarire il significato di questa definizione, consideriamo un esempio. Sia $k=5$ e supponiamo che
\[
\mathrm{core\_dist}_5(p)=0.8.
\]
Un punto $o$ a distanza $D(p,o)=0.6$ da $p$ avrà
\[
\mathrm{reach\_dist}_5(o \mid p)
= \max\{0.8,\,0.6\}
= 0.8,
\]
poiché domina la core-distance di $p$. Viceversa, un punto $o$ a distanza $D(p,o)=1.2$ da $p$ avrà
\[
\mathrm{reach\_dist}_5(o \mid p)
= \max\{0.8,\,1.2\}
= 1.2,
\]
in quanto prevale la distanza euclidea tra i due punti.


\paragraph{Reachability plot.}
OPTICS crea un ordinamento dei punti basato sulla loro reachability-distance, che può essere visualizzato in un \textbf{reachability plot}. In questo grafico, l'asse x rappresenta l'ordinamento dei punti e l'asse y rappresenta la reachability-distance di ciascun punto. Le aree con bassa reachability-distance indicano regioni di alta densità, mentre le aree con alta reachability-distance indicano regioni di bassa densità o rumore. Il reachability plot consente di identificare cluster di diverse densità senza dover specificare un valore fisso di $\varepsilon$: le valli nel grafico rappresentano cluster densi, mentre le creste rappresentano separazioni tra cluster.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{images/optics_reachability.png}
  \caption{OPTICS: esempio di \emph{reachability plot}. Le zone basse (valli) indicano cluster densi; le zone alte (creste) indicano separazioni. (immagine da libro/slide)}
  \label{fig:optics-reach}
\end{figure}

\paragraph{Estrazione dei cluster.}
Una volta ottenuto il reachability plot, si possono estrarre i cluster utilizzando diverse strategie. Una delle strategie più comuni è quella di utilizzare una soglia di reachability-distance per identificare le valli nel grafico che corrispondono ai cluster. In alternativa, si possono utilizzare regole basate sulla forma del grafico, come la regola \emph{steep down/up}, che identifica i punti in cui la reachability-distance scende bruscamente (indicando l'inizio di un cluster) e risale bruscamente (indicando la fine di un cluster).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{images/cluster_extraction_optics.png}
  \caption{OPTICS. (1) A sinistra: punti nel piano e, sotto, \emph{reachability plot}; le valli (segmenti colorati) corrispondono a regioni dense/cluster, mentre la linea orizzontale tratteggiata indica una soglia 
  $\epsilon$ che produce un taglio in stile DBSCAN. Le linee tratteggiate collegano ogni gruppo nel piano al suo intervallo nel plot. (2) A destra: regola \emph{steep down/up} per l'estrazione automatica dei cluster dal reachability plot (si entra quando la reachability scende bruscamente e si esce quando risale).}  
  \label{fig:optics-cluster-extraction}
\end{figure}

\paragraph{Complessità computazionale.}
La complessità computazionale di OPTICS è simile a quella di DBSCAN, poiché entrambi gli algoritmi si basano sul calcolo degli intorni dei punti. Utilizzando strutture dati efficienti come alberi KD o griglie spaziali, la complessità può essere ridotta a $O(n \log n)$, dove $n$ è il numero di punti nel dataset. Tuttavia, nella sua forma più semplice, senza strutture dati avanzate, la complessità è $O(n^2)$.

\subsection{HDBSCAN}
Uno dei problemi di OPTICS è la complessità computazionale, che può essere elevata per grandi dataset. HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) è un'estensione di DBSCAN e OPTICS che combina i vantaggi di entrambi gli algoritmi, creando una gerarchia di cluster basata sulla densità e utilizzando tecniche di pruning per migliorare l'efficienza. 

L'unico parametro che richiede è $MinPts$, mentre la scelta di $\varepsilon$ viene gestita automaticamente attraverso la costruzione della gerarchia dei cluster. Costruisce un dendrogramma simile a quello di OPTICS, ma utilizza una metrica di \emph{stabilità} per selezionare i cluster più significativi dalla gerarchia, migliorando così la qualità del clustering e riducendo il rumore.

Si possono definire:
\begin{itemize}
    \item Core distance di $X$: distanza dal $MinPts$-esimo vicino più prossimo di $X$.
    \item Densità di mutual reachability tra $X, Y$:
    \[
    D_{mreach}(X, Y) = \max\{\text{core\_dist}(X), \text{core\_dist}(Y), D(X, Y)\}
    \]
    \item Grafo di mutual reachability: grafo completo pesato con i punti del dataset come nodi e pesi degli archi dati dalla densità di mutual reachability.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{images/HDBSCAN_example.png}
  \caption{Esempio di clustering con HDBSCAN. }
  \label{fig:hdbscan-example}
\end{figure}

\paragraph{Cluster in HDBSCAN.}
Fissato un valore del raggio $\varepsilon$, si considera il grafo $G$ di \emph{mutual reachability}, ottenuto mantenendo solo gli archi con peso minore o uguale a $\varepsilon$. Un \textbf{cluster in HDBSCAN} è una \textbf{componente connessa} di $G$ che contiene almeno $MinPts$ punti.

Al variare di $\varepsilon$, la struttura del grafo cambia: riducendo $\varepsilon$, alcune componenti connesse possono \textbf{restringersi}, \textbf{dividersi in sotto-componenti} oppure \textbf{scomparire}, quando il numero di punti scende sotto la soglia $MinPts$. In questo modo si ottengono partizionamenti diversi dello spazio, corrispondenti a diversi livelli di densità.

Questa evoluzione dei cluster al variare di $\varepsilon$ induce naturalmente una \textbf{gerarchia di cluster}, che può essere rappresentata tramite un
\textbf{dendrogramma}. HDBSCAN seleziona infine i cluster più significativi valutandone la \textbf{stabilità}, ossia la loro persistenza lungo intervalli ampi di $\varepsilon$:
i cluster che sopravvivono più a lungo ai cambiamenti di densità vengono considerati i più robusti.


Da qui si può costruire il dendrogramma dei cluster:
\begin{itemize}
    \item Si costruisce il Minimum Spanning Tree (MST) del grafo di mutual reachability.
    \item Si ordina gli archi del MST in ordine decrescente di peso.
    \item Si rimuovono gli archi uno alla volta, partendo da quello con il peso più alto, e si tengono traccia delle componenti connesse risultanti.
    \item Ogni volta che si rimuove un arco, si crea un nodo nel dendrogramma che rappresenta la fusione delle componenti connesse.
\end{itemize}

A partire dal dendrogramma, HDBSCAN estrae l'insieme dei cluster sulla base della loro stabilità, ovvero della loro persistenza attraverso diversi livelli di densità. Un cluster è considerato stabile se rimane consistente su un intervallo significativo di valori di $\varepsilon$.

Sia $\lambda = 1 / \varepsilon$ una misura minimia di densità che un cluster $C$ deve avere a profondità $\varepsilon$ per esistere. Si definiscono:
\begin{itemize}
    \item $\lambda_{\text{min}}$: il valore di $\lambda$ al quale il cluster $C$ appare per la prima volta (nasce).
    \item $\lambda_{\text{max}}$: il valore di $\lambda$ al quale il cluster $C$ si dissolve (muore).
    \item $\lambda_{\text{max}}(X, C)$: il valore di $\lambda$ al quale il punto $X \in C$ lascia il cluster $C$ (se mai lo lascia).
\end{itemize}

\noindent
Da qui, definiamo la \textbf{stabilità} di un cluster $C$ come:
\[
S(C) = \sum_{X \in C} \left( \lambda_{\text{max}}(X, C) - \lambda_{\text{min}} \right)
\]
Questa misura quantifica quanto a lungo i punti rimangono nel cluster $C$ mentre la densità varia. Un cluster con alta stabilità indica che i suoi punti rimangono insieme su un ampio intervallo di densità, suggerendo che il cluster è robusto e significativo.

\paragraph{Estrazione dei cluster stabili.}
Poiché vogliamo estrarre tra i $K$ cluster del dendrogramma quelli più stabili (senza sovrapposizioni), si può formulare il problema come un problema di ottimizzazione:
\[
\begin{cases}
\displaystyle \max \;\; \sum_{i=1}^{K} \delta_i\, S(C_i) \\[6pt]
\delta_{1,\ldots,K} \in \{0,1\}^{K} \\[4pt]
\displaystyle \sum_{j \in H} \delta_j = 1 \qquad \forall\, H \in \mathrm{Leaf}(D)
\end{cases}
\]
dove $\mathrm{Leaf}(D)$ è l'insieme delle foglie del dendrogramma $D$ e ogni vincolo assicura che tra i cluster che condividono una foglia ne venga scelto uno solo e $I_h$ è l'insieme degli indici dei cluster che stanno nel cammino che porta dal nodo foglia $H$ alla radice del dendrogramma.

Questo problema di ottimizzazione può essere risolto in maniera \textbf{bottom-up} partendo dalle foglie del dendrogramma, decidendo, per ogni cluster $C$, se è meglio selezionare come soluzione finale $C$ al posto dei suoi discendenti (selezionando $C$ si ottiene un guadagno in stabilità pari a $S(C)$, mentre selezionando i discendenti si ottiene un guadagno pari alla somma delle loro stabilità):
\begin{enumerate}
    \item Si inizializza $\delta_i=1$ e si pone $S'(C_i) = S(C_i)$ per ogni cluster $C_i \in \text{Leaf}(D)$.
    \item Si procede risalendo il dendrogramma. Per ogni cluster $C$ non foglia, si calcola la somma delle stabilità dei suoi figli:
    \[
    S_{\text{children}} = \sum_{C_j \in \text{children}(C)} S'(C_j).
    \]
    \item Si confronta $S(C)$ con $S_{\text{children}}$:
    \begin{itemize}
        \item Se $S(C) > S_{\text{children}}$, si seleziona $C$ come cluster finale, ponendo $\delta_C = 1$ e $\delta_{C_j} = 0$ per ogni figlio $C_j$ di $C$. Si aggiorna inoltre $S'(C) = S(C)$.
        \item Altrimenti, si mantiene la selezione dei figli, ponendo $\delta_C = 0$ e mantenendo $\delta_{C_j}$ come sono per ogni figlio $C_j$ di $C$. Si aggiorna inoltre $S'(C) = S_{\text{children}}$.
    \end{itemize}
    \item Si ripete il passo 3 fino a raggiungere la radice del dendrogramma.
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.46\textwidth]{images/hdbscan_mst.png}\hfill
  \includegraphics[width=.46\textwidth]{images/hdbscan_condensed_tree.png}
  \caption{HDBSCAN. Sinistra: MST costruito sulla \emph{mutual reachability distance} (archi più pesanti vengono tagliati al crescere di \(\lambda\)). Destra: \emph{condensed tree}; in evidenza i cluster scelti massimizzando la stabilità.}
  \label{fig:hdbscan-figs}
\end{figure}