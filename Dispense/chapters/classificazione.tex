\chapter{Classificazione}\label{ch:classificazione}

\section{Introduzione}\label{sec:intro-class}
La \textbf{classificazione} suddivide un insieme di dati in \emph{classi} note a priori (etichette),
apprendendo da esempi etichettati come assegnare la classe a nuove tuple. È quindi
\emph{apprendimento supervisionato}. Al contrario, il \emph{clustering} non parte da etichette
(\emph{unsupervised}) e scopre gruppi per similarità.

\paragraph{Predizione (regressione).}
Quando il target è \emph{numerico continuo}, il compito è di \emph{predire} un valore reale
(apprendimento supervisionato \emph{continuo}), cercando una funzione che approssimi
il target, non un confine tra classi.

\subsection{Schema generale di un classificatore}\label{subsec:schema-class}
\begin{enumerate}
  \item \textbf{Costruzione del modello} (training): si apprende da un \emph{training set} etichettato.
  \item \textbf{Validazione/valutazione} (test): si misura la bontà su un \emph{test set} etichettato.
  \item \textbf{Uso} (deploy): si applica il modello a nuove tuple per predirne la classe.
\end{enumerate}

\paragraph{Overfitting.}
L’overfitting si verifica quando un modello “impara a memoria” il training, compreso il rumore: va molto bene sui dati visti ma generalizza male su dati nuovi. In pratica è un segnale che il modello è troppo complesso rispetto alle informazioni disponibili. Per ridurlo, si separano chiaramente i dati per la verifica e si preferiscono soluzioni più semplici quando offrono prestazioni simili.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.72\textwidth]{images/schema_classificatore.png}
  \caption{Schema a blocchi di un classificatore: addestramento, validazione e uso.}
  \label{fig:schema-class}
\end{figure}

\subsection{Requisiti desiderabili}\label{subsec:req}
\begin{itemize}
  \item \textbf{Accuratezza}: corretta predizione delle classi (o del valore, per i predittori).
  \item \textbf{Velocità}: tempi di training e di classificazione contenuti.
  \item \textbf{Robustezza}: tolleranza a rumore e dati mancanti.
  \item \textbf{Scalabilità}: efficienza su dataset di grandi dimensioni.
\end{itemize}

% ==========================================================
\section{Alberi decisionali}\label{sec:trees}
Gli \textbf{alberi decisionali} classificano applicando test su attributi lungo i nodi interni;
le \emph{foglie} portano le etichette di classe.

\subsection{Classificazione tramite albero}\label{subsec:tree-class}
La classe di una tupla $q$ si ottiene seguendo il cammino radice$\to$foglia guidato
dai test. Ogni cammino implementa una regola \texttt{IF-THEN} (le condizioni interne sono congiunte in AND). L’insieme di regole è \emph{esaustivo} e \emph{mutuamente esclusivo} (ogni tupla è coperta da una sola regola).

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{.50\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/weather_table.png}
  \end{minipage}\hfill
  \begin{minipage}[t]{.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/decision_tree_weather.png}
  \end{minipage}
  \caption{Dataset \emph{weather} (a sinistra) e albero decisionale appreso (a destra). 
  La tabella contiene 14 esempi con quattro attributi descrittivi (\texttt{Outlook}, \texttt{Temperature}, 
  \texttt{Humidity}, \texttt{Windy}) e la classe binaria \texttt{P/N}. 
  L’albero (stile ID3/C4.5) sceglie come radice \texttt{Outlook}; il ramo \texttt{overcast} porta 
  direttamente alla classe \texttt{P}, mentre per \texttt{sunny} si testa \texttt{Humidity} e per \texttt{rain} 
  si testa \texttt{Windy}. L’esempio illustra il passaggio da dati tabellari a regole interpretabili.}
  \label{fig:weather-tree}
\end{figure}

\subsection{Costruzione top–down}\label{subsec:topdown}
Costruzione ricorsiva dalla radice:
\begin{enumerate}
  \item Se tutte le tuple del nodo $X$ hanno la \emph{stessa} classe $C$, crea una foglia $C$.
  \item Altrimenti scegli un attributo $A$ (non ancora usato) e \emph{ramifica} $X$ (\emph{splitting}) secondo i valori/soglia di $A$; crea i figli.
  \item Per ogni figlio $X_i$: se puro, fermati; se impuro, ripeti ricorsivamente.
\end{enumerate}

\paragraph{Pruning.}
Se le tuple nel nodo sono poche o la profondità è elevata, si può fermare prima e rendere il nodo una foglia (vedere figura \ref{fig:weather-tree} con l'attributo "overcast").

\subsection{Splitting degli attributi}\label{subsec:splitting}
\begin{itemize}
  \item \textbf{Booleani/numerici}: split \emph{binario} su soglia $t$ (``$\le t$'' a sinistra, ``$>t$'' a destra).
  \item \textbf{Categoriali}: split \emph{binario} definendo un sottoinsieme non vuoto di valori (a sinistra se il valore \emph{non} appartiene al sottoinsieme, a destra altrimenti).
\end{itemize}

\subsection{Scelta dell’attributo e strategia greedy}\label{subsec:greedy}
L’albero minimale è un problema \emph{NP-hard}; si usa una strategia \emph{greedy} che, ad ogni passo, seleziona l’attributo con massima \emph{goodness} (partizioni più pure),
costruendo l’albero “più compatto” possibile.

\section{Misure di goodness}\label{sec:goodness}
La scelta dell’attributo si basa su misure di \emph{goodness}, che variano da algoritmo ad algoritmo.

\subsection{Information Gain (ID3)}\label{subsec:ig}
\paragraph{Idea.} L'\textbf{Information gain} è un algoritmo che si basa sull'idea di selezionare l'attributo che massimizza la riduzione dell'entropia riguardo alla classe delle tuple dopo lo split. Questo perché:
\begin{itemize}
  \item \textbf{Entropia massima}: si ha quando le classi sono equamente distribuite (massima incertezza).
  \item \textbf{Entropia minima}: si ha quando tutte le tuple appartengono alla stessa classe (certezza completa). 
\end{itemize}

\noindent
Sia $S_X$ l’insieme di tuple al nodo $X$, con due classi $P$ e $N$; si indichino con $p$
e $n$ le rispettive numerosità. L’\textbf{entropia} di $S_X$ è
\[
H(S_X)\;=\; -\frac{p}{p+n}\log_2\!\frac{p}{p+n}\;-\;\frac{n}{p+n}\log_2\!\frac{n}{p+n}.
\]
Sia $A$ un attributo con $k$ valori distinti, che induce la partizione
$S_X \to S_1,\dots,S_k$. Se $S_i$ contiene $p_i$ e $n_i$ elementi, allora
\[
H(S_i)\;=\; -\frac{p_i}{|S_i|}\log_2\!\frac{p_i}{|S_i|}\;-\;\frac{n_i}{|S_i|}\log_2\!\frac{n_i}{|S_i|}
\]

Possiamo anche calcolare l'\textbf{entropia media} dopo lo split su $A$:
\[
\overline{H}_A(S_X)\;=\;\sum_{i=1}^k \frac{|S_i|}{|S_X|}\,H(S_i).
\]

L’\textbf{information gain} è definito come la riduzione di entropia ottenuta dal partizionamento $S_x$ scegliendo l'attributo $A$:
\[
\mathrm{Gain}(S_X,A)\;=\;H(S_X)\;-\;\overline{H}_A(S_X).
\]
Si sceglie l’attributo con gain massimo.

\subsubsection*{Esempio e limitazioni}\label{par:ig-example}
Sul dataset “weather” (Fig.~\ref{fig:weather-tree}) si ottengono:
\[
\mathrm{Gain}(\textit{outlook})=0.246,\quad
\mathrm{Gain}(\textit{temperature})=0.029,\quad
\mathrm{Gain}(\textit{humidity})=0.151,\quad
\mathrm{Gain}(\textit{windy})=0.048.
\]

\paragraph{Limite noto.} L’Information Gain è \emph{sbilanciato} verso attributi con molti valori:
un attributo quasi univoco (es.\ \texttt{ID}) produce molte partizioni piccole (foglie pure),
abbattendo l’entropia media e gonfiando artificialmente , pur senza reale capacità
predittiva.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{images/ig_outlook_example.png}
  \caption{Esempio di scelta della radice con Information Gain sul dataset “weather”.}
  \label{fig:ig-weather}
\end{figure}

\subsection{Gain Ratio (C4.5)}\label{subsec:gain-ratio}
Il \textbf{Gain Ratio} corregge il bias dell’Information Gain verso attributi con molti valori introducendo la \emph{split information}, che misura quanta informazione è generata dal solo atto di partizionare i dati secondo l’attributo (indipendentemente dalla classe).

Sia $S$ l’insieme di tuple nel nodo corrente e $A$ un attributo che induce la partizione $S=S_1\cup\cdots\cup S_k$. Definiamo
\[
\mathrm{SplitInfo}(A,S)
= -\sum_{i=1}^k \frac{|S_i|}{|S|}\,\log_2\!\Big(\frac{|S_i|}{|S|}\Big),
\qquad
\mathrm{Gain}(A,S)=H(S)-\sum_{i=1}^{k}\frac{|S_i|}{|S|}\,H(S_i).
\]
Il \textbf{Gain Ratio} è
\[
\mathrm{GR}(A,S)=\frac{\mathrm{Gain}(A,S)}{\mathrm{SplitInfo}(A,S)}.
\]
\paragraph{Selezione in C4.5.} Per evitare divisioni spurie quando $\mathrm{SplitInfo}$ è piccola, C4.5 sceglie l’attributo con $\mathrm{GR}$ massimo tra quelli con \emph{Gain} non inferiore (ad es.) al gain medio del nodo. In pratica:
\begin{enumerate}
  \item calcola $\mathrm{Gain}(A,S)$ e scarta attributi con gain $\le 0$;
  \item tra i rimanenti, seleziona l’attributo con $\mathrm{GR}$ più alto.
\end{enumerate} 

\paragraph{Nota pratica (attributi continui).}
Per un attributo numerico $A$ si ordinano i valori e si valutano soglie candidate $t$ nelle posizioni fra due valori consecutivi: $A\le t$ vs $A>t$. Per ogni soglia si calcolano gain e gain ratio; si sceglie la soglia che massimizza la metrica.

% --------------------------------------------------------------------

\subsection{Gini Index (CART)}\label{subsec:gini}
Sia $i$ una classe e $T$ una tupla di classe $i$ scelta a caso da $S_x$. Per ricavare il \textbf{Gini Index} si calcola la probabilità che $T$ venga classificata erroneamente, ovvero che appartenga a una classe diversa da $i$: e quindi occorre considerare:
\begin{itemize}
  \item \textbf{Probabilità che $T$ sia di classe $i$}: $P(i\mid S_X)$.
  \item \textbf{Probabilità che $T$ sia di una classe diversa da $i$}: $1 - P(i\mid S_X)$.
\end{itemize}

\noindent
Dato che il ragionamento fatto vale per ogni classe, si sommano le probabilità di errore su tutte le classi:  
\[
\mathrm{Gini}(S_X) = \sum_{i=1}^n p_i (1-p_i) = \sum_{i=1}^n (p_i-p_i^2) = \sum_{i=1}^n p_i - \sum_{i=1}^n p_i^2 = 1 - \sum_{i=1}^n p_i^2 
\]

Con la supposizione che $S_x$ contenga $k$ classi e che $p_i$ sia la probabilità che una tupla scelta a caso da $S_X$ appartenga alla classe $i$, si ha che il \textbf{Gini Index} dello split è defininito come:
\[
\mathrm{Gini_{split}}(S_X) = 1 - \sum_{i=1}^k \frac{|S_i|}{|S_x|}Gini(S_i) 
\]

CART seleziona l’attributo/soglia che \emph{minimizza} $\mathrm{GiniSplit}$. Su attributi categoriali si cercano partizioni in due sottoinsiemi di valori; su continui, soglie come in C4.5.

% --------------------------------------------------------------------

\subsection{Pruning degli alberi}\label{subsec:pruning}
Alberi molto profondi generalizzano male (generano \emph{overfitting}). Per evitare questo, si effettua un \textbf{pruning}, ovvero si riduce la dimensione dell'albero sostituendo un sottoalbero con una foglia etichettato con la classe maggioritaria delle tuple nel sottoalbero, il pruning inserisce però un tasso di errore, si fa solo se necessario. Si usano due strategie principali:
\begin{description}
  \item[Pre-pruning] in fase di costruzione dell'albero si interrompe la crescita quando la goodness dello split è al di sopra di una \emph{soglia}.
  \item[Post-pruning] si costruisce l'albero completo e poi lo si riduce valutando l'errore su validation set o tramite stima incrociata. Generalmente è più dispensioso ma più efficace.
\end{description}

\subsubsection*{Pruning pessimistico (C4.5)}
Confronta l’errore stimato del \emph{sottoalbero} $T$ radicato in $X$ con l’errore stimato della \emph{foglia} che sostituisce $T$ (classe maggioritaria in $X$).

Sia \(X\) un nodo dell’albero con insieme di esempi \(S_x\) (\(N=|S_x|\)) e classe di maggioranza \(C\).
Sia \(T\) il sottoalbero radicato in \(X\) e siano \(x_1,\dots,x_k\) i figli immediati di \(X\), con
\(S_{x_i}\) gli esempi nel figlio \(x_i\) e \(C_i\) la sua classe di maggioranza. Le due quantità
\[
\begin{aligned}
E_p(T)   &= \frac{\bigl|\{\, t \in S_x \mid \mathrm{class}(t) \neq C \,\}\bigr| + \epsilon}{\lvert S_x\rvert}\\[4pt]
E_p'(T) &= \frac{\displaystyle \sum_{i=1}^{k} \bigl|\{\, t \in S_{x_i} \mid \mathrm{class}(t) \neq C_i \,\}\bigr| + k\,\epsilon}{\lvert S_x\rvert}
\end{aligned}
\]
sono le \textbf{stime del tasso di errore} usate per decidere se fare pruning.

\paragraph{Che cosa misurano.}
\begin{itemize}
  \item \(E_p(T)\) è l’\emph{errore stimato} se \textbf{si pota} \(T\) sostituendo l’intero sottoalbero con \emph{una sola foglia} etichettata con la classe di maggioranza \(C\) del nodo \(X\). Il numeratore conta le istanze di \(S_x\) che verrebbero sbagliate da tale foglia, con una \emph{correzione} \(\epsilon\) (tipicamente \(\epsilon=\tfrac12\)) per evitare stime troppo ottimistiche su campioni piccoli.
  \item \(E_p'(T)\) è l’\emph{errore stimato} se \textbf{si mantiene lo split corrente} di \(X\) nei suoi \(k\) figli, ma \emph{troncando} ognuno di essi a foglia (ognuna etichettata con la propria maggioranza \(C_i\)). Si sommano gli errori dei \(k\) figli e si aggiunge una correzione \(\epsilon\) per \emph{ciascuna} foglia (\(k\epsilon\)).
\end{itemize}

\paragraph{Decisione di pruning.}
Confrontando \(E_p(T)\) ed \(E_p'(T)\):
\begin{itemize}
  \item per \(E_p(T) \le E_p'(T)\), \emph{potare} il nodo \(X\) è preferibile, poiché l’errore stimato come foglia è minore o uguale a quello del sottoalbero.
  \item per \(E_p(T) > E_p'(T)\), conviene \emph{mantenere} lo split, in quanto l’errore stimato del sottoalbero è inferiore a quello della singola foglia.
\end{itemize}

Il valore $\epsilon$ è una sorta di “costo fisso” per ogni foglia aggiunta all’albero. L’aggiunta di questo valore agisce da \emph{regolarizzatore}: penalizza strutture con molte foglie, evitando che piccole fluttuazioni del campione giustifichino split inutili.

\subsubsection*{Cost–complexity pruning (CART)}
Si valuta il vantaggio dello \emph{split} di un nodo \(X\) confrontando la riduzione di \emph{error rate} con l’aumento di complessità (nuove foglie).

\paragraph{Errore prima e dopo lo split.}
Sia \(S_X\) l’insieme dei campioni che arrivano al nodo \(X\) e \(C\) la classe maggioritaria in \(S_X\).
\[
E(X)=\frac{\bigl|\{\,t\in S_X:\ \mathrm{class}(t)\neq C\,\}\bigr|}{|S_X|}.
\]
Se \(X\) viene diviso in \(k\) figli \(X_1,\dots,X_k\) (con classi maggioritarie \(C_1,\dots,C_k\)), l’errore \emph{atteso dopo} lo split è
\[
E'(X)=\frac{\sum_{i=1}^{k}\bigl|\{\,t\in S_{X_i}:\ \mathrm{class}(t)\neq C_i\,\}\bigr|}{|S_X|}.
\]

\paragraph{Indice di costo–complessità per lo split.}
Definiamo il guadagno medio per foglia aggiunta:
\[
\alpha(X)=\frac{E(X)-E'(X)}{k-1}.
\]

Il valore \(\alpha\) misura \emph{quanto} diminuisce l’errore per ogni foglia extra introdotta dallo split. Se \(\alpha\) è sufficientemente piccolo, ovvero quando $\alpha$ è minore di una soglia prefissata \(\alpha_0\), lo split non è conveniente e si pota il nodo \(X\).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/tree_pruning_example.png}
  \caption{Pruning: confronto errore stimato del sottoalbero vs foglia (C4.5) e principio costo–complessità (CART).}
  \label{fig:pruning}
\end{figure}

\paragraph{Pro/contro degli alberi decisionali.}
\emph{Pro:} interpretabili, veloci in predizione, gestiscono mix di attributi (continui/categoriali), poca preparazione dei dati. \emph{Contro:} instabili rispetto a piccole variazioni dei dati, propensi all’overfitting, separazioni per soglie assiali (forme complesse richiedono molti nodi), accuratezza spesso inferiore a ensemble o SVM su dati ad alta dimensionalità.

% --------------------------------------------------------------------

\section{Classificatori generativi}\label{sec:generative}
I modelli generativi producono un \textbf{modello probabilistico} a partire dai dati, predicendo la \textbf{classe} di appartenenza \emph{più probabile} per un nuovo dato a partire dal modello sviluppato. Questi modelli si basano sul \textbf{teorema di Bayes}.

\subsection{Teorema di Bayes e regola di decisione}\label{subsec:bayes-rule}
Sia $\mathbf{x}$ un’osservazione e $c\in\mathcal{C}$ una classe candidata. Per decidere la
classe usiamo il \textbf{teorema di Bayes}:
\[
P(c\mid \mathbf{x})=\frac{P(\mathbf{x}\mid c)\,P(c)}{P(\mathbf{x})}.
\]
\begin{itemize}
  \item \textbf{Probabilità a priori} $P(c)$: quanto la classe $c$ è probabile \emph{prima} di vedere i dati (in pratica: frequenza della classe nel train).
  \item \textbf{Likelihood} $P(\mathbf{x}\mid c)$: quanto è plausibile osservare $\mathbf{x}$ \emph{se} la classe fosse $c$.
  \item \textbf{Evidenza} $P(\mathbf{x})$: probabilità complessiva di osservare $\mathbf{x}$ (uguale per tutte le classi).
\end{itemize}
La decisione ottima \emph{MAP} (Maximum A Posteriori) è
\[
\hat{c}(\mathbf{x})=\arg\max_{c\in\mathcal{C}} P(c\mid \mathbf{x})
=\arg\max_{c\in\mathcal{C}} P(\mathbf{x}\mid c)\,P(c),
\]
poiché $P(\mathbf{x})$ non dipende da $c$ e non influisce sull’$\arg\max$.
\subsection{Naive Bayes}\label{subsec:naive-bayes}
\paragraph{Idea.}
Assumiamo che, fissata la classe \(c\), le feature siano indipendenti (\emph{assunzione naive}). Allora la verosimiglianza fattorizza:
\[
P(\mathbf{x}\mid c)=\prod_{j=1}^{d} P(x_j\mid c).
\]

\paragraph{Regola di decisione (MAP, in scala logaritmica).} Le probabilità condizionali sono molto piccole e un prodotto di tante quantità prossime a 0 può portare problemi di underflow. Per ovviare a questi problemi si considera il \textbf{log-likelihood}:
\[
\hat{c}(\mathbf{x})=\arg\max_{c\in\mathcal{C}}
\Big[\log P(c)+\sum_{j=1}^{d}\log P(x_j\mid c)\Big].
\]

\noindent
Questo si traduce in una somma anziché in un prodotto di termini.

\paragraph{Stima essenziale delle probabilità.}
\begin{itemize}
  \item \textbf{Prior} \(P(c)\): frequenza della classe nel training.
  \item \textbf{Attributi discreti}: frequenze condizionate con \emph{Laplace smoothing} \((+\alpha)\) per evitare zeri.
  \item \textbf{Attributi continui}: modello gaussiano per \(x_j\mid c\) con media e varianza stimate sui dati della classe.
\end{itemize}

\paragraph{Vantaggi e svantaggi.} Molto veloce e facile da implementare, l'assunzione di indipendenza condizionale potrebbe non essere sempre vera e potrebbe portare ad una perdita di accuratezza (tali dipendenze non possono essere modellate da questo modello).
% --------------------------------------------------------------------

\subsection{Reti Bayesiane}\label{subsec:bayesnet}
Una \textbf{rete bayesiana} è un DAG le cui variabili $\{X_1,\dots,X_d\}$ fattorizzano come
\[
P(X_1,\dots,X_d)=\prod_{i=1}^d P(X_i\mid \mathrm{Pa}(X_i)),
\]
dove $\mathrm{Pa}(X_i)$ sono i genitori di $X_i$ nel grafo. Il DAG codifica indipendenze condizionali; i CPT (tabelle di probabilità condizionate) specificano i parametri.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{images/bayes_network_example.png}
  \caption[Bayes net Sprinkler–Rain–Grass]{Rete bayesiana \emph{Sprinkler–Rain–Grass}: il DAG orientato specifica le dipendenze e indetermina la fattorizzazione della congiunta
  \(P(\text{Rain})\,P(\text{Sprinkler}\mid \text{Rain})\,P(\text{GrassWet}\mid \text{Sprinkler},\text{Rain})\).
  Le tabelle mostrano le CPD (Conditional Probability Tables) dei nodi.
  A differenza di Naive Bayes, le feature possono essere dipendenti dato la/e causa/e (qui \textit{Rain}), e tale dipendenza è resa esplicita dagli archi.}
  \label{fig:bayes-net}
\end{figure}

\paragraph{Uso per la classificazione.}
Dato $\mathbf{x}$, si calcolano (o si approssimano) $P(y\mid \mathbf{x})$ tramite inferenza sul DAG (\emph{esatta} o \emph{approx} con sampling/variational). Le reti bayesiane generalizzano Naive Bayes (che è un caso particolare con $Y$ genitore di tutte le feature e nessun’altra dipendenza).

\section{Classificatori discriminativi}\label{sec:discriminativi}
I classificatori discriminativi stimano direttamente una funzione di decisione $f:\mathbb{R}^d\to\mathbb{R}$ (o, opzionalmente, la probabilità condizionata $P(y\mid\mathbf{x})$) senza modellare la distribuzione congiunta $P(\mathbf{x},y)$. Dato un nuovo esempio $\mathbf{x}$ si valuta $f(\mathbf{x})$ e si assegna l'etichetta corrispondente. Rispetto ai modelli generativi richiedono generalmente meno assunzioni sui dati. Tipici esempi sono il Perceptron e le Support Vector Machines (SVM).

\subsection{Classificazione lineare e non lineare}\label{subsec:lin-nonlin}

\begin{description}
  \item[Lineare:] la regola di decisione è basata su una combinazione lineare degli attributi
  \[
  f(\mathbf{x})=\mathbf{w}\cdot\mathbf{x}+b,
  \]
  Dove $\mathbf{w}\in\mathbb{R}^d$ è il vettore dei pesi e $b\in\mathbb{R}$ è il bias (termine di soglia). 
  \item[Non lineare:] Il problema di avere spazi non lineare è che non è possibile separare le classi con un iperpiano. Per risolvere questo problema si fa una trasformazioni di spazi vettoriali in spazi di dimensione superiore dove la separazione lineare è possibile. Questo si ottiene tramite il \textbf{kernel trick}, che permette di calcolare prodotti scalari in spazi trasformati senza dover esplicitamente mappare i dati.
\end{description}

\noindent
\textit{Esistono anche classificazioni lineari e non lineari binarie, dove si utilizzano approcci simili}.

\subsection{Perceptron}\label{subsec:perceptron}
\paragraph{Definizione.}
Il Perceptron è un classificatore lineare binario che produce la funzione
\[f(\mathbf{x})=\mathbf{w}\cdot\mathbf{x}+b.\]
Con etichette $y\in\{-1,+1\}$. Si parla di un algoritmo di machine learning, quindi è necessario un modo, per il perceptron, di apprendere i parametri $\mathbf{w}$ e $b$ dai dati di addestramento.

\paragraph{Regola di aggiornamento.}
Dato un esempio $(\mathbf{x}^{(i)},y^{(i)})$, se la previsione $\hat{y}^{(i)}$ è errata (ossia $\hat{y}^{(i)}\neq y^{(i)}$) si aggiorna ogni peso component-wise secondo la notazione usata in figura:
\[
\hat{\vartheta}_j \leftarrow \vartheta_j + \alpha\,\bigl(y^{(i)}-\hat{y}^{(i)}\bigr)\,x_j^{(i)},\qquad j=0,\dots,d,
\]
dove $\alpha>0$ è il learning rate, $\vartheta_j$ indica il valore corrente del peso e $\hat{\vartheta}_j$ il valore aggiornato. Equivalentemente, in forma vettoriale si ottiene
\[
\mathbf{w}\leftarrow \mathbf{w} + \alpha\,\bigl(y^{(i)}-\hat{y}^{(i)}\bigr)\,\mathbf{x}^{(i)},\qquad b\leftarrow b + \alpha\,\bigl(y^{(i)}-\hat{y}^{(i)}\bigr).
\]
Se $\hat{y}^{(i)}=y^{(i)}$ il modello non viene modificato.

\paragraph{Proprietà.}
Se i dati sono linearmente separabili, il Perceptron converge in un numero finito di aggiornamenti (teorema di Novikoff). In pratica si itera per più epoche o fino a soddisfare un criterio di stop.

\paragraph{Algoritmo.}
L'algoritmo del perceptron è definito come:
\begin{enumerate}
  \item Inizializza i pesi $\mathbf{w}$ e il bias $b$ a zero o a valori casuali.
  \item Per ogni tupla $y_j$ nel training set:
  \begin{enumerate}
    \item Calcola la previsione $\hat{y}^{(i)}$
    \item Se $\hat{y}^{(i)}\neq y^{(i)}$, aggiorna i pesi e il bias secondo la regola di aggiornamento.
    \item Incrementa $i$ e ripeti fino a completare il training set.
  \end{enumerate}
  \item Ripeti il passo 2 per un numero prefissato di epoche o fino a soddisfare un criterio di stop. Nel caso di learning offline, si ripete il passo 2 finché l'errore medio di classificazione sul training set non scende sotto una soglia prefissata.
\end{enumerate}

\paragraph{One–Vs–One (OVO).}
One–Vs–One costruisce un classificatore binario Perceptron per ogni coppia di classi $(C_p,C_q)$; per $K$ classi si addestrano $K(K-1)/2$ modelli. In fase di predizione, ogni classificatore vota per una delle due classi che confronta e si assegna la classe con il maggior numero di voti (voting).

Motivazione: OVO è utile quando le classi sono relativamente poche e si desidera che ogni modello risolva una decisione binaria semplice; ogni modello vede dati di due sole classi, spesso permettendo separazioni più semplici e modelli più piccoli. Lo svantaggio principale è il numero di classificatori e la gestione del voto/pareggio.

\noindent
Algoritmo (per ciascuna coppia $(C_p,C_q)$):
\begin{enumerate}
  \item Costruisci il training set rimuovendo istanze non appartenenti a $C_p$ o $C_q$.
  \item Inizializza i pesi $\vartheta_j$ e il bias.
  \item Per ogni epoca e per ogni esempio $(\mathbf{x}^{(i)},y^{(i)})$ nel sottoinsieme: calcola $\hat{y}^{(i)}=\operatorname{sign}(f(\mathbf{x}^{(i)}))$; se $\hat{y}^{(i)}\neq y^{(i)}$ aggiorna
  \[\hat{\vartheta}_j\leftarrow\vartheta_j+\alpha\,(y^{(i)}-\hat{y}^{(i)})\,x_j^{(i)}\quad(j=0,\dots,d).\]
\end{enumerate}

\paragraph{One–Vs–All (OVA).}
One–Vs–All costruisce un classificatore Perceptron per ogni classe $C_k$ dove il problema è $C_k$ vs "resto". Si addestrano $K$ modelli; alla predizione si calcola il punteggio $f_k(\mathbf{x})$ per ogni modello e si sceglie la classe con il punteggio più alto.

Motivazione: OVA è più parsimonioso in termini di numero di modelli rispetto a OVO (si addestrano $K$ modelli invece di $K(K-1)/2$) e può essere più efficiente quando $K$ è grande. Tuttavia ogni modello OVA affronta un problema sbilanciato (una classe vs tutte le altre), il che può richiedere tecniche di bilanciamento o regolarizzazione.

\noindent
Algoritmo (per ciascuna classe $C_k$):
\begin{enumerate}
  \item Crea etichette binarie $y^{(i)}=+1$ se l'esempio appartiene a $C_k$, altrimenti $y^{(i)}=-1$.
  \item Inizializza i pesi $\vartheta_j$ e il bias.
  \item Per ogni epoca e per ogni esempio $(\mathbf{x}^{(i)},y^{(i)})$: calcola $\hat{y}^{(i)}=\operatorname{sign}(f(\mathbf{x}^{(i)}))$; se $\hat{y}^{(i)}\neq y^{(i)}$ aggiorna
  \[\hat{\vartheta}_j\leftarrow\vartheta_j+\alpha\,(y^{(i)}-\hat{y}^{(i)})\,x_j^{(i)}\quad(j=0,\dots,d).\]
\end{enumerate}

\noindent
\textit{Breve nota comparativa: OVO tende a produrre modelli più specialistici e può funzionare meglio quando le classi sono ben separate a coppie; OVA è più semplice ed efficiente per molti problemi pratici ma richiede attenzione allo sbilanciamento delle classi}.

\subsection{Support Vector Machines (SVM)}\label{subsec:svm}
Le SVM hanno un'idea diversa dal perceptron: cercano di trovare l'iperpiano che massimizza il \textbf{margine} tra le classi, ovvero la distanza minima tra l'iperpiano e i punti dati più vicini di ciascuna classe (i \emph{support vectors}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{images/svm_margin.png}
  \caption{Esempio di Support Vector Machine che massimizza il margine tra due classi. I punti cerchiati sono i support vectors che definiscono l'iperpiano ottimale.}
  \label{fig:svm-margin}
\end{figure}

Per dati linearmente separabili, esistono infinite iperpiani che separano le classi; le SVM scelgono quello con il margine massimo, che tende a generalizzare meglio su dati non visti.

\paragraph{Formulazione del problema.}
Sia $D$ un dataset di addestramento formato da punti \((\mathbf{x}^{(i)},y^{(i)})\), con etichette \(y^{(i)} \in \{-1,+1\}\), pesi \(\mathbf{w}\) e bias \(b\). L'iperpiano di decisione è definito da:
\[
f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b.
\]
Se indichiamo con $\gamma$ il margine, l'obiettivo delle SVM è trovare i parametri $w, b$ che massimizzano $\gamma$. Indicando con
\begin{align*}
  &H_1: \mathbf{w} \cdot \mathbf{x} + b = \gamma \\
  &H_2: \mathbf{w} \cdot \mathbf{x} + b = -\gamma
\end{align*}
le equazioni dei due iperpiani paralleli che definiscono il margine, passanti per i vettori di supporto delle due classi. Da questo si può dedurre che i punti di classe $+1$ soddisfano la disuguaglianza \(\mathbf{w} \cdot \mathbf{x}^{(i)} + b \geq \gamma\), mentre i punti di classe $-1$ soddisfano \(\mathbf{w} \cdot \mathbf{x}^{(i)} + b \leq -\gamma\). 

Combinando queste due condizioni, si ottiene la seguente disuguaglianza per tutti i punti del dataset:
\[
y^{(i)}(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) \geq \gamma, \quad \forall i.
\]
Poiché $y^{(i)}$ può essere $+1$ o $-1$, questa condizione assicura che ogni punto sia correttamente classificato e si trovi al di fuori del margine. Tuttavia si può riscrivere la formulazione in modo più semplice, fissando \(\gamma = 1\), il che porta alla condizione:
\[
y^{(i)}(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) \geq 1, \quad \forall i.
\]

Purtroppo questa formulazione "libera" in termini di $\gamma$ non è direttamente utilizzabile: se $(\mathbf{w},b)$ soddisfa i vincoli, allora anche $(\lambda\mathbf{w},\lambda b)$ soddisfa i vincoli per ogni scalare $\lambda>1$ e fornisce un margine $\lambda\gamma$ più grande. Di conseguenza non esiste un massimo finito per $\gamma$ senza imporre una normalizzazione aggiuntiva. Per evitare questa degenerazione si fissa implicitamente la scala del vettore dei pesi fissando il valore del margine (ad es. $\gamma=1$) e si minimizza la norma di $\mathbf{w}$, che è equivalente a massimizzare il margine in modo ben definito. La soluzione del problema è quella di normalizzare il vettore dei pesi in modo che il margine sia fissato a 1, quindi trasformare il vettore $\mathbf{w}$ in un vettore:
\[
\hat{\mathbf{w}} = \frac{\mathbf{w}}{\|\mathbf{w}\|}.
\]

Il vettore $\hat{\mathbf{w}}$ è il vettore unità (modulo pari a 1) dei pesi normalizzato con stessa direzione di $\mathbf{w}$. Inoltre, poiché ha la stessa direzione è perpendicolare all'iperpiano separatore:

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{images/normalized_hyperplane_svm.png}
  \caption{Iperpiani con pesi $\mathbf{w}$ normalizzati per margine unitario.}
  \label{fig:svm-margin-normalized}
\end{figure}

Indicando con $x_2$ un vettore di supporto che giace su $H_2$ e $x_1$ la sua proiezione in $H_1$ (figura \ref{fig:svm-margin-normalized}), la distanza tra i due iperpiani è data da:
\[
x_1 = x_2 + 2 \gamma = x_2 + 2 \gamma \frac{\mathbf{w}}{\|\mathbf{w}\|}
\]

\noindent
Possiamo ricalcolare l'equazione dell'iperpiano come:
\begin{align*}
&\mathbf{w}\cdot\bigl(x_2 + 2\gamma \frac{\mathbf{w}}{\|\mathbf{w}\|}\bigr) + b = 0 \\
\Rightarrow\quad &\mathbf{w}\cdot x_2 + b + 2\gamma \|\mathbf{w}\| = 0
\end{align*}

Osservando che $x_2$ giace su $H_2$ abbiamo $\mathbf{w}\cdot x_2 + b = -\gamma$. Sostituendo nell'ultima equazione:
\[
-\gamma + 2\gamma\,\|\mathbf{w}\| = 0 \quad\Longrightarrow\quad 2\gamma\,\|\mathbf{w}\| = \gamma.
\]
Se $\gamma\neq 0$ segue
\[
\|\mathbf{w}\| = \tfrac{1}{2},
\]
ma ricordando che nella definizione del margine la distanza tra le due hyperplane considerate è $2\gamma$, fissando la normalizzazione standard si ricava l'identità
\[
\gamma = \frac{1}{\|\mathbf{w}\|},
\]
da cui la conclusione voluta: massimizzare $\gamma$ equivale a minimizzare $\|\mathbf{w}\|$ (e quindi, per praticità numerica, si minimizza spesso $\tfrac{1}{2}\|\mathbf{w}\|^2$).

Il problema di ottimizzazione da risolvere, quindi, è:
\[
\begin{cases}
\displaystyle\min\;\|\mathbf{w}\|\\[6pt]
	y^{(i)}(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\ge 1\quad\forall\;1\le i\le n
\end{cases}
\]
che si può risolvere applicando il metodo di discesa del gradiente.

Nella pratica, per rendere più semplice il calcolo dei gradienti si preferisce considerare il quadrato della norma:
\[
\begin{cases}
\displaystyle\min\;\tfrac{1}{2}\,\|\mathbf{w}\|^2\\[6pt]
	y^{(i)}(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\ge 1\quad\forall\;1\le i\le n
\end{cases}
\]

\paragraph{SVM Soft margin.} Esiste un problema nella classificazione di SVM chiamato \textbf{Hard Margin}: ovvero una classificazione dei dati troppo precisa, in quanto non può ricadere all'interno dell'intervallo dei due iperpiani. Una variante si chiama \textbf{Soft Margin}, che permette ad alcuni punti di cadere all'interno del margine o addirittura di essere classificati in modo errato, introducendo delle variabili di slack $\xi_i\ge 0$ per ogni punto dati. 

Per riformulare il problema, si introducono $n$ variabili $\epsilon_1, \epsilon_2, ..., \epsilon_n$ tale che, quando $\epsilon_i = 0$, il punto ricade al di fuori del margine e soddisfa la relazione:
\[
y^{(i)}(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\ge 1,
\]

\noindent
Altrimenti se $\epsilon_i > 0$, il punto ricade all'interno del margine o è classificato in modo errato, e la relazione diventa:
\[
y^{(i)}(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)\ge 1 - \epsilon_i.
\]

Da qui si introduce un parametro $\lambda$ di \textbf{trade-off}\footnote{Il parametro di trade-off $\lambda$ bilancia l'importanza tra la massimizzazione del margine e la minimizzazione dell'errore di classificazione. Un valore più alto di $\lambda$ dà più peso alla riduzione degli errori, mentre un valore più basso enfatizza la massimizzazione del margine.} tra la minimizzazione della norma di $\mathbf{w}$ e la penalizzazione degli errori di classificazione, portando alla seguente formulazione del problema di ottimizzazione:
\[
\begin{cases}
\displaystyle\min\;\biggl(\frac{1}{n}\sum_{i=1}^n\epsilon_i + \lambda\,\|\mathbf{w}\|^2\biggr)\\[6pt]
\qquad y^{(i)}\bigl(\mathbf{w}\cdot\mathbf{x}^{(i)}+b\bigr)\ge 1-\epsilon_i, & \forall\; i=1,\dots,n\\[4pt]
\qquad \epsilon_i\ge 0, & \forall\; i=1,\dots,n
\end{cases}
\]

\noindent
Più piccolo è $\lambda$, più trascurabile è $\|\mathbf{w}\|$, meno importante è la dimensione del margine.

Il problema può essere risolto anche considerando il suo duale tramite la Lagrangiana. Introducendo i moltiplicatori di Lagrange $\alpha_i$ per i vincoli di classificazione (e $\mu_i$ per i vincoli $\epsilon_i\ge0$) si ottiene una funzione quadratica in $\alpha_i$, quindi il duale è una QP risolvibile con algoritmi dedicati. Ricavati i moltiplicatori $\alpha_i$, il vettore dei pesi si esprime come
\[
\mathbf{w}=\sum_{i=1}^n \alpha_i\,y^{(i)}\,\mathbf{x}^{(i)}.
\]
Il bias $b$ si ricava:
\[
y^{(i)}(\mathbf{w}\cdot\mathbf{x}^{(i)}+b)=1 \quad\Longrightarrow\quad b=\frac{1}{y^{(i)}} - \mathbf{w}\cdot\mathbf{x}^{(i)}.
\]

Nel caso di dati non linearmente separabili si effettua un \emph{mapping} in uno spazio di dimensione superiore tramite una funzione $\phi:\mathbb{R}^d\to\mathbb{R}^D$ (con $D>d$) e si applica SVM in tale spazio. Quindi, creiamo un nuovo problema di ottimizzazione dove al posto di $\mathbf{x}^{(i)}$ usiamo $\phi(\mathbf{x}^{(i)})$:
\[
\begin{cases}
\displaystyle\max\;\Biggl(\sum_{i=1}^n c_i - \tfrac{1}{2}\sum_{i=1}^n\sum_{j=1}^n y^{(i)} c_i\,\bigl(\phi(\mathbf{x}^{(i)})\!\cdot\!\phi(\mathbf{x}^{(j)})\bigr)\,y^{(j)} c_j\Biggr)\\[6pt]
\qquad \displaystyle\sum_{i=1}^n c_i\,y^{(i)} = 0\\[6pt]
\qquad 0 \le c_i \le \frac{1}{2n\lambda},\quad i=1,\dots,n
\end{cases}
\]

\paragraph{Kernel Trick.} Il prodotto scalare \(\phi(\mathbf{x}^{(i)})\cdot\phi(\mathbf{x}^{(j)})\) è \textbf{dispendioso} da calcolare. Per questo motivo, si usa una \textbf{funzione kernel $K$} tale che soddisfi tale condizione:
\[
K(\mathbf{x}^{(i)},\mathbf{x}^{(j)}) = \phi(\mathbf{x}^{(i)})\cdot\phi(\mathbf{x}^{(j)}).
\]

Grazie a questa funzione, si definisce il mapping del nuovo spazio permettendo di sostituire il prodotto scalare con la funzione kernel, risparmiando tempo di calcolo (esempi di kernel sono il kernel polinomiale, gaussiano e sigmoide).

\paragraph{SVM Multi-classe.} SVM, come il perceptron, può essere esteso a problemi di classificazione multi-classe usando strategie come One–Vs–One (OVO) o One–Vs–All (OVA), addestrando più modelli binari e combinando le loro predizioni per ottenere la classificazione finale.

\section{Apprendimento Lazy}
Fino a qui abbiamo visto tipi di apprendimento chiamati \textbf{eager}, in cui il modello viene costruito durante la fase di training.  Esiste un secondo tipo di apprendimento chiamato \textbf{lazy}, dove il modello memorizza un training set di dati e calcola la funzione di classificazione localmente per ogni nuovo dato.

La funzione di predizione è quindi approssimata \emph{localmente}, inoltre questi metodi funzionano su grandi dataset con pochi attributi e che si aggiornano continuamente.

\subsection{K-Nearest Neighbor}\label{subsec:knn}
K-Nearest Neighbor (KNN) è un algoritmo di classificazione lazy che assegna la classe di un nuovo esempio basandosi sulle classi dei suoi \(k\) vicini più prossimi nel training set.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{images/knn_example.png}
  \caption{Esempio di classificazione con K-NN (k=5). Il punto da classificare (in verde) è circondato dal cerchio che indica i $k$ vicini più prossimi: tra i 5 vicini ci sono tre triangoli rossi e due quadrati blu, quindi per maggioranza il punto viene assegnato alla classe rossa.}
  \label{fig:knn-example}
\end{figure}

\paragraph{Algoritmo.}
\begin{enumerate}
  \item Memorizza il training set \(D=\{(\mathbf{x}^{(i)},y^{(i)})\}_{i=1}^{n}\).
  \item Per ogni nuovo esempio \(\mathbf{x}\):
  \begin{enumerate}
    \item Calcola la distanza tra \(\mathbf{x}\) e ogni esempio in \(D\) (es. distanza euclidea).
    \item Identifica i \(k\) esempi più vicini.
    \item Assegna a \(\mathbf{x}\) la classe più frequente tra i \(k\) vicini.
  \end{enumerate}
\end{enumerate}

\paragraph{Varianti.}
Esistono alcune varianti di KNN:
\begin{itemize}
  \item \textbf{Distanza pesata:} i punti a distanza più vicina hanno un peso maggiore nel voto.
  \item \textbf{K-NN per valori continui:} Assegna agli attributi ignoti della tupla da classificare la media dei valori degli attributi delle tuple più vicine (rispetto agli attributi noti).
\end{itemize}

\section{Ensemble Learning}\label{sec:ensemble}
L'ensemble learning ha lo scopo di combinare più modelli, solitamente omogenei, per migliorare le prestazioni di classificazione rispetto a un singolo modello. L'idea è che combinando le predizioni di più modelli si possa ridurre l'errore complessivo, sfruttando la diversità tra i modelli. Poiché richiede molta potenza di calcolo, si combinano modelli semplici e veloci da addestrare.

\subsection{Bagging}
Il paradigma \emph{Bagging} consiste nell'addestrare diversi classificatori in parallelo, combinando le predizioni finali.

\paragraph{Algoritmo.}
Siano $M_1, M_2, ..., M_k$ i $k$ modelli da combinare:
\begin{enumerate}
  \item Addestra ciascun modello $M_i$ su un sottoinsieme casuale del training set o su un campione di dati estratto tramite bootstrapping\footnote{Il bootstrapping è una tecnica di campionamento con reinserimento che consente di creare più sottoinsiemi di dati a partire da un dataset originale. Ogni sottoinsieme può contenere duplicati e viene utilizzato per addestrare modelli diversi nell'ensemble learning.}.
  \item Combina opportunamente le predizioni dei modelli:
  \begin{itemize}
    \item \textbf{Classificazione:} voto di maggioranza tra le classi predette.
    \item \textbf{Regressione:} media delle predizioni.
  \end{itemize}
\end{enumerate}

\paragraph{Random Forest.} Una Random Forest combina molti alberi decisionali addestrati su sottoinsiemi casuali del training set e su sottoinsiemi casuali di feature. Ogni albero viene addestrato su un campione bootstrap del dataset e, ad ogni split, si considera solo un sottoinsieme casuale delle feature per determinare la migliore divisione. La predizione finale viene effettuata tramite voto di maggioranza tra gli alberi.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.75\textwidth]{images/random_forest.png}
  \caption{Schema di Random Forest: dai dati di training $(X)$ si costruiscono $k$ alberi decisionali $T_1(X,\alpha_1)$, $T_2(X,\alpha_2)$, $\dots$, $T_k(X,\alpha_k)$, ciascuno con parametri $\alpha_i$ diversi. La predizione finale è ottenuta combinando i risultati attraverso voting o averaging ($\sum T_i(X,\alpha_i)$).}
  \label{fig:random-forest}
\end{figure}

Il vantaggio è quello di usarlo su dataset di dati molto predittivi, in quanto riduce la correlazione tra gli alberi e migliora la generalizzazione.

\subsection{Boosting}
Il \emph{Boosting} è un paradigma di ensemble learning che costruisce sequenzialmente una serie di modelli deboli, dove ogni modello successivo si concentra sugli errori commessi dai modelli precedenti. 

\paragraph{Algoritmo.}
Siano $M_1, M_2, ..., M_k$ i $k$ modelli da combinare:
\begin{enumerate}
  \item Inizializza i pesi delle istanze del training set in modo uniforme.
  \item Per ogni modello $M_i$:
  \begin{enumerate}
    \item Addestra $M_i$ sul training set ponderato.
    \item Calcola l'errore di $M_i$ e aggiorna i pesi delle istanze: aumenta i pesi delle istanze mal classificate e diminuisce quelli delle istanze correttamente classificate.
  \end{enumerate}
  \item Combina le predizioni dei modelli, pesando ciascuna predizione in base alla sua accuratezza (anche qui, nel caso di classificazione si usa il voto ponderato, mentre nella regressione si usa una media ponderata).
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{images/boosting_example.png}
  \caption{Schema di Boosting: ogni classificatore viene addestrato su dati pesati in base agli errori dei modelli precedenti. I dati mal classificati ricevono peso maggiore, così i modelli successivi si concentrano sulle istanze più difficili. Le predizioni finali sono combinate in un ensemble ponderato.}
  \label{fig:boosting-example}
\end{figure}

\subsection{Adaboost} 
In questo algoritmo si usano solitamente \textbf{alberi decisionali} con due foglie chiamati \emph{stumps}. Combinando opprtunamente tali stumps, che sono semplici e veloci, si ottiene un classificatore accurato.

\begin{enumerate}
  \item Inizializza i pesi delle istanze del training set in modo uniforme.
  \item Per $i=1$ a $k$:
  \begin{enumerate}
    \item Crea lo stump $M_i$ che minimizza l'errore ponderato sul training set.
    \item Per lo stump $M_i$, calcola un peso $P_i$ basato sulla sua accuratezza:
    \item Aumenta i pesi delle tuple classificate in modo errato e diminuisci quelli delle tuple classificate correttamente, dopodiché normalizza tra 0 e 1 i nuovi pesi.
  \end{enumerate}
  \item Combina linearmente le predizioni dei modelli usando i pesi $P_i$.
\end{enumerate}

\paragraph{Gini Index Pesato.}
Sia $D$ un \emph{dataset} con $N$ tuple e $k$ classi $C_1, C_2, \dots, C_k$ e sia $w(\mathbf{x})$ il peso associato alla tupla $\mathbf{X}$. Indico con $T_{C_i}$ l'insieme delle \textbf{tuple} di $D$ aventi \textbf{classe} $C_i$. La probabilità di osservare una tupla di classe $C_i$ è:
\[
p_i = \frac{\sum_{\mathbf{X} \in T_{C_i}} w(\mathbf{X})}{\sum_{\mathbf{Y} \in D} w(\mathbf{Y})}
\]
Il \textbf{Gini index} dello splitting è dato da:
\[
gini_{split}(S_X) = \sum_{i=1}^k \frac{|S_i|}{|S_X|} \; gini(S_i)
\qquad\qquad
  gini(S_i) = 1 - \sum_{i=1}^n p_i^2
\]

\paragraph{Peso dello stump.} 
Considerando TotalError la somma dei pesi delle tuple classificate erroneamente dallo stump $M_i$, il peso $P_i$ dello stump si calcola come:
\[
P_i = \frac{1}{2} \log\biggl(\frac{1 - \text{TotalError}}{\text{TotalError}}\biggr).
\]

\paragraph{Aggiornamento dei pesi.}
Sia $w^{(t)}(\mathbf{X})$ il peso della tupla $\mathbf{X}$ all'iterazione $t$. Dopo aver calcolato lo stump $M_i$ e il suo peso $P_i$, si aggiorna il peso della tupla come:

\noindent
Sia $w^{(i)}(\mathbf{X})$ il peso della tupla $\mathbf{X}$ al passo $i$-esimo.

\begin{itemize}
  \item Se $\mathbf{X}$ \textbf{non} è classificata correttamente dallo stump $M_i$, il peso \textbf{aumenta}:
  \[
    w^{i+1}(\mathbf{X}) = w^{i}(\mathbf{X}) \cdot e^{w(D_i)}
  \]
  \item Se $\mathbf{X}$ è classificata \textbf{correttamente} dallo stump $M_i$, il peso \textbf{diminuisce}:
  \[
    w^{i+1}(\mathbf{X}) = w^{i}(\mathbf{X}) \cdot e^{-w(D_i)}
  \]
\end{itemize}

\noindent
Esempi che mostrano l'andamento di $e^{w(D_i)}$ e $e^{-w(D_i)}$ in funzione di $w(D_i)$:
\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/aumento_pesi_tuple.png}
    \\[4pt]
    {\small Incremento del peso: $e^{w(D_i)}$ in funzione di $w(D_i)$}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/diminuzione_pesI_tuple.png}
    \\[4pt]
    {\small Decremento del peso: $e^{-w(D_i)}$ in funzione di $w(D_i)$}
  \end{minipage}
  \caption{Andamento dell'incremento e del decremento dei pesi delle tuple per Adaboost.}
  \label{fig:adaboost-weight-changes}
\end{figure}

\subsection*{Bagging vs Boosting}
Il bagging è utilizzato maggiormente per ridurre \emph{overfitting}, mentre il boosting è più utilizzato quando si vogliono combinare classificatori molto semplice che presi singolarmente hanno bassa accuracy.

\section{Validazione di un classificatore}

\subsection{Matrice di confusione}
La matrice di confusione viene usata per rappresentare l'accuracy dove ogni riga contiene valori reali mentre ogni colonna i valori predetti. Un modello che funziona bene si vede dalle colonne, poiché se hanno valori diversi da 0 allora il modello predice bene la classe.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{images/confusion_matrix.png}
  \caption{Esempio di matrice di confusione per un classificatore a tre classi (Gatto, Cane, Coniglio): le righe indicano le classi reali, le colonne le classi predette. I numeri nelle celle sono le frequenze assolute; le somme marginali mostrano il totale per riga/colonna. Un buon classificatore presenta valori elevati sulla diagonale principale (corretta assegnazione).}
  \label{fig:confusion-matrix}
\end{figure}

\paragraph{Misure di accuratezza con due classi.}
Si possono considerare due classi "Positiva" $P$ e "Negativa" $N$. Da questo possono nascere:
\begin{itemize}
  \item \textbf{True positive}: tuple di classe $P$ che vengono classificate come $P$.
  \item \textbf{False positive}: tuple di classe $P$ che vengono classificate come $N$.
  \item \textbf{True negative}: tuple di classe $N$ che vengono classificate come $N$.
  \item \textbf{False negative}: tuple di classe $N$ che vengono classificate come $P$.
\end{itemize}


\paragraph{Misure di accuratezza (due classi).}
Per il caso binario, indicando con $\lvert\mathrm{Tpos}\rvert$ il numero di true positive, con
 $\lvert\mathrm{Tneg}\rvert$ il numero di true negative, con $\lvert\mathrm{Fpos}\rvert$ il numero di false
 positive, con $\lvert\mathrm{Fneg}\rvert$ il numero di false negative, e con $\lvert\mathrm{Pos}\rvert,\lvert\mathrm{Neg}\rvert$ i totali veri delle classi positive e negative, si definiscono le misure
pi\`u comuni:
\begin{align*}
&\text{Recall (Rec) / Sensitivity / True Positive Rate (TPR)}:\quad &&\mathrm{Rec}=\frac{\lvert\mathrm{Tpos}\rvert}{\lvert\mathrm{Pos}\rvert},\\
&\text{Specificity (Spec) / True Negative Rate (TNR)}:\quad &&\mathrm{Spec}=\frac{\lvert\mathrm{Tneg}\rvert}{\lvert\mathrm{Neg}\rvert},\\
&\text{False Positive Rate (FPR)}:\quad &&\mathrm{FPR}=\frac{\lvert\mathrm{Fpos}\rvert}{\lvert\mathrm{Neg}\rvert},\\
&\text{False Discovery Rate (FDR)}:\quad &&\mathrm{FDR}=\frac{\lvert\mathrm{Fpos}\rvert}{\lvert\mathrm{Tpos}\rvert + \lvert\mathrm{Fpos}\rvert},\\
&\text{Precision (Prec)}:\quad &&\mathrm{Prec}=\frac{\lvert\mathrm{Tpos}\rvert}{\lvert\mathrm{Tpos}\rvert + \lvert\mathrm{Fpos}\rvert},\\
&\text{Accuracy (Acc)}:\quad &&\mathrm{Acc}=\frac{\lvert\mathrm{Tpos}\rvert + \lvert\mathrm{Tneg}\rvert}{\lvert\mathrm{Pos}\rvert + \lvert\mathrm{Neg}\rvert},\\
&\text{F1 (armonica di Precision e Recall)}:\quad &&F1 = 2\cdot\frac{\mathrm{Prec}\cdot\mathrm{Rec}}{\mathrm{Prec}+\mathrm{Rec}}.
\end{align*}

Queste misure sono utili per valutare diversi aspetti del classificatore: la recall misura la capacit\`a di trovare
le istanze positive, la precision misura la correttezza delle predizioni positive, mentre l'accuracy fornisce una
visione globale. In presenza di sbilanciamento di classe conviene preferire precision/recall o F1 all'accuracy.

\subsection{Soglia discriminativa in un classificatore binario}
In alcuni casi, la distinzione tra due classi (caso binario) si basa su un valore soglia $\sigma$. Ad esempio, predico se una mail è spam oppure no sulla base di tale soglia. Le performance del classificatore sono valutate al variare di $\sigma$ tramite \textbf{curve ROC}. Esse rappresentano il \emph{True Positive Rate (TPR)} in funzione del \emph{False Positive Rate (FPR)}, al variare di $\sigma$.

Se la soglia è molto alta, tutte le tuple sono classificate come \emph{negative}: nessuna tupla negativa è classificata come positiva (\( \mathrm{FPR}=0 \)) e nessuna tupla positiva è classificata come positiva (\( \mathrm{TPR}=0 \)). 

\subsection{Curva ROC}
Se la soglia è molto bassa, tutte le tuple sono classificate come \emph{positive}: tutte le tuple negative sono classificate come positive (\( \mathrm{FPR}=1 \)) e tutte le tuple positive sono classificate come positive (\( \mathrm{TPR}=1 \)). Per valori intermedi di soglia si ottengono valori di TPR e FPR compresi tra 0 e 1.

Situazione ideale: \( \mathrm{TPR} \) aumenta fino a 1 mentre \( \mathrm{FPR} \) si mantiene pari a 0 (\emph{miglior classificatore}). Un \emph{classificatore random} ha sempre valori di \( \mathrm{TPR} \) e \( \mathrm{FPR} \) uguali al variare di $\sigma$.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.6\linewidth]{images/curve-roc-example.png}
  \caption{Esempio di curva ROC al variare della soglia $\sigma$.}
\end{figure}

\subsection{Curva di Precision-Recall}
A differenza della curva ROC, la \textbf{curva di Precision-Recall} rappresenta la \textbf{Precision} in funzione della \textbf{Recall} al variare di $\sigma$.

Le curve \textbf{ROC} si utilizzano nel caso di dataset \emph{bilanciati} (frequenza simile delle due classi), mentre la \textbf{curva di Precision-Recall} è preferibile nel caso di dataset \emph{sbilanciati}.

Come indicatore dell’accuratezza del classificatore si usa l’area al di sotto delle curve ROC e Precision-Recall, detta \textbf{Area Under the Curve (AUC)}. \(\mathrm{AUC}\in[0,1]\); \(\mathrm{AUC}=1\) indica un classificatore perfetto.

\subsection{Validazione di un classificatore}
Per validare un classificatore si considerano solitamente diversi \emph{partizionamenti} del dataset in \emph{training} e \emph{test set} e si addestra il classificatore sul training set così ottenuto.
Generalmente si utilizzano due metodi:
\begin{description}
  \item[Holdout:] Si fissa una percentuale $X$, dopo si partiziona il dataset in due set indipendenti, training e test sets, in base a $X$, si addestra il modello sul training set e si applica il classificatore al test set per misurare l'accuratezza. Una variante prevede di ripetere l'holdout $k$ volte e di calcolare la media delle accuratezze ottenute.
  \item[K-Fold Cross Validation:] Si partiziona il dataset in $k$ sottoinsiemi (fold) di dimensioni approssimativamente uguali. Si eseguono $k$ iterazioni, in ciascuna si usa un fold come test set e gli altri $k-1$ fold come training set. Si calcola l'accuratezza per ogni iterazione e si fa la media delle $k$ accuratezze ottenute per avere una stima complessiva delle prestazioni del classificatore.
\end{description}